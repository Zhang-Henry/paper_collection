forum,title,keywords,abstract,pdf,match,venue,year,type
https://openreview.net/forum?id=zAdUB0aCTQ,{'value': 'AgentBench: Evaluating LLMs as Agents'},"{'value': ['Large language models', 'Autonomous agents', 'Reasoning', 'Evaluation', 'Benchmark']}","{'value': ""The potential of Large Language Model (LLM) as agents has been widely acknowledged recently.\nThus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments.\nWe present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities.\nOur extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B.\nWe identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.\nImproving instruction following and training on high quality multi-round alignment data could improve agent performance.\nAnd different from existing assumptions, training on code present ambivalent impacts on different agent tasks.\nDatasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.""}",https://openreview.net{'value': '/pdf/6eee0bd1fd98c135372baedb2a5644233a013bb2.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=z6KS9D1dxt,{'value': 'Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game'},"{'value': ['Multi-agent reinforcement learning', 'Robustness', 'Game Theory', 'Adversarial Attack']}","{'value': 'In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist and the corresponding policy weakly dominates previous approaches as time goes to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experiments on matrix game, Level-based Foraging and StarCraft II indicate that, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations, showing resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.'}",https://openreview.net{'value': '/pdf/ae82041a50b5244c3cc55d71579b529584e36982.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=yoVq2BGQdP,{'value': 'Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning'},"{'value': ['Fairness', 'Multi-Agent Reinforcement Learning', 'Markov Decision Process']}","{'value': ""Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a  Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach.""}",https://openreview.net{'value': '/pdf/832d7ea57a154b15de9382f6f8db49dad34bc05e.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=wFWuX1Fhtj,{'value': 'On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning'},"{'value': ['Multi-Agent Reinforcement Learning', 'Constrained reinforcement learning', 'Primal-Dual', 'Duality gap', 'Primal algorithm']}","{'value': 'Constrained cooperative multi-agent reinforcement learning (MARL) is an emerging learning framework that has been widely applied to manage multi-agent systems, and many primal-dual type algorithms have been developed for it. However, the convergence of primal-dual algorithms crucially relies on strong duality -- a condition that has not been formally proved in constrained cooperative MARL. In this work, we prove that strong duality fails to hold in constrained cooperative MARL, by revealing a nonconvex quadratic type constraint on the occupation measure induced by the product policy. Consequently, our reanalysis of the primal-dual algorithm shows that its convergence rate is hindered by the nonzero duality gap. Then, we propose a decentralized primal approach for constrained cooperative MARL to avoid the duality gap, and our analysis shows that its convergence is hindered by another gap induced by the advantage functions. Moreover, we compare these two types of algorithms via concrete examples, and show that neither of them always outperforms the other one. Our study reveals that constrained cooperative MARL is generally a challenging and highly nonconvex problem, and its fundamental structure is very different from that of single-agent constrained RL.'}",https://openreview.net{'value': '/pdf/fbb1a9eec51b42b14f0f37676f9db8b154cbc9d2.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=vZZ4hhniJU,{'value': 'Learning Multi-Agent Communication with Contrastive Learning'},"{'value': ['Multi-Agent Reinforcement Learning', 'Emergent Communication', 'Contrastive Learning']}","{'value': 'Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.'}",https://openreview.net{'value': '/pdf/2c9dcdc69b3da56a7ebdec7279ee8bc8087b5c39.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=tmqOhBC4a5,{'value': 'Maximum Entropy Heterogeneous-Agent Reinforcement Learning'},"{'value': ['cooperative multi-agent reinforcement learning', 'heterogeneous-agent soft actor-critic', 'maximum entropy heterogeneous-agent mirror learning']}","{'value': '*Multi-agent reinforcement learning* (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning \\emph{stochastic} policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose *Heterogeneous-Agent Soft Actor-Critic* (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to *quantal response equilibrium* (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named *Maximum Entropy Heterogeneous-Agent Mirror Learning* (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration.'}",https://openreview.net{'value': '/pdf/82bacc9b0a9551bf4922e43270f4c315044f70af.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=s9z0HzWJJp,{'value': 'SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series'},"{'value': ['Large Langauge Models', 'Agent', 'Prompt Tunning', 'Time series forcasting']}","{'value': 'We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called ""hyperportfolio"", that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to ""invest"". We also propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis & Proof prompting for producing in-depth analyses on input news, articles, etc. to assist decision-making. We perform experiments and ablation studies to explore the factors that impact performance. The results show that our proposed method achieves improvements of 32.4% and 30.4% compared to the state-of-the-art method in the two experimental settings.'}",https://openreview.net{'value': '/pdf/473beb76f232cb31d5e4d7594a17375a1347cef9.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=pAoqRlTBtY,{'value': 'Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning'},"{'value': ['Causal Reasoning', 'Causal Discovery', 'Structural Causal Models', 'Large Language Models']}","{'value': ""Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.""}",https://openreview.net{'value': '/pdf/62fc3766e10c6f5fa2f2a9b44b46098519f89596.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=oQKKlzxV1o,{'value': 'Online Information Acquisition: Hiring Multiple Agents'},"{'value': ['online learning', 'information acquisition', 'mechanism design']}","{'value': ""We investigate the mechanism design problem faced by a principal who hires \\emph{multiple} agents to gather and report costly information. Then, the principal exploits the  information to make an informed decision.  We model this problem as a game, where the principal announces a mechanism consisting in action recommendations and a payment function, a.k.a. scoring rule. Then, each agent chooses an effort level and receives partial information about an underlying state of nature based on the effort. Finally, the agents report the information (possibly non-truthfully), the principal takes a decision based on this information, and the agents are paid according to the scoring rule. While previous work focuses on single-agent problems, we consider multi-agents settings. This poses the challenge of coordinating the agents' efforts and aggregating correlated information. Indeed, we show that optimal mechanisms must correlate agents' efforts, which introduces externalities among the agents, and hence complex incentive compatibility constraints and equilibrium selection problems. First, we design a polynomial-time algorithm to find an optimal incentive compatible mechanism. Then, we study an online problem, where the principal repeatedly interacts with a group of unknown agents. We design a no-regret algorithm that provides $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret with respect to an optimal mechanism, matching the state-of-the-art bound for single-agent settings.""}",https://openreview.net{'value': '/pdf/83fcb8ac23e715a1b18621cd00f0a8a7d023dc7a.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=oKn9c6ytLx,{'value': 'WebArena: A Realistic Web Environment for Building Autonomous Agents'},{'value': ['language-guided agents; web automation; benchmark']},"{'value': 'With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting.  The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \\ours can be used to measure such progress.\\footnote{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.}'}",https://openreview.net{'value': '/pdf/384c500abb2b5adc4f3c40956a267477925c4b94.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=o7qhUMylLU,{'value': 'Sample-Efficient Multi-Agent RL: An Optimization Perspective'},"{'value': ['multi-agent reinforcement learning', 'theory', 'general function approximation', 'general-sum Markov Games']}","{'value': 'We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under general function approximation. \n    In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.'}",https://openreview.net{'value': '/pdf/fe5b0154a5ddbf71df6a0cd9077a345a6fa6065c.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=mM7VurbA4r,{'value': 'SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents'},"{'value': ['Social', 'Interaction', 'Agent', 'Social intelligence', 'Large Language Models', 'Evaluation', 'Theory of Mind']}","{'value': ""*Humans are social beings*; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and *interact* under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.""}",https://openreview.net{'value': '/pdf/6aece1f9088fc415196df5830f1ca62e6dbf37d3.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=hNhwSmtXRh,{'value': 'Lemur: Harmonizing Natural Language and Code for Language Agents'},"{'value': ['large language model', 'agent', 'code generation', 'reasoning', 'decision making']}","{'value': 'We introduce Lemur and Lemur-Chat, openly accessible language models optimized\nfor both natural language and coding capabilities to serve as the backbone\nof versatile language agents. The evolution from language chat models to\nfunctional language agents demands that models not only master human interaction,\nreasoning, and planning but also ensure grounding in the relevant environments.\nThis calls for a harmonious blend of language and coding capabilities\nin the models. Lemur and Lemur-Chat are proposed to address this necessity,\ndemonstrating balanced proficiencies in both domains, unlike existing\nopen-source models that tend to specialize in either. Through meticulous pretraining\nusing a code-intensive corpus and instruction fine-tuning on text and code\ndata, our models achieve state-of-the-art averaged performance across diverse\ntext and coding benchmarks. Comprehensive experiments demonstrate Lemur’s\nsuperiority over existing open-source models and its proficiency across various\nagent tasks involving human communication, tool usage, and interaction under\nfully- and partially- observable environments. The harmonization between natural\nand programming languages enables Lemur-Chat to significantly narrow the\ngap with proprietary models on agent abilities, providing key insights into developing\nadvanced open-source agents adept at reasoning, planning, and operating\nseamlessly across environments. Our model and code have been open-sourced at\nhttps://github.com/OpenLemur/Lemur.'}",https://openreview.net{'value': '/pdf/8ef62990871ebf2cac77dc6ea498085f167f070a.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Z59Rb5bPPP,{'value': 'Trajeglish: Traffic Modeling as Next-Token Prediction'},"{'value': ['self-driving', 'traffic modeling', 'autonomous vehicles', 'simulation', 'motion prediction', 'transformer']}","{'value': 'A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.'}",https://openreview.net{'value': '/pdf/6a1754e5443464338e361380403b4fec2f5d94b2.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=W3VsHuga3j,{'value': 'Modeling Boundedly Rational Agents with Latent Inference Budgets'},"{'value': ['neurosymbolic', 'planning', 'rationality']}","{'value': 'We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating constrained inference. In this work, we introduce a latent inference budget model (L-IBM) that models these constraints explicitly, via a latent variable (inferred jointly with a model of agents’ goals) that controls the runtime of an iterative inference algorithm. L-IBMs make it possible to learn agent models using data from diverse populations of suboptimal actors. In three modeling tasks—inferring navigation goals from routes, inferring communicative intents from human utterances, and predicting next moves in human chess games—we show that L-IBMs match or outperforms Boltzmann models of decision-making under uncertainty. Moreover, the inferred inference budgets are themselves meaningful, efficient to compute, and correlated with measures of player skill, partner skill and task difficulty.'}",https://openreview.net{'value': '/pdf/b6fcabc7a4521fa9b8ff72f6d09fbafc1e118951.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=VtmBAGCN7o,{'value': 'MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework'},"{'value': ['Autonomous Agent', 'Meta Programming', 'Multi-Agent Society', 'Group Intelligence']}","{'value': 'Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.'}",https://openreview.net{'value': '/pdf/474fc6dad3bd9bf7fdb97c7cd72b2cc0649a9647.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=UBVNwD3hPN,{'value': 'CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents'},"{'value': ['Interactive Environments', 'Benchmark', 'Reinforcement Learning', 'Language Agent', 'Multi-agent']}","{'value': 'The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization’s profound alignment with human society requires sophisticated learning and prior knowledge, while its ever-changing space and action space demand robust reasoning for generalization. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. Overall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.'}",https://openreview.net{'value': '/pdf/8e9b4884f752883a17e58942608d9cc769610a01.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Ts95eXsPBc,{'value': 'Spatially-Aware Transformers for Embodied Agents'},"{'value': ['Episodic Memory', 'Spatial Inference', 'Prediction', 'Generation', 'Reinforcement Learning']}","{'value': 'Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning. The source code for our models and experiments will be available at \\href{https://github.com/spatially_aware_transformer}{https://github.com/spatially_aware_transformer}.'}",https://openreview.net{'value': '/pdf/1dc0ff45874e872749597c38bcfc2df0fa7ed0d6.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=SBoRhRCzM3,{'value': 'THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS'},"{'value': ['LLM Complex Reasoning', 'Language Model Reasoning']}","{'value': 'Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. \nHowever, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}.\nTo address these issues, we propose \\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs.\nThese analogous problems are related to the input one, with reusable solutions and problem-solving strategies.\nThus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. \nTo achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. \nThen, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch.\nTP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. \nExperiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\\% improvement of human preference in Creative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent Planning.'}",https://openreview.net{'value': '/pdf/1cddcaa3f523f7ad1c9aea87d5180f2d173e4bb8.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=S2oTVrlcp3,{'value': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents'},{'value': ['Large Language Models']},"{'value': ""Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately.\nSmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. \nWe release our benchmark at https://github.com/microsoft/SmartPlay""}",https://openreview.net{'value': '/pdf/f84669704e333d9a0d57f5fd1dfd011b03109021.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Qox9rO0kN0,{'value': 'Learning Multi-Agent Communication from Graph Modeling Perspective'},"{'value': ['communication learning', 'multi-agent reinforcement learning']}","{'value': 'In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.'}",https://openreview.net{'value': '/pdf/b18ae005fc01451473ec4301d9802e5267831053.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=NltzxpG0nz,{'value': 'Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds'},"{'value': ['large multimodal pre-training', 'open-world embodied agent', 'large language model']}","{'value': ""Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks and carry out experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. The project’s website and code can be found at https://sites.google.com/view/steve-eye.""}",https://openreview.net{'value': '/pdf/a46ce05ff44a87fada32b918f094df26df3d2f07.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=MCNqgUFTHI,{'value': 'Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents'},"{'value': ['Dialogue Policy Planning', 'Proactive Dialogue', 'Large Language Model']}","{'value': 'Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.'}",https://openreview.net{'value': '/pdf/30c7e22635f23140a4f37883885b6e80b5f7c750.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=M6XWoEdmwf,{'value': 'AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents'},"{'value': ['Meta-RL', 'Generalization', 'Long-Term Memory', 'Transformers']}","{'value': ""We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments.""}",https://openreview.net{'value': '/pdf/6ffd1eb5dc0bd2b144d5d0309763b3ed5e114e8b.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=LjivA1SLZ6,{'value': 'Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning'},"{'value': ['Multi-agent reinforcement learning', 'episodic control', 'episodic incentive', 'state embedding']}","{'value': 'In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.'}",https://openreview.net{'value': '/pdf/8b2d5ac5539754d00bf99458a60c63157c74fbdb.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=KOZu91CzbK,{'value': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization'},"{'value': ['Language Agent', 'AI Agent', 'Reinforcement Learning']}","{'value': 'Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.'}",https://openreview.net{'value': '/pdf/d02f39256b41ec50ca8cd3a5b136065bc7a4caae.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=GEcwtMk1uA,{'value': 'Identifying the Risks of LM Agents with an LM-Emulated Sandbox'},"{'value': ['Language Model Agent', 'Tool Use', 'Evaluation', 'Safety', 'Language Model']}","{'value': 'Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes toolkits and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.'}",https://openreview.net{'value': '/pdf/d1601f78407737fc216de9e6ec0085038f8c885f.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=FQepisCUWu,{'value': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate'},"{'value': ['Large Language Model', 'Multi-Agent Debate', 'LLM evaluators']}","{'value': ""Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies.\nIn this paper, we construct a multi-agent referee team called $\\textbf{ChatEval}$ to autonomously discuss and evaluate the quality of different texts. \nOur experiments on two benchmarks illustrate that ChatEval delivers superior accuracy and correlation in alignment with human assessment. Furthermore, we find that the diverse role prompts (different personas) are essential in the multi-agent debate process; that is, utilizing the same role description in the prompts can lead to a degradation in performance. Our qualitative analysis also shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments.""}",https://openreview.net{'value': '/pdf/6950b70dde4f8d7425424dce9cc4063f505b2194.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EpVe8jAjdx,{'value': 'Privileged Sensing Scaffolds Reinforcement Learning'},"{'value': ['reinforcement learning', 'model-based reinforcement learning', 'world models', 'robotics', 'privileged information', 'asymmetric learning', 'multimodality', 'perception', 'sensing']}","{'value': 'We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon “sensory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose “Scaffolder”, a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new “S3” suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/'}",https://openreview.net{'value': '/pdf/8ab7ec56b6e56ac45ce2b68e95b277283dbb8377.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Ep0TtjVoap,{'value': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving'},"{'value': ['Large Language Models', 'Mathematical Reasoning', 'Tool Learning']}","{'value': ""Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.""}",https://openreview.net{'value': '/pdf/2b0b45b11d0f61912efb1a932fb494d36f7b88e6.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EnXJfQqy0K,{'value': 'Building Cooperative Embodied Agents Modularly with Large Language Models'},"{'value': ['Large Language Models', 'Embodied Intelligence', 'Multi-Agent Cooperation', 'Human-AI Interaction', 'Communication']}","{'value': 'In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.'}",https://openreview.net{'value': '/pdf/9baa34313f1aea292b16e30697c20766afadddb1.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EHg5GDnyq1,{'value': 'AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors'},"{'value': ['large language mode', 'agent', 'multi-agent']}","{'value': 'Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework AgentVerse that can effectively orchestrate a collaborative group of expert agents as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that AgentVerse can proficiently deploy multi-agent groups that outperform a single agent. Extensive experiments on text understanding, reasoning, coding, tool utilization, and embodied AI confirm the effectiveness of AgentVerse. Moreover, our analysis of agent interactions within AgentVerse reveals the emergence of specific collaborative behaviors, contributing to heightened group efficiency. We will release our codebase, AgentVerse, to further facilitate multi-agent research.'}",https://openreview.net{'value': '/pdf/7c9bd9a841a2ba0c11ea97abb5e982430c2fc95e.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=BqEvdOS1Hs,{'value': 'Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain'},"{'value': ['human enhancement', 'human-agent collaboration', 'game playing', 'deep reinforcement learning']}","{'value': 'Existing game AI research mainly focuses on enhancing agents\' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading to poor experiences for their human partners. In other words, most game AI agents are modeled in a ""self-centered"" manner. In this paper, we propose a ""human-centered"" modeling scheme for collaborative agents that aims to enhance the experience of humans. Specifically, we model the experience of humans as the goals they expect to achieve during the task. We expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents\' original abilities (e.g., winning games). To achieve this, we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG approach introduces a ""baseline"", which corresponds to the extent to which humans primitively achieve their goals, and encourages agents to learn behaviors that can effectively enhance humans in achieving their goals better. We evaluate the RLHG agent in the popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both objective performance and subjective preference results show that the RLHG agent provides participants better gaming experience.'}",https://openreview.net{'value': '/pdf/6ce678ca6108e4a85517e2526c83d7d968f7108b.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ADSxCpCu9s,{'value': 'LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents'},"{'value': ['task planning', 'language models', 'benchmarking', 'embodied agents', 'home robots']}","{'value': 'Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.'}",https://openreview.net{'value': '/pdf/35abce446ca9b6e7a9136f7c38556084c63538ec.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=9JQtrumvg8,"{'value': 'A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis'}","{'value': ['Web Navigation', 'Web Automation', 'Large Language Models', 'Language Model Agents', 'Tool Use', 'Program Synthesis']}","{'value': 'Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.\nHowever, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.\nWe introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.\nWebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.\nWe design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.\nWe empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.'}",https://openreview.net{'value': '/pdf/0b27823f96e3efd0ed6921aafc4fe4643d1aeec5.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=8HCARN2hhw,{'value': 'Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction'},"{'value': ['Navigation', 'Embodied AI', 'Perception']}","{'value': 'Agents navigating in 3D environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly defined interpretation, whereas classical robotics addresses this with scene reconstruction resulting in some form of map, usually estimated with geometry and sensor models and/or learning. In this work we propose to learn an actionable representation of the scene independently of the targeted downstream task and without explicitly optimizing reconstruction. The learned representation is optimized by a blind auxiliary agent trained to navigate with it on multiple short sub episodes branching out from a waypoint and, most importantly, without any direct visual observation. We argue and show that the blindness property is important and forces the (trained) latent representation to be the only means for planning. With probing experiments we show that the learned representation optimizes navigability and not reconstruction. On downstream tasks we show that it is robust to changes in distribution, in particular the sim2real gap, which we evaluate with a real physical robot in a real office building, significantly improving performance.'}",https://openreview.net{'value': '/pdf/2b5b56f45f498f5877f3de2ea54e150352e0a408.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=7M0EzjugaN,{'value': 'Online Continual Learning for Interactive Instruction Following Agents'},"{'value': ['Embodied AI', 'Continual Learning']}","{'value': 'In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic, since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous ‘data prior’ based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed challenging Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior arts in our empirical validations by noticeable margins.'}",https://openreview.net{'value': '/pdf/9bd1847dd40bdb80227f6ddd6480e1eee45a4999.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=22pyNMuIoa,{'value': 'PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization'},"{'value': ['Large Language Models', 'Expert-level Prompt Optimization', 'Strategic Planning']}","{'value': ""Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts.""}",https://openreview.net{'value': '/pdf/d4dbdee0d105cd3020bffdc2f56d99c33429d49c.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=zAdUB0aCTQ,{'value': 'AgentBench: Evaluating LLMs as Agents'},"{'value': ['Large language models', 'Autonomous agents', 'Reasoning', 'Evaluation', 'Benchmark']}","{'value': ""The potential of Large Language Model (LLM) as agents has been widely acknowledged recently.\nThus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments.\nWe present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities.\nOur extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B.\nWe identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.\nImproving instruction following and training on high quality multi-round alignment data could improve agent performance.\nAnd different from existing assumptions, training on code present ambivalent impacts on different agent tasks.\nDatasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.""}",https://openreview.net{'value': '/pdf/6eee0bd1fd98c135372baedb2a5644233a013bb2.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=z6KS9D1dxt,{'value': 'Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game'},"{'value': ['Multi-agent reinforcement learning', 'Robustness', 'Game Theory', 'Adversarial Attack']}","{'value': 'In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist and the corresponding policy weakly dominates previous approaches as time goes to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experiments on matrix game, Level-based Foraging and StarCraft II indicate that, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations, showing resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.'}",https://openreview.net{'value': '/pdf/ae82041a50b5244c3cc55d71579b529584e36982.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=yoVq2BGQdP,{'value': 'Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning'},"{'value': ['Fairness', 'Multi-Agent Reinforcement Learning', 'Markov Decision Process']}","{'value': ""Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a  Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach.""}",https://openreview.net{'value': '/pdf/832d7ea57a154b15de9382f6f8db49dad34bc05e.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=wFWuX1Fhtj,{'value': 'On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning'},"{'value': ['Multi-Agent Reinforcement Learning', 'Constrained reinforcement learning', 'Primal-Dual', 'Duality gap', 'Primal algorithm']}","{'value': 'Constrained cooperative multi-agent reinforcement learning (MARL) is an emerging learning framework that has been widely applied to manage multi-agent systems, and many primal-dual type algorithms have been developed for it. However, the convergence of primal-dual algorithms crucially relies on strong duality -- a condition that has not been formally proved in constrained cooperative MARL. In this work, we prove that strong duality fails to hold in constrained cooperative MARL, by revealing a nonconvex quadratic type constraint on the occupation measure induced by the product policy. Consequently, our reanalysis of the primal-dual algorithm shows that its convergence rate is hindered by the nonzero duality gap. Then, we propose a decentralized primal approach for constrained cooperative MARL to avoid the duality gap, and our analysis shows that its convergence is hindered by another gap induced by the advantage functions. Moreover, we compare these two types of algorithms via concrete examples, and show that neither of them always outperforms the other one. Our study reveals that constrained cooperative MARL is generally a challenging and highly nonconvex problem, and its fundamental structure is very different from that of single-agent constrained RL.'}",https://openreview.net{'value': '/pdf/fbb1a9eec51b42b14f0f37676f9db8b154cbc9d2.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=vZZ4hhniJU,{'value': 'Learning Multi-Agent Communication with Contrastive Learning'},"{'value': ['Multi-Agent Reinforcement Learning', 'Emergent Communication', 'Contrastive Learning']}","{'value': 'Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.'}",https://openreview.net{'value': '/pdf/2c9dcdc69b3da56a7ebdec7279ee8bc8087b5c39.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=tmqOhBC4a5,{'value': 'Maximum Entropy Heterogeneous-Agent Reinforcement Learning'},"{'value': ['cooperative multi-agent reinforcement learning', 'heterogeneous-agent soft actor-critic', 'maximum entropy heterogeneous-agent mirror learning']}","{'value': '*Multi-agent reinforcement learning* (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning \\emph{stochastic} policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose *Heterogeneous-Agent Soft Actor-Critic* (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to *quantal response equilibrium* (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named *Maximum Entropy Heterogeneous-Agent Mirror Learning* (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration.'}",https://openreview.net{'value': '/pdf/82bacc9b0a9551bf4922e43270f4c315044f70af.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=s9z0HzWJJp,{'value': 'SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series'},"{'value': ['Large Langauge Models', 'Agent', 'Prompt Tunning', 'Time series forcasting']}","{'value': 'We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called ""hyperportfolio"", that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to ""invest"". We also propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis & Proof prompting for producing in-depth analyses on input news, articles, etc. to assist decision-making. We perform experiments and ablation studies to explore the factors that impact performance. The results show that our proposed method achieves improvements of 32.4% and 30.4% compared to the state-of-the-art method in the two experimental settings.'}",https://openreview.net{'value': '/pdf/473beb76f232cb31d5e4d7594a17375a1347cef9.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=pAoqRlTBtY,{'value': 'Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning'},"{'value': ['Causal Reasoning', 'Causal Discovery', 'Structural Causal Models', 'Large Language Models']}","{'value': ""Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.""}",https://openreview.net{'value': '/pdf/62fc3766e10c6f5fa2f2a9b44b46098519f89596.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=oQKKlzxV1o,{'value': 'Online Information Acquisition: Hiring Multiple Agents'},"{'value': ['online learning', 'information acquisition', 'mechanism design']}","{'value': ""We investigate the mechanism design problem faced by a principal who hires \\emph{multiple} agents to gather and report costly information. Then, the principal exploits the  information to make an informed decision.  We model this problem as a game, where the principal announces a mechanism consisting in action recommendations and a payment function, a.k.a. scoring rule. Then, each agent chooses an effort level and receives partial information about an underlying state of nature based on the effort. Finally, the agents report the information (possibly non-truthfully), the principal takes a decision based on this information, and the agents are paid according to the scoring rule. While previous work focuses on single-agent problems, we consider multi-agents settings. This poses the challenge of coordinating the agents' efforts and aggregating correlated information. Indeed, we show that optimal mechanisms must correlate agents' efforts, which introduces externalities among the agents, and hence complex incentive compatibility constraints and equilibrium selection problems. First, we design a polynomial-time algorithm to find an optimal incentive compatible mechanism. Then, we study an online problem, where the principal repeatedly interacts with a group of unknown agents. We design a no-regret algorithm that provides $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret with respect to an optimal mechanism, matching the state-of-the-art bound for single-agent settings.""}",https://openreview.net{'value': '/pdf/83fcb8ac23e715a1b18621cd00f0a8a7d023dc7a.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=oKn9c6ytLx,{'value': 'WebArena: A Realistic Web Environment for Building Autonomous Agents'},{'value': ['language-guided agents; web automation; benchmark']},"{'value': 'With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting.  The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \\ours can be used to measure such progress.\\footnote{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.}'}",https://openreview.net{'value': '/pdf/384c500abb2b5adc4f3c40956a267477925c4b94.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=o7qhUMylLU,{'value': 'Sample-Efficient Multi-Agent RL: An Optimization Perspective'},"{'value': ['multi-agent reinforcement learning', 'theory', 'general function approximation', 'general-sum Markov Games']}","{'value': 'We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under general function approximation. \n    In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.'}",https://openreview.net{'value': '/pdf/fe5b0154a5ddbf71df6a0cd9077a345a6fa6065c.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=mM7VurbA4r,{'value': 'SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents'},"{'value': ['Social', 'Interaction', 'Agent', 'Social intelligence', 'Large Language Models', 'Evaluation', 'Theory of Mind']}","{'value': ""*Humans are social beings*; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and *interact* under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.""}",https://openreview.net{'value': '/pdf/6aece1f9088fc415196df5830f1ca62e6dbf37d3.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=hNhwSmtXRh,{'value': 'Lemur: Harmonizing Natural Language and Code for Language Agents'},"{'value': ['large language model', 'agent', 'code generation', 'reasoning', 'decision making']}","{'value': 'We introduce Lemur and Lemur-Chat, openly accessible language models optimized\nfor both natural language and coding capabilities to serve as the backbone\nof versatile language agents. The evolution from language chat models to\nfunctional language agents demands that models not only master human interaction,\nreasoning, and planning but also ensure grounding in the relevant environments.\nThis calls for a harmonious blend of language and coding capabilities\nin the models. Lemur and Lemur-Chat are proposed to address this necessity,\ndemonstrating balanced proficiencies in both domains, unlike existing\nopen-source models that tend to specialize in either. Through meticulous pretraining\nusing a code-intensive corpus and instruction fine-tuning on text and code\ndata, our models achieve state-of-the-art averaged performance across diverse\ntext and coding benchmarks. Comprehensive experiments demonstrate Lemur’s\nsuperiority over existing open-source models and its proficiency across various\nagent tasks involving human communication, tool usage, and interaction under\nfully- and partially- observable environments. The harmonization between natural\nand programming languages enables Lemur-Chat to significantly narrow the\ngap with proprietary models on agent abilities, providing key insights into developing\nadvanced open-source agents adept at reasoning, planning, and operating\nseamlessly across environments. Our model and code have been open-sourced at\nhttps://github.com/OpenLemur/Lemur.'}",https://openreview.net{'value': '/pdf/8ef62990871ebf2cac77dc6ea498085f167f070a.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Z59Rb5bPPP,{'value': 'Trajeglish: Traffic Modeling as Next-Token Prediction'},"{'value': ['self-driving', 'traffic modeling', 'autonomous vehicles', 'simulation', 'motion prediction', 'transformer']}","{'value': 'A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.'}",https://openreview.net{'value': '/pdf/6a1754e5443464338e361380403b4fec2f5d94b2.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=W3VsHuga3j,{'value': 'Modeling Boundedly Rational Agents with Latent Inference Budgets'},"{'value': ['neurosymbolic', 'planning', 'rationality']}","{'value': 'We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating constrained inference. In this work, we introduce a latent inference budget model (L-IBM) that models these constraints explicitly, via a latent variable (inferred jointly with a model of agents’ goals) that controls the runtime of an iterative inference algorithm. L-IBMs make it possible to learn agent models using data from diverse populations of suboptimal actors. In three modeling tasks—inferring navigation goals from routes, inferring communicative intents from human utterances, and predicting next moves in human chess games—we show that L-IBMs match or outperforms Boltzmann models of decision-making under uncertainty. Moreover, the inferred inference budgets are themselves meaningful, efficient to compute, and correlated with measures of player skill, partner skill and task difficulty.'}",https://openreview.net{'value': '/pdf/b6fcabc7a4521fa9b8ff72f6d09fbafc1e118951.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=VtmBAGCN7o,{'value': 'MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework'},"{'value': ['Autonomous Agent', 'Meta Programming', 'Multi-Agent Society', 'Group Intelligence']}","{'value': 'Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.'}",https://openreview.net{'value': '/pdf/474fc6dad3bd9bf7fdb97c7cd72b2cc0649a9647.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=UBVNwD3hPN,{'value': 'CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents'},"{'value': ['Interactive Environments', 'Benchmark', 'Reinforcement Learning', 'Language Agent', 'Multi-agent']}","{'value': 'The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization’s profound alignment with human society requires sophisticated learning and prior knowledge, while its ever-changing space and action space demand robust reasoning for generalization. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. Overall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.'}",https://openreview.net{'value': '/pdf/8e9b4884f752883a17e58942608d9cc769610a01.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Ts95eXsPBc,{'value': 'Spatially-Aware Transformers for Embodied Agents'},"{'value': ['Episodic Memory', 'Spatial Inference', 'Prediction', 'Generation', 'Reinforcement Learning']}","{'value': 'Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning. The source code for our models and experiments will be available at \\href{https://github.com/spatially_aware_transformer}{https://github.com/spatially_aware_transformer}.'}",https://openreview.net{'value': '/pdf/1dc0ff45874e872749597c38bcfc2df0fa7ed0d6.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=SBoRhRCzM3,{'value': 'THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS'},"{'value': ['LLM Complex Reasoning', 'Language Model Reasoning']}","{'value': 'Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. \nHowever, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}.\nTo address these issues, we propose \\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs.\nThese analogous problems are related to the input one, with reusable solutions and problem-solving strategies.\nThus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. \nTo achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. \nThen, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch.\nTP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. \nExperiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\\% improvement of human preference in Creative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent Planning.'}",https://openreview.net{'value': '/pdf/1cddcaa3f523f7ad1c9aea87d5180f2d173e4bb8.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=S2oTVrlcp3,{'value': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents'},{'value': ['Large Language Models']},"{'value': ""Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately.\nSmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. \nWe release our benchmark at https://github.com/microsoft/SmartPlay""}",https://openreview.net{'value': '/pdf/f84669704e333d9a0d57f5fd1dfd011b03109021.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Qox9rO0kN0,{'value': 'Learning Multi-Agent Communication from Graph Modeling Perspective'},"{'value': ['communication learning', 'multi-agent reinforcement learning']}","{'value': 'In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.'}",https://openreview.net{'value': '/pdf/b18ae005fc01451473ec4301d9802e5267831053.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=NltzxpG0nz,{'value': 'Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds'},"{'value': ['large multimodal pre-training', 'open-world embodied agent', 'large language model']}","{'value': ""Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks and carry out experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. The project’s website and code can be found at https://sites.google.com/view/steve-eye.""}",https://openreview.net{'value': '/pdf/a46ce05ff44a87fada32b918f094df26df3d2f07.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=MCNqgUFTHI,{'value': 'Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents'},"{'value': ['Dialogue Policy Planning', 'Proactive Dialogue', 'Large Language Model']}","{'value': 'Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.'}",https://openreview.net{'value': '/pdf/30c7e22635f23140a4f37883885b6e80b5f7c750.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=M6XWoEdmwf,{'value': 'AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents'},"{'value': ['Meta-RL', 'Generalization', 'Long-Term Memory', 'Transformers']}","{'value': ""We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments.""}",https://openreview.net{'value': '/pdf/6ffd1eb5dc0bd2b144d5d0309763b3ed5e114e8b.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=LjivA1SLZ6,{'value': 'Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning'},"{'value': ['Multi-agent reinforcement learning', 'episodic control', 'episodic incentive', 'state embedding']}","{'value': 'In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.'}",https://openreview.net{'value': '/pdf/8b2d5ac5539754d00bf99458a60c63157c74fbdb.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=KOZu91CzbK,{'value': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization'},"{'value': ['Language Agent', 'AI Agent', 'Reinforcement Learning']}","{'value': 'Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.'}",https://openreview.net{'value': '/pdf/d02f39256b41ec50ca8cd3a5b136065bc7a4caae.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=GEcwtMk1uA,{'value': 'Identifying the Risks of LM Agents with an LM-Emulated Sandbox'},"{'value': ['Language Model Agent', 'Tool Use', 'Evaluation', 'Safety', 'Language Model']}","{'value': 'Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes toolkits and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.'}",https://openreview.net{'value': '/pdf/d1601f78407737fc216de9e6ec0085038f8c885f.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=FQepisCUWu,{'value': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate'},"{'value': ['Large Language Model', 'Multi-Agent Debate', 'LLM evaluators']}","{'value': ""Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies.\nIn this paper, we construct a multi-agent referee team called $\\textbf{ChatEval}$ to autonomously discuss and evaluate the quality of different texts. \nOur experiments on two benchmarks illustrate that ChatEval delivers superior accuracy and correlation in alignment with human assessment. Furthermore, we find that the diverse role prompts (different personas) are essential in the multi-agent debate process; that is, utilizing the same role description in the prompts can lead to a degradation in performance. Our qualitative analysis also shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments.""}",https://openreview.net{'value': '/pdf/6950b70dde4f8d7425424dce9cc4063f505b2194.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EpVe8jAjdx,{'value': 'Privileged Sensing Scaffolds Reinforcement Learning'},"{'value': ['reinforcement learning', 'model-based reinforcement learning', 'world models', 'robotics', 'privileged information', 'asymmetric learning', 'multimodality', 'perception', 'sensing']}","{'value': 'We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon “sensory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose “Scaffolder”, a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new “S3” suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/'}",https://openreview.net{'value': '/pdf/8ab7ec56b6e56ac45ce2b68e95b277283dbb8377.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Ep0TtjVoap,{'value': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving'},"{'value': ['Large Language Models', 'Mathematical Reasoning', 'Tool Learning']}","{'value': ""Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.""}",https://openreview.net{'value': '/pdf/2b0b45b11d0f61912efb1a932fb494d36f7b88e6.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EnXJfQqy0K,{'value': 'Building Cooperative Embodied Agents Modularly with Large Language Models'},"{'value': ['Large Language Models', 'Embodied Intelligence', 'Multi-Agent Cooperation', 'Human-AI Interaction', 'Communication']}","{'value': 'In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.'}",https://openreview.net{'value': '/pdf/9baa34313f1aea292b16e30697c20766afadddb1.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EHg5GDnyq1,{'value': 'AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors'},"{'value': ['large language mode', 'agent', 'multi-agent']}","{'value': 'Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework AgentVerse that can effectively orchestrate a collaborative group of expert agents as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that AgentVerse can proficiently deploy multi-agent groups that outperform a single agent. Extensive experiments on text understanding, reasoning, coding, tool utilization, and embodied AI confirm the effectiveness of AgentVerse. Moreover, our analysis of agent interactions within AgentVerse reveals the emergence of specific collaborative behaviors, contributing to heightened group efficiency. We will release our codebase, AgentVerse, to further facilitate multi-agent research.'}",https://openreview.net{'value': '/pdf/7c9bd9a841a2ba0c11ea97abb5e982430c2fc95e.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=BqEvdOS1Hs,{'value': 'Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain'},"{'value': ['human enhancement', 'human-agent collaboration', 'game playing', 'deep reinforcement learning']}","{'value': 'Existing game AI research mainly focuses on enhancing agents\' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading to poor experiences for their human partners. In other words, most game AI agents are modeled in a ""self-centered"" manner. In this paper, we propose a ""human-centered"" modeling scheme for collaborative agents that aims to enhance the experience of humans. Specifically, we model the experience of humans as the goals they expect to achieve during the task. We expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents\' original abilities (e.g., winning games). To achieve this, we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG approach introduces a ""baseline"", which corresponds to the extent to which humans primitively achieve their goals, and encourages agents to learn behaviors that can effectively enhance humans in achieving their goals better. We evaluate the RLHG agent in the popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both objective performance and subjective preference results show that the RLHG agent provides participants better gaming experience.'}",https://openreview.net{'value': '/pdf/6ce678ca6108e4a85517e2526c83d7d968f7108b.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ADSxCpCu9s,{'value': 'LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents'},"{'value': ['task planning', 'language models', 'benchmarking', 'embodied agents', 'home robots']}","{'value': 'Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.'}",https://openreview.net{'value': '/pdf/35abce446ca9b6e7a9136f7c38556084c63538ec.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=9JQtrumvg8,"{'value': 'A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis'}","{'value': ['Web Navigation', 'Web Automation', 'Large Language Models', 'Language Model Agents', 'Tool Use', 'Program Synthesis']}","{'value': 'Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.\nHowever, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.\nWe introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.\nWebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.\nWe design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.\nWe empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.'}",https://openreview.net{'value': '/pdf/0b27823f96e3efd0ed6921aafc4fe4643d1aeec5.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=8HCARN2hhw,{'value': 'Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction'},"{'value': ['Navigation', 'Embodied AI', 'Perception']}","{'value': 'Agents navigating in 3D environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly defined interpretation, whereas classical robotics addresses this with scene reconstruction resulting in some form of map, usually estimated with geometry and sensor models and/or learning. In this work we propose to learn an actionable representation of the scene independently of the targeted downstream task and without explicitly optimizing reconstruction. The learned representation is optimized by a blind auxiliary agent trained to navigate with it on multiple short sub episodes branching out from a waypoint and, most importantly, without any direct visual observation. We argue and show that the blindness property is important and forces the (trained) latent representation to be the only means for planning. With probing experiments we show that the learned representation optimizes navigability and not reconstruction. On downstream tasks we show that it is robust to changes in distribution, in particular the sim2real gap, which we evaluate with a real physical robot in a real office building, significantly improving performance.'}",https://openreview.net{'value': '/pdf/2b5b56f45f498f5877f3de2ea54e150352e0a408.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=7M0EzjugaN,{'value': 'Online Continual Learning for Interactive Instruction Following Agents'},"{'value': ['Embodied AI', 'Continual Learning']}","{'value': 'In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic, since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous ‘data prior’ based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed challenging Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior arts in our empirical validations by noticeable margins.'}",https://openreview.net{'value': '/pdf/9bd1847dd40bdb80227f6ddd6480e1eee45a4999.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=22pyNMuIoa,{'value': 'PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization'},"{'value': ['Large Language Models', 'Expert-level Prompt Optimization', 'Strategic Planning']}","{'value': ""Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts.""}",https://openreview.net{'value': '/pdf/d4dbdee0d105cd3020bffdc2f56d99c33429d49c.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=zAdUB0aCTQ,{'value': 'AgentBench: Evaluating LLMs as Agents'},Xiao Liu; Hao Yu; Hanchen Zhang; Yifan Xu; Xuanyu Lei; Hanyu Lai; Yu Gu; Hangliang Ding; Kaiwen Men; Kejuan Yang; Shudan Zhang; Xiang Deng; Aohan Zeng; Zhengxiao Du; Chenhui Zhang; Sheng Shen; Tianjun Zhang; Yu Su; Huan Sun; Minlie Huang; Yuxiao Dong; Jie Tang,~Xiao_Liu15; ~Hao_Yu12; ~Hanchen_Zhang1; ~Yifan_Xu7; ~Xuanyu_Lei1; ~Hanyu_Lai2; ~Yu_Gu5; ~Hangliang_Ding1; ~Kaiwen_Men1; ~Kejuan_Yang1; ~Shudan_Zhang1; ~Xiang_Deng2; ~Aohan_Zeng1; ~Zhengxiao_Du1; ~Chenhui_Zhang1; ~Sheng_Shen2; ~Tianjun_Zhang1; ~Yu_Su2; ~Huan_Sun1; ~Minlie_Huang1; ~Yuxiao_Dong1; ~Jie_Tang1,"{'value': ['Large language models', 'Autonomous agents', 'Reasoning', 'Evaluation', 'Benchmark']}","{'value': ""The potential of Large Language Model (LLM) as agents has been widely acknowledged recently.\nThus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments.\nWe present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities.\nOur extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B.\nWe identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.\nImproving instruction following and training on high quality multi-round alignment data could improve agent performance.\nAnd different from existing assumptions, training on code present ambivalent impacts on different agent tasks.\nDatasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.""}",https://openreview.net{'value': '/pdf/6eee0bd1fd98c135372baedb2a5644233a013bb2.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=z6KS9D1dxt,{'value': 'Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game'},Simin Li; Jun Guo; Jingqiao Xiu; Ruixiao Xu; Xin Yu; Jiakai Wang; Aishan Liu; Yaodong Yang; Xianglong Liu,~Simin_Li5; ~Jun_Guo6; ~Jingqiao_Xiu1; ~Ruixiao_Xu1; ~Xin_Yu7; ~Jiakai_Wang1; ~Aishan_Liu1; ~Yaodong_Yang1; ~Xianglong_Liu3,"{'value': ['Multi-agent reinforcement learning', 'Robustness', 'Game Theory', 'Adversarial Attack']}","{'value': 'In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist and the corresponding policy weakly dominates previous approaches as time goes to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experiments on matrix game, Level-based Foraging and StarCraft II indicate that, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations, showing resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.'}",https://openreview.net{'value': '/pdf/ae82041a50b5244c3cc55d71579b529584e36982.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=yoVq2BGQdP,{'value': 'Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning'},Peizhong Ju; Arnob Ghosh; Ness Shroff,~Peizhong_Ju1; ~Arnob_Ghosh3; ~Ness_Shroff1,"{'value': ['Fairness', 'Multi-Agent Reinforcement Learning', 'Markov Decision Process']}","{'value': ""Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a  Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach.""}",https://openreview.net{'value': '/pdf/832d7ea57a154b15de9382f6f8db49dad34bc05e.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=wFWuX1Fhtj,{'value': 'On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning'},Ziyi Chen; Yi Zhou; Heng Huang,~Ziyi_Chen2; ~Yi_Zhou2; ~Heng_Huang1,"{'value': ['Multi-Agent Reinforcement Learning', 'Constrained reinforcement learning', 'Primal-Dual', 'Duality gap', 'Primal algorithm']}","{'value': 'Constrained cooperative multi-agent reinforcement learning (MARL) is an emerging learning framework that has been widely applied to manage multi-agent systems, and many primal-dual type algorithms have been developed for it. However, the convergence of primal-dual algorithms crucially relies on strong duality -- a condition that has not been formally proved in constrained cooperative MARL. In this work, we prove that strong duality fails to hold in constrained cooperative MARL, by revealing a nonconvex quadratic type constraint on the occupation measure induced by the product policy. Consequently, our reanalysis of the primal-dual algorithm shows that its convergence rate is hindered by the nonzero duality gap. Then, we propose a decentralized primal approach for constrained cooperative MARL to avoid the duality gap, and our analysis shows that its convergence is hindered by another gap induced by the advantage functions. Moreover, we compare these two types of algorithms via concrete examples, and show that neither of them always outperforms the other one. Our study reveals that constrained cooperative MARL is generally a challenging and highly nonconvex problem, and its fundamental structure is very different from that of single-agent constrained RL.'}",https://openreview.net{'value': '/pdf/fbb1a9eec51b42b14f0f37676f9db8b154cbc9d2.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=vZZ4hhniJU,{'value': 'Learning Multi-Agent Communication with Contrastive Learning'},Yat Long Lo; Biswa Sengupta; Jakob Nicolaus Foerster; Michael Noukhovitch,~Yat_Long_Lo1; ~Biswa_Sengupta5; ~Jakob_Nicolaus_Foerster1; ~Michael_Noukhovitch1,"{'value': ['Multi-Agent Reinforcement Learning', 'Emergent Communication', 'Contrastive Learning']}","{'value': 'Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.'}",https://openreview.net{'value': '/pdf/2c9dcdc69b3da56a7ebdec7279ee8bc8087b5c39.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=tmqOhBC4a5,{'value': 'Maximum Entropy Heterogeneous-Agent Reinforcement Learning'},Jiarong Liu; Yifan Zhong; Siyi Hu; Haobo Fu; QIANG FU; Xiaojun Chang; Yaodong Yang,~Jiarong_Liu1; ~Yifan_Zhong2; ~Siyi_Hu1; ~Haobo_Fu2; ~QIANG_FU8; ~Xiaojun_Chang4; ~Yaodong_Yang1,"{'value': ['cooperative multi-agent reinforcement learning', 'heterogeneous-agent soft actor-critic', 'maximum entropy heterogeneous-agent mirror learning']}","{'value': '*Multi-agent reinforcement learning* (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning \\emph{stochastic} policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose *Heterogeneous-Agent Soft Actor-Critic* (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to *quantal response equilibrium* (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named *Maximum Entropy Heterogeneous-Agent Mirror Learning* (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration.'}",https://openreview.net{'value': '/pdf/82bacc9b0a9551bf4922e43270f4c315044f70af.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=svIdLLZpsA,{'value': 'Real-Fake: Effective Training Data Synthesis Through Distribution Matching'},Jianhao Yuan; Jie Zhang; Shuyang Sun; Philip Torr; Bo Zhao,~Jianhao_Yuan2; ~Jie_Zhang14; ~Shuyang_Sun1; ~Philip_Torr1; ~Bo_Zhao4,"{'value': ['Training Data Synthesis', 'Distribution Matching', 'Data Efficiency']}","{'value': 'Synthetic training data has gained prominence in numerous learning tasks and scenarios, offering advantages such as dataset augmentation, generalization evalua\x02tion, and privacy preservation. Despite these benefits, the efficiency of synthetic data generated by\ncurrent methodologies remains inferior when training advanced deep models exclusively, limiting its practical utility. To address this challenge, we analyze the principles underlying training data synthesis for supervised learning and elucidate a principled theoretical framework from the distribution-matching perspective that explicates the mechanisms governing synthesis efficacy. Through extensive experiments, we demonstrate the effectiveness of our synthetic data across diverse image classification tasks, both as a replacement for and augmenta\x02tion to real datasets, while also benefits such as out-of-distribution generalization, privacy preservation, and scalability. Specifically, we achieve 70.9% top1 classification accuracy on ImageNet1K when training solely with synthetic data equivalent\nto 1 × the original real data size, which increases to 76.0% when scaling up to 10 × synthetic data.'}",https://openreview.net{'value': '/pdf/22c2e29e22c1cc145ff0d253589743fdd0e72267.pdf'},{'title_filter': 'Data Synthesis'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=sBQwvucduK,{'value': 'MagicDrive: Street View Generation with Diverse 3D Geometry Control'},Ruiyuan Gao; Kai Chen; Enze Xie; Lanqing HONG; Zhenguo Li; Dit-Yan Yeung; Qiang Xu,~Ruiyuan_Gao2; ~Kai_Chen11; ~Enze_Xie1; ~Lanqing_HONG1; ~Zhenguo_Li1; ~Dit-Yan_Yeung2; ~Qiang_Xu1,"{'value': ['diffusion models', 'street view generation', '3D control']}","{'value': ""Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework, offering diverse 3D geometry controls including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view image & video synthesis that captures nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV segmentation and 3D object detection. Project Website: https://flymin.github.io/magicdrive""}",https://openreview.net{'value': '/pdf/24ee27e06af8a9a4d217bf99e7d46340c5b078b0.pdf'},{'abstract_filter': 'Data Synthesis'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=s9z0HzWJJp,{'value': 'SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series'},Junyan Cheng; Peter Chin,~Junyan_Cheng1; ~Peter_Chin1,"{'value': ['Large Langauge Models', 'Agent', 'Prompt Tunning', 'Time series forcasting']}","{'value': 'We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called ""hyperportfolio"", that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to ""invest"". We also propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis & Proof prompting for producing in-depth analyses on input news, articles, etc. to assist decision-making. We perform experiments and ablation studies to explore the factors that impact performance. The results show that our proposed method achieves improvements of 32.4% and 30.4% compared to the state-of-the-art method in the two experimental settings.'}",https://openreview.net{'value': '/pdf/473beb76f232cb31d5e4d7594a17375a1347cef9.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=pAoqRlTBtY,{'value': 'Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning'},Ahmed Abdulaal; adamos hadjivasiliou; Nina Montana-Brown; Tiantian He; Ayodeji Ijishakin; Ivana Drobnjak; Daniel C. Castro; Daniel C. Alexander,~Ahmed_Abdulaal1; ~adamos_hadjivasiliou1; ~Nina_Montana-Brown1; tiantian.he.20@ucl.ac.uk; ~Ayodeji_Ijishakin1; ~Ivana_Drobnjak2; ~Daniel_C._Castro1; ~Daniel_C._Alexander1,"{'value': ['Causal Reasoning', 'Causal Discovery', 'Structural Causal Models', 'Large Language Models']}","{'value': ""Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.""}",https://openreview.net{'value': '/pdf/62fc3766e10c6f5fa2f2a9b44b46098519f89596.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=oQKKlzxV1o,{'value': 'Online Information Acquisition: Hiring Multiple Agents'},Federico Cacciamani; Matteo Castiglioni; Nicola Gatti,~Federico_Cacciamani1; ~Matteo_Castiglioni1; ~Nicola_Gatti1,"{'value': ['online learning', 'information acquisition', 'mechanism design']}","{'value': ""We investigate the mechanism design problem faced by a principal who hires \\emph{multiple} agents to gather and report costly information. Then, the principal exploits the  information to make an informed decision.  We model this problem as a game, where the principal announces a mechanism consisting in action recommendations and a payment function, a.k.a. scoring rule. Then, each agent chooses an effort level and receives partial information about an underlying state of nature based on the effort. Finally, the agents report the information (possibly non-truthfully), the principal takes a decision based on this information, and the agents are paid according to the scoring rule. While previous work focuses on single-agent problems, we consider multi-agents settings. This poses the challenge of coordinating the agents' efforts and aggregating correlated information. Indeed, we show that optimal mechanisms must correlate agents' efforts, which introduces externalities among the agents, and hence complex incentive compatibility constraints and equilibrium selection problems. First, we design a polynomial-time algorithm to find an optimal incentive compatible mechanism. Then, we study an online problem, where the principal repeatedly interacts with a group of unknown agents. We design a no-regret algorithm that provides $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret with respect to an optimal mechanism, matching the state-of-the-art bound for single-agent settings.""}",https://openreview.net{'value': '/pdf/83fcb8ac23e715a1b18621cd00f0a8a7d023dc7a.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=oKn9c6ytLx,{'value': 'WebArena: A Realistic Web Environment for Building Autonomous Agents'},Shuyan Zhou; Frank F. Xu; Hao Zhu; Xuhui Zhou; Robert Lo; Abishek Sridhar; Xianyi Cheng; Tianyue Ou; Yonatan Bisk; Daniel Fried; Uri Alon; Graham Neubig,~Shuyan_Zhou1; ~Frank_F._Xu1; ~Hao_Zhu1; ~Xuhui_Zhou1; ~Robert_Lo1; ~Abishek_Sridhar1; ~Xianyi_Cheng1; ~Tianyue_Ou1; ~Yonatan_Bisk1; ~Daniel_Fried1; ~Uri_Alon1; ~Graham_Neubig1,{'value': ['language-guided agents; web automation; benchmark']},"{'value': 'With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting.  The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \\ours can be used to measure such progress.\\footnote{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.}'}",https://openreview.net{'value': '/pdf/384c500abb2b5adc4f3c40956a267477925c4b94.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=o7qhUMylLU,{'value': 'Sample-Efficient Multi-Agent RL: An Optimization Perspective'},Nuoya Xiong; Zhihan Liu; Zhaoran Wang; Zhuoran Yang,~Nuoya_Xiong1; ~Zhihan_Liu1; ~Zhaoran_Wang1; ~Zhuoran_Yang1,"{'value': ['multi-agent reinforcement learning', 'theory', 'general function approximation', 'general-sum Markov Games']}","{'value': 'We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under general function approximation. \n    In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.'}",https://openreview.net{'value': '/pdf/fe5b0154a5ddbf71df6a0cd9077a345a6fa6065c.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=mM7VurbA4r,{'value': 'SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents'},Xuhui Zhou; Hao Zhu; Leena Mathur; Ruohong Zhang; Haofei Yu; Zhengyang Qi; Louis-Philippe Morency; Yonatan Bisk; Daniel Fried; Graham Neubig; Maarten Sap,~Xuhui_Zhou1; ~Hao_Zhu1; ~Leena_Mathur1; ~Ruohong_Zhang1; ~Haofei_Yu1; ~Zhengyang_Qi1; ~Louis-Philippe_Morency1; ~Yonatan_Bisk1; ~Daniel_Fried1; ~Graham_Neubig1; ~Maarten_Sap1,"{'value': ['Social', 'Interaction', 'Agent', 'Social intelligence', 'Large Language Models', 'Evaluation', 'Theory of Mind']}","{'value': ""*Humans are social beings*; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and *interact* under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.""}",https://openreview.net{'value': '/pdf/6aece1f9088fc415196df5830f1ca62e6dbf37d3.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=hNhwSmtXRh,{'value': 'Lemur: Harmonizing Natural Language and Code for Language Agents'},Yiheng Xu; Hongjin SU; Chen Xing; Boyu Mi; Qian Liu; Weijia Shi; Binyuan Hui; Fan Zhou; Yitao Liu; Tianbao Xie; Zhoujun Cheng; Siheng Zhao; Lingpeng Kong; Bailin Wang; Caiming Xiong; Tao Yu,~Yiheng_Xu1; ~Hongjin_SU1; ~Chen_Xing2; ~Boyu_Mi1; ~Qian_Liu2; ~Weijia_Shi1; ~Binyuan_Hui1; ~Fan_Zhou6; ~Yitao_Liu2; ~Tianbao_Xie1; ~Zhoujun_Cheng1; ~Siheng_Zhao1; ~Lingpeng_Kong1; ~Bailin_Wang3; ~Caiming_Xiong1; ~Tao_Yu5,"{'value': ['large language model', 'agent', 'code generation', 'reasoning', 'decision making']}","{'value': 'We introduce Lemur and Lemur-Chat, openly accessible language models optimized\nfor both natural language and coding capabilities to serve as the backbone\nof versatile language agents. The evolution from language chat models to\nfunctional language agents demands that models not only master human interaction,\nreasoning, and planning but also ensure grounding in the relevant environments.\nThis calls for a harmonious blend of language and coding capabilities\nin the models. Lemur and Lemur-Chat are proposed to address this necessity,\ndemonstrating balanced proficiencies in both domains, unlike existing\nopen-source models that tend to specialize in either. Through meticulous pretraining\nusing a code-intensive corpus and instruction fine-tuning on text and code\ndata, our models achieve state-of-the-art averaged performance across diverse\ntext and coding benchmarks. Comprehensive experiments demonstrate Lemur’s\nsuperiority over existing open-source models and its proficiency across various\nagent tasks involving human communication, tool usage, and interaction under\nfully- and partially- observable environments. The harmonization between natural\nand programming languages enables Lemur-Chat to significantly narrow the\ngap with proprietary models on agent abilities, providing key insights into developing\nadvanced open-source agents adept at reasoning, planning, and operating\nseamlessly across environments. Our model and code have been open-sourced at\nhttps://github.com/OpenLemur/Lemur.'}",https://openreview.net{'value': '/pdf/8ef62990871ebf2cac77dc6ea498085f167f070a.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Z59Rb5bPPP,{'value': 'Trajeglish: Traffic Modeling as Next-Token Prediction'},Jonah Philion; Xue Bin Peng; Sanja Fidler,~Jonah_Philion1; ~Xue_Bin_Peng1; ~Sanja_Fidler1,"{'value': ['self-driving', 'traffic modeling', 'autonomous vehicles', 'simulation', 'motion prediction', 'transformer']}","{'value': 'A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.'}",https://openreview.net{'value': '/pdf/6a1754e5443464338e361380403b4fec2f5d94b2.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=W3VsHuga3j,{'value': 'Modeling Boundedly Rational Agents with Latent Inference Budgets'},Athul Paul Jacob; Abhishek Gupta; Jacob Andreas,~Athul_Paul_Jacob1; ~Abhishek_Gupta1; ~Jacob_Andreas1,"{'value': ['neurosymbolic', 'planning', 'rationality']}","{'value': 'We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating constrained inference. In this work, we introduce a latent inference budget model (L-IBM) that models these constraints explicitly, via a latent variable (inferred jointly with a model of agents’ goals) that controls the runtime of an iterative inference algorithm. L-IBMs make it possible to learn agent models using data from diverse populations of suboptimal actors. In three modeling tasks—inferring navigation goals from routes, inferring communicative intents from human utterances, and predicting next moves in human chess games—we show that L-IBMs match or outperforms Boltzmann models of decision-making under uncertainty. Moreover, the inferred inference budgets are themselves meaningful, efficient to compute, and correlated with measures of player skill, partner skill and task difficulty.'}",https://openreview.net{'value': '/pdf/b6fcabc7a4521fa9b8ff72f6d09fbafc1e118951.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=VtmBAGCN7o,{'value': 'MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework'},Sirui Hong; Mingchen Zhuge; Jonathan Chen; Xiawu Zheng; Yuheng Cheng; Jinlin Wang; Ceyao Zhang; Zili Wang; Steven Ka Shing Yau; Zijuan Lin; Liyang Zhou; Chenyu Ran; Lingfeng Xiao; Chenglin Wu; Jürgen Schmidhuber,~Sirui_Hong1; ~Mingchen_Zhuge2; ~Jonathan_Chen3; ~Xiawu_Zheng1; ~Yuheng_Cheng1; ~Jinlin_Wang1; ~Ceyao_Zhang1; ~Zili_Wang1; ~Steven_Ka_Shing_Yau1; ~Zijuan_Lin1; ~Liyang_Zhou2; ~Chenyu_Ran1; ~Lingfeng_Xiao1; ~Chenglin_Wu2; ~Jürgen_Schmidhuber1,"{'value': ['Autonomous Agent', 'Meta Programming', 'Multi-Agent Society', 'Group Intelligence']}","{'value': 'Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.'}",https://openreview.net{'value': '/pdf/474fc6dad3bd9bf7fdb97c7cd72b2cc0649a9647.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=UBVNwD3hPN,{'value': 'CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents'},Siyuan Qi; Shuo Chen; Yexin Li; Xiangyu Kong; Junqi Wang; Bangcheng Yang; Pring Wong; Yifan Zhong; Xiaoyuan Zhang; Zhaowei Zhang; Nian Liu; Yaodong Yang; Song-Chun Zhu,~Siyuan_Qi1; ~Shuo_Chen3; ~Yexin_Li1; ~Xiangyu_Kong1; ~Junqi_Wang1; ~Bangcheng_Yang1; ~Pring_Wong2; ~Yifan_Zhong2; ~Xiaoyuan_Zhang3; ~Zhaowei_Zhang2; ~Nian_Liu4; ~Yaodong_Yang1; ~Song-Chun_Zhu1,"{'value': ['Interactive Environments', 'Benchmark', 'Reinforcement Learning', 'Language Agent', 'Multi-agent']}","{'value': 'The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization’s profound alignment with human society requires sophisticated learning and prior knowledge, while its ever-changing space and action space demand robust reasoning for generalization. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. Overall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.'}",https://openreview.net{'value': '/pdf/8e9b4884f752883a17e58942608d9cc769610a01.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Ts95eXsPBc,{'value': 'Spatially-Aware Transformers for Embodied Agents'},Junmo Cho; Jaesik Yoon; Sungjin Ahn,~Junmo_Cho1; ~Jaesik_Yoon1; ~Sungjin_Ahn1,"{'value': ['Episodic Memory', 'Spatial Inference', 'Prediction', 'Generation', 'Reinforcement Learning']}","{'value': 'Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning. The source code for our models and experiments will be available at \\href{https://github.com/spatially_aware_transformer}{https://github.com/spatially_aware_transformer}.'}",https://openreview.net{'value': '/pdf/1dc0ff45874e872749597c38bcfc2df0fa7ed0d6.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=SBoRhRCzM3,{'value': 'THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS'},Junchi Yu; Ran He; Zhitao Ying,~Junchi_Yu1; ~Ran_He1; ~Zhitao_Ying1,"{'value': ['LLM Complex Reasoning', 'Language Model Reasoning']}","{'value': 'Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. \nHowever, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}.\nTo address these issues, we propose \\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs.\nThese analogous problems are related to the input one, with reusable solutions and problem-solving strategies.\nThus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. \nTo achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. \nThen, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch.\nTP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. \nExperiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\\% improvement of human preference in Creative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent Planning.'}",https://openreview.net{'value': '/pdf/1cddcaa3f523f7ad1c9aea87d5180f2d173e4bb8.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=S2oTVrlcp3,{'value': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents'},Yue Wu; Xuan Tang; Tom Mitchell; Yuanzhi Li,~Yue_Wu17; ~Xuan_Tang2; ~Tom_Mitchell2; ~Yuanzhi_Li1,{'value': ['Large Language Models']},"{'value': ""Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately.\nSmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. \nWe release our benchmark at https://github.com/microsoft/SmartPlay""}",https://openreview.net{'value': '/pdf/f84669704e333d9a0d57f5fd1dfd011b03109021.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Qox9rO0kN0,{'value': 'Learning Multi-Agent Communication from Graph Modeling Perspective'},Shengchao Hu; Li Shen; Ya Zhang; Dacheng Tao,~Shengchao_Hu1; ~Li_Shen1; ~Ya_Zhang1; ~Dacheng_Tao1,"{'value': ['communication learning', 'multi-agent reinforcement learning']}","{'value': 'In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.'}",https://openreview.net{'value': '/pdf/b18ae005fc01451473ec4301d9802e5267831053.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=NltzxpG0nz,{'value': 'Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds'},Sipeng Zheng; jiazheng liu; Yicheng Feng; Zongqing Lu,~Sipeng_Zheng1; ~jiazheng_liu2; ~Yicheng_Feng1; ~Zongqing_Lu2,"{'value': ['large multimodal pre-training', 'open-world embodied agent', 'large language model']}","{'value': ""Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks and carry out experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. The project’s website and code can be found at https://sites.google.com/view/steve-eye.""}",https://openreview.net{'value': '/pdf/a46ce05ff44a87fada32b918f094df26df3d2f07.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=MCNqgUFTHI,{'value': 'Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents'},Yang Deng; Wenxuan Zhang; Wai Lam; See-Kiong Ng; Tat-Seng Chua,~Yang_Deng4; ~Wenxuan_Zhang1; ~Wai_Lam1; ~See-Kiong_Ng1; ~Tat-Seng_Chua2,"{'value': ['Dialogue Policy Planning', 'Proactive Dialogue', 'Large Language Model']}","{'value': 'Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.'}",https://openreview.net{'value': '/pdf/30c7e22635f23140a4f37883885b6e80b5f7c750.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=M6XWoEdmwf,{'value': 'AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents'},Jake Grigsby; Linxi Fan; Yuke Zhu,~Jake_Grigsby1; ~Linxi_Fan2; ~Yuke_Zhu1,"{'value': ['Meta-RL', 'Generalization', 'Long-Term Memory', 'Transformers']}","{'value': ""We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments.""}",https://openreview.net{'value': '/pdf/6ffd1eb5dc0bd2b144d5d0309763b3ed5e114e8b.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=LjivA1SLZ6,{'value': 'Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning'},Hyungho Na; Yunkyeong Seo; Il-chul Moon,~Hyungho_Na1; ~Yunkyeong_Seo1; ~Il-chul_Moon1,"{'value': ['Multi-agent reinforcement learning', 'episodic control', 'episodic incentive', 'state embedding']}","{'value': 'In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.'}",https://openreview.net{'value': '/pdf/8b2d5ac5539754d00bf99458a60c63157c74fbdb.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=KOZu91CzbK,{'value': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization'},Weiran Yao; Shelby Heinecke; Juan Carlos Niebles; Zhiwei Liu; Yihao Feng; Le Xue; Rithesh R N; Zeyuan Chen; Jianguo Zhang; Devansh Arpit; Ran Xu; Phil L Mui; Huan Wang; Caiming Xiong; Silvio Savarese,~Weiran_Yao1; ~Shelby_Heinecke1; ~Juan_Carlos_Niebles1; ~Zhiwei_Liu3; ~Yihao_Feng1; ~Le_Xue1; ~Rithesh_R_N1; ~Zeyuan_Chen1; ~Jianguo_Zhang3; ~Devansh_Arpit2; ~Ran_Xu1; ~Phil_L_Mui1; ~Huan_Wang1; ~Caiming_Xiong1; ~Silvio_Savarese1,"{'value': ['Language Agent', 'AI Agent', 'Reinforcement Learning']}","{'value': 'Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.'}",https://openreview.net{'value': '/pdf/d02f39256b41ec50ca8cd3a5b136065bc7a4caae.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=GEcwtMk1uA,{'value': 'Identifying the Risks of LM Agents with an LM-Emulated Sandbox'},Yangjun Ruan; Honghua Dong; Andrew Wang; Silviu Pitis; Yongchao Zhou; Jimmy Ba; Yann Dubois; Chris J. Maddison; Tatsunori Hashimoto,~Yangjun_Ruan1; ~Honghua_Dong1; ~Andrew_Wang4; ~Silviu_Pitis1; ~Yongchao_Zhou1; ~Jimmy_Ba1; ~Yann_Dubois1; ~Chris_J._Maddison1; ~Tatsunori_Hashimoto1,"{'value': ['Language Model Agent', 'Tool Use', 'Evaluation', 'Safety', 'Language Model']}","{'value': 'Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes toolkits and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.'}",https://openreview.net{'value': '/pdf/d1601f78407737fc216de9e6ec0085038f8c885f.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=FQepisCUWu,{'value': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate'},Chi-Min Chan; Weize Chen; Yusheng Su; Jianxuan Yu; Wei Xue; Shanghang Zhang; Jie Fu; Zhiyuan Liu,~Chi-Min_Chan1; ~Weize_Chen1; ~Yusheng_Su1; ~Jianxuan_Yu1; ~Wei_Xue5; ~Shanghang_Zhang4; ~Jie_Fu2; ~Zhiyuan_Liu1,"{'value': ['Large Language Model', 'Multi-Agent Debate', 'LLM evaluators']}","{'value': ""Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies.\nIn this paper, we construct a multi-agent referee team called $\\textbf{ChatEval}$ to autonomously discuss and evaluate the quality of different texts. \nOur experiments on two benchmarks illustrate that ChatEval delivers superior accuracy and correlation in alignment with human assessment. Furthermore, we find that the diverse role prompts (different personas) are essential in the multi-agent debate process; that is, utilizing the same role description in the prompts can lead to a degradation in performance. Our qualitative analysis also shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments.""}",https://openreview.net{'value': '/pdf/6950b70dde4f8d7425424dce9cc4063f505b2194.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EpVe8jAjdx,{'value': 'Privileged Sensing Scaffolds Reinforcement Learning'},Edward S. Hu; James Springer; Oleh Rybkin; Dinesh Jayaraman,~Edward_S._Hu1; ~James_Springer1; ~Oleh_Rybkin1; ~Dinesh_Jayaraman2,"{'value': ['reinforcement learning', 'model-based reinforcement learning', 'world models', 'robotics', 'privileged information', 'asymmetric learning', 'multimodality', 'perception', 'sensing']}","{'value': 'We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon “sensory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose “Scaffolder”, a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new “S3” suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/'}",https://openreview.net{'value': '/pdf/8ab7ec56b6e56ac45ce2b68e95b277283dbb8377.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Ep0TtjVoap,{'value': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving'},Zhibin Gou; Zhihong Shao; Yeyun Gong; yelong shen; Yujiu Yang; Minlie Huang; Nan Duan; Weizhu Chen,~Zhibin_Gou1; ~Zhihong_Shao1; ~Yeyun_Gong2; ~yelong_shen1; ~Yujiu_Yang2; ~Minlie_Huang1; ~Nan_Duan1; ~Weizhu_Chen1,"{'value': ['Large Language Models', 'Mathematical Reasoning', 'Tool Learning']}","{'value': ""Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.""}",https://openreview.net{'value': '/pdf/2b0b45b11d0f61912efb1a932fb494d36f7b88e6.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EnXJfQqy0K,{'value': 'Building Cooperative Embodied Agents Modularly with Large Language Models'},Hongxin Zhang; Weihua Du; Jiaming Shan; Qinhong Zhou; Yilun Du; Joshua B. Tenenbaum; Tianmin Shu; Chuang Gan,~Hongxin_Zhang1; ~Weihua_Du1; ~Jiaming_Shan1; ~Qinhong_Zhou1; ~Yilun_Du1; ~Joshua_B._Tenenbaum1; ~Tianmin_Shu1; ~Chuang_Gan1,"{'value': ['Large Language Models', 'Embodied Intelligence', 'Multi-Agent Cooperation', 'Human-AI Interaction', 'Communication']}","{'value': 'In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.'}",https://openreview.net{'value': '/pdf/9baa34313f1aea292b16e30697c20766afadddb1.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EHg5GDnyq1,{'value': 'AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors'},Weize Chen; Yusheng Su; Jingwei Zuo; Cheng Yang; Chenfei Yuan; Chi-Min Chan; Heyang Yu; Yaxi Lu; Yi-Hsin Hung; Chen Qian; Yujia Qin; Xin Cong; Ruobing Xie; Zhiyuan Liu; Maosong Sun; Jie Zhou,~Weize_Chen1; ~Yusheng_Su1; ~Jingwei_Zuo2; ~Cheng_Yang6; ~Chenfei_Yuan1; ~Chi-Min_Chan1; ~Heyang_Yu1; ~Yaxi_Lu1; ~Yi-Hsin_Hung1; ~Chen_Qian8; ~Yujia_Qin1; ~Xin_Cong1; ~Ruobing_Xie2; ~Zhiyuan_Liu1; ~Maosong_Sun1; ~Jie_Zhou8,"{'value': ['large language mode', 'agent', 'multi-agent']}","{'value': 'Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework AgentVerse that can effectively orchestrate a collaborative group of expert agents as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that AgentVerse can proficiently deploy multi-agent groups that outperform a single agent. Extensive experiments on text understanding, reasoning, coding, tool utilization, and embodied AI confirm the effectiveness of AgentVerse. Moreover, our analysis of agent interactions within AgentVerse reveals the emergence of specific collaborative behaviors, contributing to heightened group efficiency. We will release our codebase, AgentVerse, to further facilitate multi-agent research.'}",https://openreview.net{'value': '/pdf/7c9bd9a841a2ba0c11ea97abb5e982430c2fc95e.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=BqEvdOS1Hs,{'value': 'Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain'},Yiming Gao; Feiyu Liu; Liang Wang; Dehua Zheng; Zhenjie Lian; Weixuan Wang; Wenjin Yang; Siqin Li; Xianliang Wang; Wenhui Chen; Jing Dai; QIANG FU; Yang Wei; Lanxiao Huang; Wei Liu,~Yiming_Gao4; ~Feiyu_Liu1; ~Liang_Wang10; ~Dehua_Zheng1; ~Zhenjie_Lian1; ~Weixuan_Wang1; ~Wenjin_Yang2; ~Siqin_Li1; ~Xianliang_Wang1; ~Wenhui_Chen1; ~Jing_Dai2; ~QIANG_FU8; ~Yang_Wei2; ~Lanxiao_Huang1; ~Wei_Liu3,"{'value': ['human enhancement', 'human-agent collaboration', 'game playing', 'deep reinforcement learning']}","{'value': 'Existing game AI research mainly focuses on enhancing agents\' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading to poor experiences for their human partners. In other words, most game AI agents are modeled in a ""self-centered"" manner. In this paper, we propose a ""human-centered"" modeling scheme for collaborative agents that aims to enhance the experience of humans. Specifically, we model the experience of humans as the goals they expect to achieve during the task. We expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents\' original abilities (e.g., winning games). To achieve this, we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG approach introduces a ""baseline"", which corresponds to the extent to which humans primitively achieve their goals, and encourages agents to learn behaviors that can effectively enhance humans in achieving their goals better. We evaluate the RLHG agent in the popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both objective performance and subjective preference results show that the RLHG agent provides participants better gaming experience.'}",https://openreview.net{'value': '/pdf/6ce678ca6108e4a85517e2526c83d7d968f7108b.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ADSxCpCu9s,{'value': 'LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents'},Jae-Woo Choi; Youngwoo Yoon; Hyobin Ong; Jaehong Kim; Minsu Jang,~Jae-Woo_Choi1; ~Youngwoo_Yoon1; ~Hyobin_Ong1; ~Jaehong_Kim3; ~Minsu_Jang1,"{'value': ['task planning', 'language models', 'benchmarking', 'embodied agents', 'home robots']}","{'value': 'Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.'}",https://openreview.net{'value': '/pdf/35abce446ca9b6e7a9136f7c38556084c63538ec.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=9JQtrumvg8,"{'value': 'A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis'}",Izzeddin Gur; Hiroki Furuta; Austin V Huang; Mustafa Safdari; Yutaka Matsuo; Douglas Eck; Aleksandra Faust,~Izzeddin_Gur1; ~Hiroki_Furuta1; ~Austin_V_Huang1; ~Mustafa_Safdari1; ~Yutaka_Matsuo1; ~Douglas_Eck1; ~Aleksandra_Faust1,"{'value': ['Web Navigation', 'Web Automation', 'Large Language Models', 'Language Model Agents', 'Tool Use', 'Program Synthesis']}","{'value': 'Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.\nHowever, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.\nWe introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.\nWebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.\nWe design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.\nWe empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.'}",https://openreview.net{'value': '/pdf/0b27823f96e3efd0ed6921aafc4fe4643d1aeec5.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=8HCARN2hhw,{'value': 'Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction'},Guillaume Bono; Leonid Antsfeld; Assem Sadek; Gianluca Monaci; Christian Wolf,~Guillaume_Bono1; ~Leonid_Antsfeld1; ~Assem_Sadek1; ~Gianluca_Monaci1; ~Christian_Wolf5,"{'value': ['Navigation', 'Embodied AI', 'Perception']}","{'value': 'Agents navigating in 3D environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly defined interpretation, whereas classical robotics addresses this with scene reconstruction resulting in some form of map, usually estimated with geometry and sensor models and/or learning. In this work we propose to learn an actionable representation of the scene independently of the targeted downstream task and without explicitly optimizing reconstruction. The learned representation is optimized by a blind auxiliary agent trained to navigate with it on multiple short sub episodes branching out from a waypoint and, most importantly, without any direct visual observation. We argue and show that the blindness property is important and forces the (trained) latent representation to be the only means for planning. With probing experiments we show that the learned representation optimizes navigability and not reconstruction. On downstream tasks we show that it is robust to changes in distribution, in particular the sim2real gap, which we evaluate with a real physical robot in a real office building, significantly improving performance.'}",https://openreview.net{'value': '/pdf/2b5b56f45f498f5877f3de2ea54e150352e0a408.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=7M0EzjugaN,{'value': 'Online Continual Learning for Interactive Instruction Following Agents'},Byeonghwi Kim; Minhyuk Seo; Jonghyun Choi,~Byeonghwi_Kim1; ~Minhyuk_Seo1; ~Jonghyun_Choi1,"{'value': ['Embodied AI', 'Continual Learning']}","{'value': 'In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic, since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous ‘data prior’ based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed challenging Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior arts in our empirical validations by noticeable margins.'}",https://openreview.net{'value': '/pdf/9bd1847dd40bdb80227f6ddd6480e1eee45a4999.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=7ERQPyR2eb,{'value': 'Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis'},Zhenhui Ye; Tianyun Zhong; Yi Ren; Jiaqi Yang; Weichuang Li; Jiawei Huang; Ziyue Jiang; Jinzheng He; Rongjie Huang; Jinglin Liu; Chen Zhang; Xiang Yin; Zejun MA; Zhou Zhao,~Zhenhui_Ye1; ~Tianyun_Zhong3; ~Yi_Ren2; ~Jiaqi_Yang8; ~Weichuang_Li1; ~Jiawei_Huang5; ~Ziyue_Jiang1; ~Jinzheng_He1; ~Rongjie_Huang1; ~Jinglin_Liu1; ~Chen_Zhang3; ~Xiang_Yin2; ~Zejun_MA1; ~Zhou_Zhao3,"{'value': ['One-shot Talking Face Generation', 'Neural Radiance Field']}","{'value': 'One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable talking face animation. Besides, while the existing works mainly focus on synthesizing the head part, it is also vital to generate natural torso and background segments to obtain a realistic talking portrait video. To address these limitations, we present Real3D-Potrait, a framework that (1) improves the one-shot 3D reconstruction power with a large image-to-plane model that distills 3D prior knowledge from a 3D face generative model; (2) facilitates accurate motion-conditioned animation with an efficient motion adapter; (3) synthesizes realistic video with natural torso movement and switchable background using a head-torso-background super-resolution model; and (4) supports one-shot audio-driven talking face generation with a generalizable audio-to-motion model. Extensive experiments show that Real3D-Portrait generalizes well to unseen identities and generates more realistic talking portrait videos compared to previous methods. Video samples are available at https://real3dportrait.github.io.'}",https://openreview.net{'value': '/pdf/c59a7b868258573cafe7e9a84243e3f096008b51.pdf'},{'title_filter': 'Data Synthesis'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=4Ay23yeuz0,{'value': 'Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space'},Hengrui Zhang; Jiani Zhang; Zhengyuan Shen; Balasubramaniam Srinivasan; Xiao Qin; Christos Faloutsos; Huzefa Rangwala; George Karypis,~Hengrui_Zhang1; ~Jiani_Zhang2; ~Zhengyuan_Shen1; ~Balasubramaniam_Srinivasan1; ~Xiao_Qin3; ~Christos_Faloutsos1; ~Huzefa_Rangwala2; ~George_Karypis1,"{'value': ['Tabular data', 'tabular generation', 'diffusion models']}","{'value': 'Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces TabSyn, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed Tabsyn include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capturing inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data; (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that Tabsyn outperforms existing methods. Specifically, it reduces the error rates by 86% and 67% for column-wise distribution and pair-wise column correlation estimations compared with the most competitive baselines. The code has been made available at https://github.com/amazon-science/tabsyn.'}",https://openreview.net{'value': '/pdf/a916d9616f8be0fc9c47c323b6afe8398acf898d.pdf'},{'title_filter': 'Data Synthesis'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=22pyNMuIoa,{'value': 'PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization'},Xinyuan Wang; Chenxi Li; Zhen Wang; Fan Bai; Haotian Luo; Jiayou Zhang; Nebojsa Jojic; Eric Xing; Zhiting Hu,xiw136@ucsd.edu; chl078@ucsd.edu; ~Zhen_Wang6; ~Fan_Bai5; 1203616626@sjtu.edu.cn; ~Jiayou_Zhang1; ~Nebojsa_Jojic1; ~Eric_Xing1; ~Zhiting_Hu3,"{'value': ['Large Language Models', 'Expert-level Prompt Optimization', 'Strategic Planning']}","{'value': ""Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts.""}",https://openreview.net{'value': '/pdf/d4dbdee0d105cd3020bffdc2f56d99c33429d49c.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=zwU9scoU4A,{'value': 'Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach'},Christian Fabian; Kai Cui; Heinz Koeppl,~Christian_Fabian1; ~Kai_Cui3; ~Heinz_Koeppl1,"{'value': ['Mean Field Games', 'Equilibrium Learning', 'Networks', 'Large Graphs', 'Multi Agent Systems']}","{'value': 'Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.'}",https://openreview.net{'value': '/pdf/71668b85e315060e15202cddaf4245a5fb813cae.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ziDFH8TPPK,{'value': 'Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data'},Young-Jae Park; Minseok Seo; Doyi Kim; Hyeri Kim; Sanghoon Choi; Beomkyu Choi; Jeongwon Ryu; Sohee Son; Hae-Gon Jeon; Yeji Choi,~Young-Jae_Park1; ~Minseok_Seo1; ~Doyi_Kim1; ~Hyeri_Kim1; ~Sanghoon_Choi1; ~Beomkyu_Choi1; ~Jeongwon_Ryu1; ~Sohee_Son2; ~Hae-Gon_Jeon3; ~Yeji_Choi1,"{'value': ['Weather Forecasting', 'Typhoon Trajectory Forecasting', 'Tropical Cyclone', 'Climate Change']}","{'value': 'In the face of escalating climate changes, typhoon intensities and their ensuing damage have surged. Accurate trajectory prediction is crucial for effective damage control. Traditional physics-based models, while comprehensive, are computationally intensive and rely heavily on the expertise of forecasters. Contemporary data-driven methods often rely on reanalysis data, which can be considered to be the closest to the true representation of weather conditions. However, reanalysis data is not produced in real-time and requires time for adjustment since prediction models are calibrated with observational data. This reanalysis data, such as ERA5, falls short in challenging real-world situations. Optimal preparedness necessitates predictions at least 72 hours in advance, beyond the capabilities of standard physics models. In response to these constraints, we present an approach that harnesses real-time Unified Model (UM) data, sidestepping the limitations of reanalysis data. Our model provides predictions at 6-hour intervals for up to 72 hours in advance and outperforms both state-of-the-art data-driven methods and numerical weather prediction models. In line with our efforts to mitigate adversities inflicted by \\rthree{typhoons}, we release our preprocessed \\textit{PHYSICS TRACK} dataset, which includes ERA5 reanalysis data, typhoon best-track, and UM forecast data.'}",https://openreview.net{'value': '/pdf/3f35815328b8f1ce4a016bc05e882d4f298a97f8.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=zAdUB0aCTQ,{'value': 'AgentBench: Evaluating LLMs as Agents'},Xiao Liu; Hao Yu; Hanchen Zhang; Yifan Xu; Xuanyu Lei; Hanyu Lai; Yu Gu; Hangliang Ding; Kaiwen Men; Kejuan Yang; Shudan Zhang; Xiang Deng; Aohan Zeng; Zhengxiao Du; Chenhui Zhang; Sheng Shen; Tianjun Zhang; Yu Su; Huan Sun; Minlie Huang; Yuxiao Dong; Jie Tang,~Xiao_Liu15; ~Hao_Yu12; ~Hanchen_Zhang1; ~Yifan_Xu7; ~Xuanyu_Lei1; ~Hanyu_Lai2; ~Yu_Gu5; ~Hangliang_Ding1; ~Kaiwen_Men1; ~Kejuan_Yang1; ~Shudan_Zhang1; ~Xiang_Deng2; ~Aohan_Zeng1; ~Zhengxiao_Du1; ~Chenhui_Zhang1; ~Sheng_Shen2; ~Tianjun_Zhang1; ~Yu_Su2; ~Huan_Sun1; ~Minlie_Huang1; ~Yuxiao_Dong1; ~Jie_Tang1,"{'value': ['Large language models', 'Autonomous agents', 'Reasoning', 'Evaluation', 'Benchmark']}","{'value': ""The potential of Large Language Model (LLM) as agents has been widely acknowledged recently.\nThus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments.\nWe present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities.\nOur extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B.\nWe identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.\nImproving instruction following and training on high quality multi-round alignment data could improve agent performance.\nAnd different from existing assumptions, training on code present ambivalent impacts on different agent tasks.\nDatasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.""}",https://openreview.net{'value': '/pdf/6eee0bd1fd98c135372baedb2a5644233a013bb2.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=z6KS9D1dxt,{'value': 'Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game'},Simin Li; Jun Guo; Jingqiao Xiu; Ruixiao Xu; Xin Yu; Jiakai Wang; Aishan Liu; Yaodong Yang; Xianglong Liu,~Simin_Li5; ~Jun_Guo6; ~Jingqiao_Xiu1; ~Ruixiao_Xu1; ~Xin_Yu7; ~Jiakai_Wang1; ~Aishan_Liu1; ~Yaodong_Yang1; ~Xianglong_Liu3,"{'value': ['Multi-agent reinforcement learning', 'Robustness', 'Game Theory', 'Adversarial Attack']}","{'value': 'In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist and the corresponding policy weakly dominates previous approaches as time goes to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experiments on matrix game, Level-based Foraging and StarCraft II indicate that, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations, showing resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.'}",https://openreview.net{'value': '/pdf/ae82041a50b5244c3cc55d71579b529584e36982.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=yoVq2BGQdP,{'value': 'Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning'},Peizhong Ju; Arnob Ghosh; Ness Shroff,~Peizhong_Ju1; ~Arnob_Ghosh3; ~Ness_Shroff1,"{'value': ['Fairness', 'Multi-Agent Reinforcement Learning', 'Markov Decision Process']}","{'value': ""Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a  Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach.""}",https://openreview.net{'value': '/pdf/832d7ea57a154b15de9382f6f8db49dad34bc05e.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ymjI8feDTD,{'value': 'Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion'},Dongjun Kim; Chieh-Hsin Lai; Wei-Hsiang Liao; Naoki Murata; Yuhta Takida; Toshimitsu Uesaka; Yutong He; Yuki Mitsufuji; Stefano Ermon,~Dongjun_Kim1; ~Chieh-Hsin_Lai2; ~Wei-Hsiang_Liao1; ~Naoki_Murata1; ~Yuhta_Takida1; ~Toshimitsu_Uesaka1; ~Yutong_He1; ~Yuki_Mitsufuji1; ~Stefano_Ermon1,{'value': ['Diffusion Models; Score-based Models; Generative Models; Generative AI; Deep Generative Models; Distillation Models']},"{'value': ""Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 1.92). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's access to the score function can streamline the adoption of established controllable/conditional generation methods from the diffusion community. This access also enables the computation of likelihood. The code is available at https://github.com/sony/ctm.""}",https://openreview.net{'value': '/pdf/f46141686c6fc27b73793bb443c11c61e7dc87b8.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=yN4Wv17ss3,{'value': 'Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining'},Licong Lin; Yu Bai; Song Mei,~Licong_Lin2; ~Yu_Bai1; ~Song_Mei1,"{'value': ['transformers', 'in-context learning', 'reinforcement learning', 'learning theory']}","{'value': 'Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. \n\nThis paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods --- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.'}",https://openreview.net{'value': '/pdf/70a651da16858dd6e579e700fefb2f9ecccf9d17.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=y21ZO6M86t,{'value': 'PolyGCL: GRAPH CONTRASTIVE LEARNING via Learnable Spectral Polynomial Filters'},Jingyu Chen; Runlin Lei; Zhewei Wei,~Jingyu_Chen4; ~Runlin_Lei1; ~Zhewei_Wei1,"{'value': ['Graph Contrastive Learning', 'Spectral Graph Neural Networks', 'Polynomial Filter', 'Heterophilic Graph Representation Learning']}","{'value': 'Recently, Graph Contrastive Learning (GCL) has achieved significantly superior performance in self-supervised graph representation learning. \nHowever, the existing GCL technique has inherent smooth characteristics because of its low-pass GNN encoder and objective based on homophily assumption, which poses a challenge when applying it to heterophilic graphs.\nIn supervised learning tasks, spectral GNNs with polynomial approximation excel in both homophilic and heterophilic settings by adaptively fitting graph filters of arbitrary shapes. \nYet, their applications in unsupervised learning are rarely explored.\nBased on the above analysis, a natural question arises: Can we incorporate the excellent properties of spectral polynomial filters into graph contrastive learning?\nIn this paper, we address the question by studying the necessity of introducing high-pass information for heterophily from a spectral perspective.\nWe propose PolyGCL, a GCL pipeline that utilizes polynomial filters to achieve contrastive learning between the low-pass and high-pass views.\nSpecifically, PolyGCL utilizes polynomials with learnable filter functions to generate different spectral views and an objective that incorporates high-pass information through a linear combination. \nWe theoretically prove that PolyGCL outperforms previous GCL paradigms when applied to graphs with varying levels of homophily.\nWe conduct extensive experiments on both synthetic and real-world datasets, which demonstrate the promising performance of PolyGCL on homophilic and heterophilic graphs.'}",https://openreview.net{'value': '/pdf/e0bdb5536d418b614a12c003721153c1e6fbaf4b.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=xriGRsoAza,{'value': 'Inherently Interpretable Time Series Classification via Multiple Instance Learning'},Joseph Early; Gavin Cheung; Kurt Cutajar; Hanting Xie; Jas Kandola; Niall Twomey,~Joseph_Early1; ~Gavin_Cheung1; ~Kurt_Cutajar1; ~Hanting_Xie1; ~Jas_Kandola1; ~Niall_Twomey1,"{'value': ['Multiple Instance Learning', 'Time Series Classification', 'Interpretability']}","{'value': 'Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with MILLET is the first to develop general MIL methods for TSC and apply them to an extensive variety of domains.'}",https://openreview.net{'value': '/pdf/b63fbc2c94da98976eead6f787efe4df88f0e8a7.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=xpw7V0P136,{'value': 'Teaching Language Models to Hallucinate Less with Synthetic Tasks'},Erik Jones; Hamid Palangi; Clarisse Simões Ribeiro; Varun Chandrasekaran; Subhabrata Mukherjee; Arindam Mitra; Ahmed Hassan Awadallah; Ece Kamar,~Erik_Jones3; ~Hamid_Palangi1; ~Clarisse_Simões_Ribeiro1; ~Varun_Chandrasekaran1; ~Subhabrata_Mukherjee2; ~Arindam_Mitra1; ~Ahmed_Hassan_Awadallah1; ~Ece_Kamar1,"{'value': ['hallucination', 'large language models', 'synthetic data', 'prefix tuning', 'safety']}","{'value': ""Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing to make LLMs hallucinate less is challenging, as hallucination is hard to efficiently, cheaply, and reliably evaluate at each optimization step. In this work, we show that reducing hallucination on a _synthetic task_ can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix tuning on the synthetic task, then uses the system message on realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, we reduce hallucination for two 13B-parameter LLMs using supervision signal from only a synthetic retrieval task. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively _increase_ hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.""}",https://openreview.net{'value': '/pdf/97af4a6f1720acbe37f8a543a0246b3e97778b76.pdf'},{'title_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=xhEN0kJh4q,{'value': 'Robust Model-Based Optimization for Challenging Fitness Landscapes'},Saba Ghaffari; Ehsan Saleh; Alex Schwing; Yu-Xiong Wang; Martin D. Burke; Saurabh Sinha,~Saba_Ghaffari1; ~Ehsan_Saleh1; ~Alex_Schwing1; ~Yu-Xiong_Wang1; ~Martin_D._Burke1; ~Saurabh_Sinha4,"{'value': ['Model based optimization', 'protein engineering']}","{'value': 'Protein design, a grand challenge of the day, involves optimization on a fitness landscape, and leading methods adopt a model-based approach where a model is trained on a training set (protein sequences and fitness) and proposes candidates to explore next. These methods are challenged by sparsity of high-fitness samples in the training set, a problem that has been in the literature. A less recognized but equally important problem stems from the distribution of training samples in the design space: leading methods are not designed for scenarios where the desired optimum is in a region that is not only poorly represented in training data, but also relatively far from the highly represented low-fitness regions. We show that this problem of “separation” in the design space is a significant bottleneck in existing model-based optimization tools and propose a new approach that uses a novel VAE as its search model to overcome the problem. We demonstrate its advantage over prior methods in robustly finding improved samples, regardless of the imbalance and separation between low- and high-fitness samples. Our comprehensive benchmark on real and semi-synthetic protein datasets as well as solution design for physics-informed neural networks, showcases the generality of our approach in discrete and continuous design spaces. Our implementation is available at https://github.com/sabagh1994/PGVAE.'}",https://openreview.net{'value': '/pdf/fb0639e5f082d1a2381a60dc9bb079cb3f5de709.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=xJbsmB8UMx,{'value': 'SALMON: Self-Alignment with Instructable Reward Models'},Zhiqing Sun; Yikang Shen; Hongxin Zhang; Qinhong Zhou; Zhenfang Chen; David Daniel Cox; Yiming Yang; Chuang Gan,~Zhiqing_Sun1; ~Yikang_Shen1; ~Hongxin_Zhang1; ~Qinhong_Zhou1; ~Zhenfang_Chen1; ~David_Daniel_Cox1; ~Yiming_Yang1; ~Chuang_Gan1,"{'value': ['AI Alignment', 'Large Language Models', 'Scalable Oversight']}","{'value': 'Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON, to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is an instructable reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the instructable reward model, subsequently influencing the behavior of the RL-trained policy models, and reducing the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.'}",https://openreview.net{'value': '/pdf/04557b575ab9a976ec3055c2028ee79850aab8b7.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=xEJMoj1SpX,{'value': 'Elucidating the Exposure Bias in Diffusion Models'},Mang Ning; Mingxiao Li; Jianlin Su; Albert Ali Salah; Itir Onal Ertugrul,~Mang_Ning1; ~Mingxiao_Li1; ~Jianlin_Su1; ~Albert_Ali_Salah2; ~Itir_Onal_Ertugrul3,"{'value': ['Generative models', 'Diffusion models']}","{'value': 'Diffusion models have demonstrated impressive generative capabilities, but their exposure bias problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output, mitigating the input mismatch between training and sampling. Experiments on various diffusion frameworks (ADM, DDIM, EDM, LDM, DiT, PFGM++) verify the effectiveness of our method. Remarkably, our ADM-ES, as a state-of-the-art stochastic sampler, obtains 2.17 FID on CIFAR-10 under 100-step unconditional generation. The code is at https://github.com/forever208/ADM-ES'}",https://openreview.net{'value': '/pdf/23c45aa3b51fc2ab122bb8b5df43097416deae04.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=wFf9m4v7oC,{'value': 'Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder'},Ziqi Xu; Debo Cheng; Jiuyong Li; Jixue Liu; Lin Liu; Kui Yu,~Ziqi_Xu1; ~Debo_Cheng1; ~Jiuyong_Li1; ~Jixue_Liu1; ~Lin_Liu4; ~Kui_Yu2,"{'value': ['Causal Inference', 'Variational AutoEncoder', 'Front Door Criterion']}","{'value': 'An essential and challenging problem in causal inference is causal effect estimation from observational data. The problem becomes more difficult with the presence of unobserved confounding variables. The front-door adjustment is an approach for dealing with unobserved confounding variables. However, the restriction for the standard front-door adjustment is difficult to satisfy in practice. In this paper, we relax some of the restrictions by proposing the concept of conditional front-door (CFD) adjustment and develop the theorem that guarantees the causal effect identifiability of CFD adjustment. By leveraging the ability of deep generative models, we propose CFDiVAE to learn the representation of the CFD adjustment variable directly from data with the identifiable Variational AutoEncoder and formally prove the model identifiability. Extensive experiments on synthetic datasets validate the effectiveness of CFDiVAE and its superiority over existing methods. The experiments also show that the performance of CFDiVAE is less sensitive to the causal strength of unobserved confounding variables. We further apply CFDiVAE to a real-world dataset to demonstrate its potential application.'}",https://openreview.net{'value': '/pdf/61971822380ee8ab8823422eec90ea99e5dcecce.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=wFWuX1Fhtj,{'value': 'On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning'},Ziyi Chen; Yi Zhou; Heng Huang,~Ziyi_Chen2; ~Yi_Zhou2; ~Heng_Huang1,"{'value': ['Multi-Agent Reinforcement Learning', 'Constrained reinforcement learning', 'Primal-Dual', 'Duality gap', 'Primal algorithm']}","{'value': 'Constrained cooperative multi-agent reinforcement learning (MARL) is an emerging learning framework that has been widely applied to manage multi-agent systems, and many primal-dual type algorithms have been developed for it. However, the convergence of primal-dual algorithms crucially relies on strong duality -- a condition that has not been formally proved in constrained cooperative MARL. In this work, we prove that strong duality fails to hold in constrained cooperative MARL, by revealing a nonconvex quadratic type constraint on the occupation measure induced by the product policy. Consequently, our reanalysis of the primal-dual algorithm shows that its convergence rate is hindered by the nonzero duality gap. Then, we propose a decentralized primal approach for constrained cooperative MARL to avoid the duality gap, and our analysis shows that its convergence is hindered by another gap induced by the advantage functions. Moreover, we compare these two types of algorithms via concrete examples, and show that neither of them always outperforms the other one. Our study reveals that constrained cooperative MARL is generally a challenging and highly nonconvex problem, and its fundamental structure is very different from that of single-agent constrained RL.'}",https://openreview.net{'value': '/pdf/fbb1a9eec51b42b14f0f37676f9db8b154cbc9d2.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=vkkHqoerLV,{'value': 'Alice Benchmarks: Connecting Real World Re-Identification with the Synthetic'},Xiaoxiao Sun; Yue Yao; Shengjin Wang; Hongdong Li; Liang Zheng,~Xiaoxiao_Sun1; ~Yue_Yao1; ~Shengjin_Wang1; ~Hongdong_Li1; ~Liang_Zheng4,"{'value': ['Synthetic Data', 'Object re-ID', 'Benchmarks']}","{'value': 'For object re-identification (re-ID), learning from synthetic data has become a promising strategy to cheaply acquire large-scale annotated datasets and effective models, with few privacy concerns. Many interesting research problems arise from this strategy, e.g., how to reduce the domain gap between synthetic source and real-world target. To facilitate developing more new approaches in learning from synthetic data, we introduce the Alice benchmarks, large-scale datasets providing benchmarks as well as evaluation protocols to the research community. Within the Alice benchmarks, two object re-ID tasks are offered: person and vehicle re-ID. We collected and annotated two challenging real-world target datasets: AlicePerson and AliceVehicle, captured under various illuminations, image resolutions, etc. As an important feature of our real target, the clusterability of its training set is not manually guaranteed to make it closer to a real domain adaptation test scenario. Correspondingly, we reuse existing PersonX and VehicleX as synthetic source domains. The primary goal is to train models from synthetic data that can work effectively in the real world. In this paper, we detail the settings of Alice benchmarks, provide an analysis of existing commonly-used domain adaptation methods, and discuss some interesting future directions. An online server has been set up for the community to evaluate methods conveniently and fairly. Datasets and the online server details are available at https://sites.google.com/view/alice-benchmarks.'}",https://openreview.net{'value': '/pdf/b35efd7b4482c9cea9bc862179b522b5782d15d3.pdf'},{'title_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=vZZ4hhniJU,{'value': 'Learning Multi-Agent Communication with Contrastive Learning'},Yat Long Lo; Biswa Sengupta; Jakob Nicolaus Foerster; Michael Noukhovitch,~Yat_Long_Lo1; ~Biswa_Sengupta5; ~Jakob_Nicolaus_Foerster1; ~Michael_Noukhovitch1,"{'value': ['Multi-Agent Reinforcement Learning', 'Emergent Communication', 'Contrastive Learning']}","{'value': 'Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.'}",https://openreview.net{'value': '/pdf/2c9dcdc69b3da56a7ebdec7279ee8bc8087b5c39.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=vXrIQLzIKY,{'value': 'Xformer: Hybrid X-Shaped Transformer for Image Denoising'},Jiale Zhang; Yulun Zhang; Jinjin Gu; Jiahua Dong; Linghe Kong; Xiaokang Yang,~Jiale_Zhang3; ~Yulun_Zhang1; ~Jinjin_Gu1; ~Jiahua_Dong1; ~Linghe_Kong1; ~Xiaokang_Yang1,"{'value': ['Image denoising', 'hybrid Transformer blocks']}","{'value': 'In this paper, we present a hybrid X-shaped vision Transformer, named Xformer, which performs notably on image denoising tasks. We explore strengthening the global representation of tokens from different scopes. In detail, we adopt two types of Transformer blocks. The spatial-wise Transformer block performs fine-grained local patches interactions across tokens defined by spatial dimension. The channel-wise Transformer block performs direct global context interactions across tokens defined by channel dimension. Based on the concurrent network structure, we design two branches to conduct these two interaction fashions. Within each branch, we employ an encoder-decoder architecture to capture multi-scale features. Besides, we propose the Bidirectional Connection Unit (BCU) to couple the learned representations from these two branches while providing enhanced information fusion. The joint designs make our Xformer powerful to conduct global information modeling in both spatial and channel dimensions. Extensive experiments show that Xformer, under the comparable model complexity, achieves state-of-the-art performance on the synthetic and real-world image denoising tasks. We also provide code and models at https://github.com/gladzhang/Xformer.'}",https://openreview.net{'value': '/pdf/d4e56a2cae8e9a23ca086434c998bef898c630e2.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=vBw8JGBJWj,{'value': 'Encoding Unitig-level Assembly Graphs with Heterophilous Constraints for Metagenomic Contigs Binning'},Hansheng Xue; Vijini Mallawaarachchi; Lexing Xie; Vaibhav Rajan,~Hansheng_Xue2; ~Vijini_Mallawaarachchi1; ~Lexing_Xie1; ~Vaibhav_Rajan2,"{'value': ['Metagenomics Binning', 'Computational Genomics', 'Graph Neural Networks']}","{'value': 'Metagenomics studies genomic material derived from mixed microbial communities in diverse environments, holding considerable significance for both human health and environmental sustainability. Metagenomic binning refers to the clustering of genomic subsequences obtained from high-throughput DNA sequencing into distinct bins, each representing a constituent organism within the community. Mainstream binning methods primarily rely on sequence features such as composition and abundance, making them unable to effectively handle sequences shorter than 1,000 bp and inherent noise within sequences. Several binning tools have emerged, aiming to enhance binning outcomes by using the assembly graph generated by assemblers, which encodes valuable overlapping information among genomic sequences. However, existing assembly graph-based binners mainly focus on simplified contig-level assembly graphs that are recreated from assembler’s original graphs, unitig-level assembly graphs. The simplification reduces the resolution of the connectivity information in original graphs. In this paper, we design a novel binning tool named UnitigBin, which leverages representation learning on unitig-level assembly graphs while adhering to heterophilious constraints imposed by single-copy marker genes, ensuring that constrained contigs cannot be grouped together. Extensive experiments conducted on synthetic and real datasets demonstrate that UnitigBin significantly surpasses state-of-the-art binning tools.'}",https://openreview.net{'value': '/pdf/5e09b5e2a56f96a1275298d6160aaeaac721aa8d.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=up6hr4hIQH,{'value': 'Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks'},Xu Zheng; Farhad Shirani; Tianchun Wang; Wei Cheng; Zhuomin Chen; Haifeng Chen; Hua Wei; Dongsheng Luo,~Xu_Zheng3; ~Farhad_Shirani1; ~Tianchun_Wang1; ~Wei_Cheng1; ~Zhuomin_Chen1; ~Haifeng_Chen1; ~Hua_Wei1; ~Dongsheng_Luo1,"{'value': ['Graph Neural Networks', 'Graph Explanation', 'XAI']}","{'value': ""Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes --- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metrics often fail to align with this definition across various statistical scenarios. The reason is due to potential distribution shifts when subgraphs are removed in computing these fidelity measures. Subsequently, a robust class of fidelity measures are introduced, and it is shown analytically that they are resilient to distribution shift issues and are applicable in a wide range of scenarios. Extensive empirical analysis on both synthetic and real datasets are provided to illustrate that the proposed metrics are more coherent with gold standard metrics.""}",https://openreview.net{'value': '/pdf/fae3ff8c249e4f09e8ee94b743c1708a7901c3a2.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=uIKZSStON3,{'value': 'In-context Exploration-Exploitation for Reinforcement Learning'},Zhenwen Dai; Federico Tomasi; Sina Ghiassian,~Zhenwen_Dai1; ~Federico_Tomasi1; ~Sina_Ghiassian1,"{'value': ['Offline reinforcement learning', 'in-context learning', 'decision transformer']}","{'value': 'In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.'}",https://openreview.net{'value': '/pdf/2be54fea28c2cbe6e795c9f61f8f1775cf5d6b94.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=u3dHl287oB,{'value': 'The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting — An Analytical Model'},Daniel Goldfarb; Itay Evron; Nir Weinberger; Daniel Soudry; PAul HAnd,~Daniel_Goldfarb1; ~Itay_Evron1; ~Nir_Weinberger1; ~Daniel_Soudry1; ~PAul_HAnd1,"{'value': ['deep learning', 'continual learning', 'overparameterization', 'task similarity', 'catastrophic forgetting', 'theory']}","{'value': 'In continual learning, catastrophic forgetting is affected by multiple aspects of the tasks. Previous works have analyzed separately how forgetting is affected by either task similarity or overparameterization. In contrast, our paper examines how task similarity and overparameterization jointly affect forgetting in an analyzable model. Specifically, we focus on two-task continual linear regression, where the second task is a random orthogonal transformation of an arbitrary first task (an abstraction of random permutation tasks). We derive an exact analytical expression for the expected forgetting — and uncover a nuanced pattern. In highly overparameterized models, intermediate task similarity causes the most forgetting. However, near the interpolation threshold, forgetting decreases monotonically with the expected task similarity. We validate our findings with linear regression on synthetic data, and with neural networks on established permutation task benchmarks.'}",https://openreview.net{'value': '/pdf/06a7a2ecd558c7a2c6c073fbda3281d0b217ccf5.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=tmqOhBC4a5,{'value': 'Maximum Entropy Heterogeneous-Agent Reinforcement Learning'},Jiarong Liu; Yifan Zhong; Siyi Hu; Haobo Fu; QIANG FU; Xiaojun Chang; Yaodong Yang,~Jiarong_Liu1; ~Yifan_Zhong2; ~Siyi_Hu1; ~Haobo_Fu2; ~QIANG_FU8; ~Xiaojun_Chang4; ~Yaodong_Yang1,"{'value': ['cooperative multi-agent reinforcement learning', 'heterogeneous-agent soft actor-critic', 'maximum entropy heterogeneous-agent mirror learning']}","{'value': '*Multi-agent reinforcement learning* (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning \\emph{stochastic} policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose *Heterogeneous-Agent Soft Actor-Critic* (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to *quantal response equilibrium* (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named *Maximum Entropy Heterogeneous-Agent Mirror Learning* (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration.'}",https://openreview.net{'value': '/pdf/82bacc9b0a9551bf4922e43270f4c315044f70af.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=tVMPfEGT2w,{'value': 'Provable Offline Preference-Based Reinforcement Learning'},Wenhao Zhan; Masatoshi Uehara; Nathan Kallus; Jason D. Lee; Wen Sun,~Wenhao_Zhan1; ~Masatoshi_Uehara1; ~Nathan_Kallus1; ~Jason_D._Lee1; ~Wen_Sun1,"{'value': ['reinforcement learning theory', 'offline reinforcement learning']}","{'value': 'In this paper, we investigate the problem of offline Preference-based Reinforcement Learning (PbRL) with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation (MLE) with general function approximation from offline data and (2) solve a distributionally robust planning problem over a confidence set around the MLE. We consider the general reward setting where the reward can be defined over the whole trajectory and provide a novel guarantee that allows us to learn any target policy with a polynomial number of samples, as long as the target policy is covered by the offline data. This guarantee is the first of its kind with general function approximation. To measure the coverage of the target policy, we introduce a new single-policy concentrability coefficient, which can be upper bounded by the per-trajectory concentrability coefficient. We also establish lower bounds that highlight the necessity of such concentrability and the difference from standard RL, where state-action-wise rewards are directly observed. We further extend and analyze our algorithm when the feedback is given over action pairs.'}",https://openreview.net{'value': '/pdf/ef2a33a9b6e9fd7ea7de7dbba6688f49e0e58206.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=tUtGjQEDd4,{'value': 'Generative Modeling with Phase Stochastic Bridge'},Tianrong Chen; Jiatao Gu; Laurent Dinh; Evangelos Theodorou; Joshua M. Susskind; Shuangfei Zhai,~Tianrong_Chen1; ~Jiatao_Gu1; ~Laurent_Dinh1; ~Evangelos_Theodorou1; ~Joshua_M._Susskind1; ~Shuangfei_Zhai3,"{'value': ['Generative Modeling', 'Stochastic Optimal Control', 'Diffusion Model']}","{'value': 'Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \\textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluations (NFEs). Furthermore, our approach rivals the performance of diffusion models equipped with efficient sampling techniques, underscoring its potential as a new tool generative modeling.'}",https://openreview.net{'value': '/pdf/5d5ddf9cd03dbc97896ca72e62060b33d19f59e7.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=tGQirjzddO,{'value': 'Reasoning with Latent Diffusion in Offline Reinforcement Learning'},Siddarth Venkatraman; Shivesh Khaitan; Ravi Tej Akella; John Dolan; Jeff Schneider; Glen Berseth,~Siddarth_Venkatraman1; ~Shivesh_Khaitan1; ~Ravi_Tej_Akella1; ~John_Dolan1; ~Jeff_Schneider1; ~Glen_Berseth1,"{'value': ['Reinforcement Learning', 'Diffusion Models']}","{'value': 'Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset while avoiding extrapolation errors arising due to a lack of support in the dataset. Existing approaches use conservative methods that are tricky to tune and struggle with multi-modal data or rely on noisy Monte Carlo return-to-go samples for reward conditioning. In this work, we propose a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining. The latent space is also expressive and gracefully copes with multi-modal data. We show that the learned temporally-abstract latent space encodes richer task-specific information for offline RL tasks as compared to raw state-actions. This improves credit assignment and facilitates faster reward propagation during Q-learning. Our method demonstrates state-of-the-art performance on the D4RL benchmarks, particularly excelling in long-horizon, sparse-reward tasks.'}",https://openreview.net{'value': '/pdf/281a715551f213c9fd6a0f53e2b18793f88a6358.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=tBROYsEz9G,{'value': 'How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data'},Mihaela C Stoian; Salijona Dyrmishi; Maxime Cordy; Thomas Lukasiewicz; Eleonora Giunchiglia,~Mihaela_C_Stoian1; ~Salijona_Dyrmishi1; ~Maxime_Cordy1; ~Thomas_Lukasiewicz2; ~Eleonora_Giunchiglia1,{'value': ['Synthetic Data Generation']},"{'value': 'Deep Generative Models (DGMs) have been shown to be powerful tools for generating tabular data, as they have been increasingly able to capture the complex distributions that characterize them. However, to generate realistic synthetic data, it is often not enough to have a good approximation of their distribution, as it also requires compliance with constraints that encode essential background knowledge on the problem at hand. In this paper, we address this limitation and show how DGMs for tabular data can be transformed into Constrained Deep Generative Models (C-DGMs), whose generated samples are guaranteed to be compliant with the given constraints. This is achieved by automatically parsing the constraints and transforming them into a Constraint Layer (CL) seamlessly integrated with the DGM. Our extensive experimental analysis with various DGMs and tasks reveals that standard DGMs often violate constraints, some exceeding 95% non-compliance, while their corresponding C-DGMs are never non-compliant. Then, we quantitatively demonstrate that, at training time, C-DGMs are able to exploit the background knowledge expressed by the constraints to outperform their standard counterparts with up to 4.5% improvement in utility and detection. Further, we show how our CL does not necessarily need to be integrated at training time, as it can be also used as a guardrail at inference time, still producing some improvements in the overall performance of the models. Finally, we show that our CL does not hinder the sample generation time of the models.'}",https://openreview.net{'value': '/pdf/46eb8ee1d8b91eea904b9aa9d0b235f22f645325.pdf'},{'title_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=t8cBsT9mcg,{'value': 'Classification with Conceptual Safeguards'},Hailey Joren; Charles Thomas Marx; Berk Ustun,~Hailey_Joren1; ~Charles_Thomas_Marx1; ~Berk_Ustun1,"{'value': ['classification', 'selective classification', 'concept bottleneck models', 'uncertainty quantification', 'selective classification']}","{'value': 'We propose a new approach to promote safety in classification tasks with concept annotations. Our approach – called a *conceptual safeguard* – acts as a verification layer for models that predict a target outcome by first predicting the presence of intermediate concepts. Given this architecture, a safeguard ensures that a model meets a minimal level of accuracy by abstaining from uncertain predictions. In contrast to a standard selective classifier, a safeguard provides an avenue to improve coverage by allowing a human to confirm the presence of uncertain concepts on instances on which it abstains. We develop methods to build safeguards that maximize coverage without compromising safety, namely techniques to propagate the uncertainty in concept predictions and to flag salient concepts for human review. We benchmark our approach on a collection of real-world and synthetic datasets, showing that it can improve performance and coverage in deep learning tasks.'}",https://openreview.net{'value': '/pdf/79dc9cbf67d5cebb843b50f7307562e8705cb089.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=t3vnnLeajU,{'value': 'Controlling Vision-Language Models for Multi-Task Image Restoration'},Ziwei Luo; Fredrik K. Gustafsson; Zheng Zhao; Jens Sjölund; Thomas B. Schön,~Ziwei_Luo1; ~Fredrik_K._Gustafsson1; ~Zheng_Zhao1; ~Jens_Sjölund1; ~Thomas_B._Schön1,"{'value': ['Image restoration', 'vision-language model', 'low-level vision']}","{'value': 'Vision-language models such as CLIP have shown great impact on diverse downstream tasks for zero-shot or label-free predictions. However, when it comes to low-level vision such as image restoration their performance deteriorates dramatically due to corrupted inputs. In this paper, we present a degradation-aware vision-language model (DA-CLIP) to better transfer pretrained vision-language models to low-level vision tasks as a multi-task framework for image restoration. More specifically, DA-CLIP trains an additional controller that adapts the fixed CLIP image encoder to predict high-quality feature embeddings. By integrating the embedding into an image restoration network via cross-attention, we are able to pilot the model to learn a high-fidelity image reconstruction. The controller itself will also output a degradation feature that matches the real corruptions of the input, yielding a natural classifier for different degradation types. In addition, we construct a mixed degradation dataset with synthetic captions for DA-CLIP training. Our approach advances state-of-the-art performance on both degradation-specific and unified image restoration tasks, showing a promising direction of prompting image restoration with large-scale pretrained vision-language models. Our code is available at https://github.com/Algolzw/daclip-uir.'}",https://openreview.net{'value': '/pdf/da340221403bab803b16154d81342e02c4eeaa69.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=svIdLLZpsA,{'value': 'Real-Fake: Effective Training Data Synthesis Through Distribution Matching'},Jianhao Yuan; Jie Zhang; Shuyang Sun; Philip Torr; Bo Zhao,~Jianhao_Yuan2; ~Jie_Zhang14; ~Shuyang_Sun1; ~Philip_Torr1; ~Bo_Zhao4,"{'value': ['Training Data Synthesis', 'Distribution Matching', 'Data Efficiency']}","{'value': 'Synthetic training data has gained prominence in numerous learning tasks and scenarios, offering advantages such as dataset augmentation, generalization evalua\x02tion, and privacy preservation. Despite these benefits, the efficiency of synthetic data generated by\ncurrent methodologies remains inferior when training advanced deep models exclusively, limiting its practical utility. To address this challenge, we analyze the principles underlying training data synthesis for supervised learning and elucidate a principled theoretical framework from the distribution-matching perspective that explicates the mechanisms governing synthesis efficacy. Through extensive experiments, we demonstrate the effectiveness of our synthetic data across diverse image classification tasks, both as a replacement for and augmenta\x02tion to real datasets, while also benefits such as out-of-distribution generalization, privacy preservation, and scalability. Specifically, we achieve 70.9% top1 classification accuracy on ImageNet1K when training solely with synthetic data equivalent\nto 1 × the original real data size, which increases to 76.0% when scaling up to 10 × synthetic data.'}",https://openreview.net{'value': '/pdf/22c2e29e22c1cc145ff0d253589743fdd0e72267.pdf'},{'title_filter': 'Data Synthesis'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=sllU8vvsFF,{'value': 'LRM: Large Reconstruction Model for Single Image to 3D'},Yicong Hong; Kai Zhang; Jiuxiang Gu; Sai Bi; Yang Zhou; Difan Liu; Feng Liu; Kalyan Sunkavalli; Trung Bui; Hao Tan,~Yicong_Hong1; ~Kai_Zhang7; ~Jiuxiang_Gu2; ~Sai_Bi1; ~Yang_Zhou10; ~Difan_Liu2; ~Feng_Liu6; ~Kalyan_Sunkavalli1; ~Trung_Bui1; ~Hao_Tan1,"{'value': ['3D Reconstruction', 'Large-Scale Training', 'Transformers']}","{'value': 'We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our LRM project webpage: https://yiconghong.me/LRM.'}",https://openreview.net{'value': '/pdf/21831b2594b6b1378c517f290ba90103625d2d55.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=sBQwvucduK,{'value': 'MagicDrive: Street View Generation with Diverse 3D Geometry Control'},Ruiyuan Gao; Kai Chen; Enze Xie; Lanqing HONG; Zhenguo Li; Dit-Yan Yeung; Qiang Xu,~Ruiyuan_Gao2; ~Kai_Chen11; ~Enze_Xie1; ~Lanqing_HONG1; ~Zhenguo_Li1; ~Dit-Yan_Yeung2; ~Qiang_Xu1,"{'value': ['diffusion models', 'street view generation', '3D control']}","{'value': ""Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework, offering diverse 3D geometry controls including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view image & video synthesis that captures nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV segmentation and 3D object detection. Project Website: https://flymin.github.io/magicdrive""}",https://openreview.net{'value': '/pdf/24ee27e06af8a9a4d217bf99e7d46340c5b078b0.pdf'},{'abstract_filter': 'Data Synthesis'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=s9z0HzWJJp,{'value': 'SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series'},Junyan Cheng; Peter Chin,~Junyan_Cheng1; ~Peter_Chin1,"{'value': ['Large Langauge Models', 'Agent', 'Prompt Tunning', 'Time series forcasting']}","{'value': 'We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called ""hyperportfolio"", that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to ""invest"". We also propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis & Proof prompting for producing in-depth analyses on input news, articles, etc. to assist decision-making. We perform experiments and ablation studies to explore the factors that impact performance. The results show that our proposed method achieves improvements of 32.4% and 30.4% compared to the state-of-the-art method in the two experimental settings.'}",https://openreview.net{'value': '/pdf/473beb76f232cb31d5e4d7594a17375a1347cef9.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=rtx8B94JMS,{'value': 'Variational Inference for SDEs Driven by Fractional Noise'},Rembert Daems; Manfred Opper; Guillaume Crevecoeur; Tolga Birdal,~Rembert_Daems1; ~Manfred_Opper1; ~Guillaume_Crevecoeur1; ~Tolga_Birdal3,"{'value': ['variational inference', 'neural sdes', 'stochastic differential equations', 'brownian motion', 'fractional noise', 'fractional brownian motion', 'markov approximation', 'markov representation']}","{'value': 'We present a novel variational framework for performing inference in (neural) stochastic differential equations (SDEs) driven by Markov-approximate fractional Brownian motion (fBM). SDEs offer a versatile tool for modeling real-world continuous-time dynamic systems with inherent noise and randomness. Combining SDEs with the powerful inference capabilities of variational methods, enables the learning of representative distributions through stochastic gradient descent. However, conventional SDEs typically assume  the underlying noise to follow a Brownian motion (BM), which hinders their ability to capture long-term dependencies. In contrast, fractional Brownian motion (fBM) extends BM to encompass non-Markovian dynamics, but existing methods for inferring fBM parameters are either computationally demanding or statistically inefficient. \n\nIn this paper, building upon the Markov approximation of fBM, we derive the evidence lower bound essential for efficient variational inference of posterior path measures, drawing from the well-established field of stochastic analysis. Additionally, we provide a closed-form expression for optimal approximation coefficients and propose to use neural networks to learn the drift, diffusion and control terms within our variational posterior, leading to the variational training of neural-SDEs. In this framework, we also optimize the Hurst index, governing the nature of our fractional noise. Beyond validation on synthetic data, we contribute a novel architecture for variational latent video prediction,—an approach that, to the best of our knowledge, enables the first variational neural-SDE application to video perception.'}",https://openreview.net{'value': '/pdf/d5ed1bbdd5e5bd4a9dd5fa78f017dc8eed8f7fbc.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=rtl4XnJYBh,{'value': 'Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift'},Yihao Xue; Siddharth Joshi; Dang Nguyen; Baharan Mirzasoleiman,~Yihao_Xue1; ~Siddharth_Joshi1; ~Dang_Nguyen2; ~Baharan_Mirzasoleiman1,"{'value': ['multi-model contrastive learning', 'contrastive learning theory', 'representation learning', 'theory', 'robustness', 'distribution shift']}","{'value': ""Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable representations is not understood. In this work, we rigorously analyze this problem and \nuncover two mechanisms behind MMCL's robustness: \\emph{intra-class contrasting}, which allows the model to learn features with a high variance, and \\emph{inter-class feature sharing}, where annotated details in one class help learning other classes better. Both mechanisms prevent spurious features that are over-represented in the training data to overshadow the generalizable core features. This yields superior zero-shot classification accuracy under distribution shift. Furthermore, we theoretically demonstrate the benefits of using rich captions on robustness and explore the effect of annotating different types of details in the captions. We validate our theoretical findings through experiments, including a well-designed synthetic experiment and an experiment involving training CLIP models on MSCOCO/Conceptual Captions and evaluating them on shifted ImageNets.""}",https://openreview.net{'value': '/pdf/6a626c46d27f126eacb827909e73e287e464009d.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=rpH9FcCEV6,{'value': 'An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization'},Fei Kong; Jinhao Duan; RuiPeng Ma; Heng Tao Shen; Xiaoshuang Shi; Xiaofeng Zhu; Kaidi Xu,~Fei_Kong1; ~Jinhao_Duan1; ~RuiPeng_Ma1; ~Heng_Tao_Shen3; ~Xiaoshuang_Shi1; ~Xiaofeng_Zhu7; ~Kaidi_Xu1,"{'value': ['MIA', 'Diffusion Model', 'Text-To-Speech']}","{'value': 'Recently, diffusion models have achieved remarkable success in generating tasks, including image and audio generation. However, like other generative models, diffusion models are prone to privacy issues. In this paper, we propose an efficient query-based membership inference attack (MIA), namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by $\\epsilon$ initialized in $t=0$ and predicted point to infer memberships. Experimental results indicate that the proposed method can achieve competitive performance with only two queries that achieve at least 6$\\times$ efficiency than the previous SOTA baseline on both discrete-time and continuous-time diffusion models. Moreover, previous works on the privacy of diffusion models have focused on vision tasks without considering audio tasks. Therefore, we also explore the robustness of diffusion models to MIA in the text-to-speech (TTS) task, which is an audio generation task. To the best of our knowledge, this work is the first to study the robustness of diffusion models to MIA in the TTS task. Experimental results indicate that models with mel-spectrogram (image-like) output are vulnerable to MIA, while models with audio output are relatively robust to MIA. Code is available at https://github.com/kong13661/PIA.'}",https://openreview.net{'value': '/pdf/cb41e2e98318608968c3fc0c8111e58b9f9d9922.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=rnHNDihrIT,{'value': 'Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets'},Yihuan Mao; Chengjie Wu; Xi Chen; Hao Hu; Ji Jiang; Tianze Zhou; Tangjie Lv; Changjie Fan; Zhipeng Hu; Yi Wu; Yujing Hu; Chongjie Zhang,~Yihuan_Mao2; ~Chengjie_Wu1; ~Xi_Chen18; ~Hao_Hu3; ~Ji_Jiang1; ~Tianze_Zhou2; ~Tangjie_Lv1; ~Changjie_Fan1; ~Zhipeng_Hu1; ~Yi_Wu1; ~Yujing_Hu2; ~Chongjie_Zhang1,"{'value': ['Reinforcement Learning', 'Diversity in RL', 'Offline RL']}","{'value': 'Previous literature on policy diversity in reinforcement learning (RL) either focuses on the online setting or ignores the policy performance. In contrast, offline RL, which aims to learn high-quality policies from batched data, has yet to fully leverage the intrinsic diversity of the offline dataset. Addressing this dichotomy and aiming to balance quality and diversity poses a significant challenge to extant methodologies. This paper introduces a novel approach, termed Stylized Offline RL (SORL), which is designed to extract high-performing, stylistically diverse policies from a dataset characterized by distinct behavioral patterns. Drawing inspiration from the venerable Expectation-Maximization (EM) algorithm, SORL innovatively alternates between policy learning and trajectory clustering, a mechanism that promotes policy diversification. To further augment policy performance, we introduce advantage-weighted style learning into the SORL framework. Experimental evaluations across multiple environments demonstrate the significant superiority of SORL over previous methods in extracting high-quality policies with diverse behaviors. A case in point is that SORL successfully learns strong policies with markedly distinct playing patterns from a real-world human dataset of a popular basketball video game ""Dunk City Dynasty.""'}",https://openreview.net{'value': '/pdf/2ca57d1d09d2c77b2d390bac2cb3c761f94d4358.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=rTBL8OhdhH,{'value': 'Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching'},Ziyao Guo; Kai Wang; George Cazenavette; HUI LI; Kaipeng Zhang; Yang You,~Ziyao_Guo1; ~Kai_Wang8; ~George_Cazenavette1; ~HUI_LI17; ~Kaipeng_Zhang1; ~Yang_You1,{'value': ['Dataset Distillation']},"{'value': 'The ultimate goal of Dataset Distillation is to synthesize a small synthetic dataset such that a model trained on this synthetic set will perform equally well as a model trained on the full, real dataset. Until now, no method of Dataset Distillation has reached this completely lossless goal, in part due to the fact that previous methods only remain effective when the total number of synthetic samples is extremely small. Since only so much information can be contained in such a small number of samples, it seems that to achieve truly loss dataset distillation, we must develop a distillation method that remains effective as the size of the synthetic dataset grows. In this work, we present such an algorithm and elucidate why existing methods fail to generate larger, high-quality synthetic sets. Current state-of-the-art methods rely on trajectory-matching, or optimizing the synthetic data to induce similar long-term training dynamics as the real data. We empirically find that the training stage of the trajectories we choose to match (i.e., early or late) greatly affects the effectiveness of the distilled dataset. Specifically, early trajectories (where the teacher network learns easy patterns) work well for a low-cardinality synthetic set since there are fewer examples wherein to distribute the necessary information. Conversely, late trajectories (where the teacher network learns hard patterns) provide better signals for larger synthetic sets since there are now enough samples to represent the necessary complex patterns. Based on our findings, we propose to align the difficulty of the generated patterns with the size of the synthetic dataset. In doing so, we successfully scale trajectory matching-based methods to larger synthetic datasets, achieving lossless dataset distillation for the very first time. Code and distilled datasets will be released.'}",https://openreview.net{'value': '/pdf/76b4ca911ace4176947d021053d07d288e44f1a2.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=rDH7dIFn20,{'value': 'Variance-aware Regret Bounds for Stochastic Contextual Dueling Bandits'},Qiwei Di; Tao Jin; Yue Wu; Heyang Zhao; Farzad Farnoud; Quanquan Gu,~Qiwei_Di1; ~Tao_Jin3; ~Yue_Wu12; ~Heyang_Zhao1; ~Farzad_Farnoud1; ~Quanquan_Gu1,"{'value': ['Dueling Bandit', 'Variance-aware', 'contextual bandit']}","{'value': 'Dueling bandits is a prominent framework for decision-making involving preferential feedback, a valuable feature that fits various applications involving human interaction, such as ranking, information retrieval, and recommendation systems. While substantial efforts have been made to minimize the cumulative regret in dueling bandits, a notable gap in the current research is the absence of regret bounds that account for the inherent uncertainty in pairwise comparisons between the dueling arms. Intuitively, greater uncertainty suggests a higher level of difficulty in the problem.  To bridge this gap, this paper studies the problem of contextual dueling bandits, where the binary comparison of dueling arms is generated from a generalized linear model (GLM). We propose a new SupLinUCB-type algorithm that enjoys computational efficiency and a variance-aware regret bound $\\tilde O\\big(d\\sqrt{\\sum_{t=1}^T\\sigma_t^2} + d\\big)$, where $\\sigma_t$ is the variance of the pairwise comparison at round $t$, $d$ is the dimension of the context vectors, and $T$ is the time horizon. Our regret bound naturally aligns with the intuitive expectation — in scenarios where the comparison is deterministic, the algorithm only suffers from an $\\tilde O(d)$ regret. We perform empirical experiments on synthetic data to confirm the advantage of our method over previous variance-agnostic algorithms.'}",https://openreview.net{'value': '/pdf/4c1a08aad3975e9de1adbba054eea8e3f1287418.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=qup9xD8mW4,{'value': 'Behaviour Distillation'},Andrei Lupu; Chris Lu; Jarek Luca Liesen; Robert Tjarko Lange; Jakob Nicolaus Foerster,~Andrei_Lupu1; ~Chris_Lu1; ~Jarek_Luca_Liesen1; ~Robert_Tjarko_Lange1; ~Jakob_Nicolaus_Foerster1,"{'value': ['knowledge distillation', 'evolutionary strategies', 'reinforcement learning', 'dataset distillation']}","{'value': 'Dataset distillation aims to condense large datasets into a small number of synthetic examples that can be used as drop-in replacements when training new models. It has applications to interpretability, neural architecture search, privacy, and continual learning. Despite strong successes in supervised domains, such methods have not yet been extended to reinforcement learning, where the lack of a fixed dataset renders most distillation methods unusable.\nFilling the gap, we formalize $\\textit{behaviour distillation}$, a setting that aims to discover and then condense the information required for training an expert policy into a synthetic dataset of state-action pairs, $\\textit{without access to expert data}$. \nWe then introduce Hallucinating Datasets with Evolution Strategies (HaDES), a method for behaviour distillation that can discover datasets of $\\textit{just four}$ state-action pairs which, under supervised learning, train agents to competitive performance levels in continuous control tasks.\nWe show that these datasets generalize out of distribution to training policies with a wide range of architectures and hyperparameters. We also demonstrate application to a downstream task, namely training multi-task agents in a zero-shot fashion.\nBeyond behaviour distillation, HaDES provides significant improvements in neuroevolution for RL over previous approaches and achieves SoTA results on one standard supervised dataset distillation task. Finally, we show that visualizing the synthetic datasets can provide human-interpretable task insights.'}",https://openreview.net{'value': '/pdf/c401434987a8b2aa4b593e8609ec6cc7085d772b.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=qoHeuRAcSl,{'value': 'Grounding Language Plans in Demonstrations Through Counterfactual Perturbations'},Yanwei Wang; Tsun-Hsuan Wang; Jiayuan Mao; Michael Hagenow; Julie Shah,~Yanwei_Wang1; ~Tsun-Hsuan_Wang2; ~Jiayuan_Mao1; hagenow@mit.edu; ~Julie_Shah2,"{'value': ['Grounding LLM', 'Learning Mode Abstractions for Manipulation', 'Learning from Demonstration', 'Robotics', 'Task and Motion Planning']}","{'value': ""Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide/""}",https://openreview.net{'value': '/pdf/095205c11ca0cd7a485da10923a605bbfd899160.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=qHGgNyQk31,{'value': 'Seer: Language Instructed Video Prediction with Latent Diffusion Models'},Xianfan Gu; Chuan Wen; Weirui Ye; Jiaming Song; Yang Gao,~Xianfan_Gu1; ~Chuan_Wen1; ~Weirui_Ye1; ~Jiaming_Song1; ~Yang_Gao1,"{'value': ['video prediction', 'multi-modality generation', 'diffusion model', 'language-vision model']}","{'value': ""Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning.\nTo tackle this task and empower robots with the ability to foresee the future, we propose a sample and computation-efficient model, named Seer, by inflating the pretrained text-to-image (T2I) stable diffusion models along the temporal axis. We enhance the U-Net and language conditioning model by incorporating computation-efficient spatial-temporal attention. Furthermore, we introduce a novel Frame Sequential Text Decomposer module that dissects a sentence's global instruction into temporally aligned sub-instructions, ensuring precise integration into each frame of generation. Our framework allows us to effectively leverage the extensive prior knowledge embedded in pretrained T2I models across the frames. \nWith the adaptable-designed architecture, Seer makes it possible to generate high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a few layers on a small amount of data. The experimental results on Something Something V2 (SSv2), Bridgedata and EpicKitchens-100 datasets demonstrate our superior video prediction performance with around 480-GPU hours versus CogVideo with over 12,480-GPU hours: achieving the 31\\% FVD improvement compared to the current SOTA model on SSv2 and 83.7\\% average preference in the human evaluation. Our project is available at https://seervideodiffusion.github.io/""}",https://openreview.net{'value': '/pdf/b4ad5effde78b04f3efda6933cc9586927c29004.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=qDhq1icpO8,{'value': 'Conditional Instrumental Variable Regression with Representation Learning for Causal Inference'},Debo Cheng; Ziqi Xu; Jiuyong Li; Lin Liu; Jixue Liu; Thuc Duy Le,~Debo_Cheng1; ~Ziqi_Xu1; ~Jiuyong_Li1; ~Lin_Liu4; ~Jixue_Liu1; ~Thuc_Duy_Le1,"{'value': ['Causal Inference', 'Variational AutoEncoder', 'Instrumental Variable']}","{'value': 'This paper studies the challenging problem of estimating causal effects from observational data, in the presence of unobserved confounders. The two-stage least square (TSLS) method and its variants with a standard instrumental variable (IV) are commonly used to eliminate confounding bias, including the bias caused by unobserved confounders, but they rely on the linearity assumption. Besides, the strict condition of unconfounded instruments posed on a standard IV is too strong to be practical. To address these challenging and practical problems of the standard IV method (linearity assumption and the strict condition), in this paper, we use a conditional IV (CIV) to relax the unconfounded instrument condition of standard IV and propose a non-linear \\underline{CIV} regression with \\underline{C}onfounding \\underline{B}alancing \\underline{R}epresentation \\underline{L}earning,  CBRL.CIV, for jointly eliminating the confounding bias from unobserved confounders and balancing the observed confounders, without the linearity assumption. We theoretically demonstrate the soundness of CBRL.CIV. Extensive experiments on synthetic and two real-world datasets show the competitive performance of CBRL.CIV against state-of-the-art IV-based estimators and superiority in dealing with the non-linear situation.'}",https://openreview.net{'value': '/pdf/c4bb7521109fe7cfec2d5121dbcf0770cb83cab2.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=qDdSRaOiyb,{'value': 'Explaining Time Series via Contrastive and Locally Sparse Perturbations'},Zichuan Liu; Yingying ZHANG; Tianchun Wang; Zefan Wang; Dongsheng Luo; Mengnan Du; Min Wu; Yi Wang; Chunlin Chen; Lunting Fan; Qingsong Wen,~Zichuan_Liu3; ~Yingying_ZHANG4; ~Tianchun_Wang1; ~Zefan_Wang2; ~Dongsheng_Luo1; ~Mengnan_Du1; ~Min_Wu2; ~Yi_Wang43; ~Chunlin_Chen1; ~Lunting_Fan1; ~Qingsong_Wen2,"{'value': ['time series', 'explainability', 'perturbation']}","{'value': 'Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns.\nAlthough previous saliency-based methods addressed the challenges,\ntheir perturbation may not alleviate the distribution shift issue, which is inevitable especially in heterogeneous samples.\nWe present ContraLSP, a locally sparse model that introduces counterfactual samples to build uninformative perturbations but keeps distribution using contrastive learning.\nFurthermore, we incorporate sample-specific sparse gates to generate more binary-skewed and smooth masks, which easily integrate temporal trends and select the salient features parsimoniously.\nEmpirical studies on both synthetic and real-world datasets show that ContraLSP outperforms state-of-the-art models, demonstrating a substantial improvement in explanation quality for time series data.\nThe source code is available at \\url{https://github.com/zichuan-liu/ContraLSP}.'}",https://openreview.net{'value': '/pdf/139dd222fff7b006a509d6db2182fd7b8e17c46b.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=pAoqRlTBtY,{'value': 'Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning'},Ahmed Abdulaal; adamos hadjivasiliou; Nina Montana-Brown; Tiantian He; Ayodeji Ijishakin; Ivana Drobnjak; Daniel C. Castro; Daniel C. Alexander,~Ahmed_Abdulaal1; ~adamos_hadjivasiliou1; ~Nina_Montana-Brown1; tiantian.he.20@ucl.ac.uk; ~Ayodeji_Ijishakin1; ~Ivana_Drobnjak2; ~Daniel_C._Castro1; ~Daniel_C._Alexander1,"{'value': ['Causal Reasoning', 'Causal Discovery', 'Structural Causal Models', 'Large Language Models']}","{'value': ""Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.""}",https://openreview.net{'value': '/pdf/62fc3766e10c6f5fa2f2a9b44b46098519f89596.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=p4eG8rCa0b,{'value': 'Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis'},Jonghyun Lee; Hansam Cho; YoungJoon Yoo; Seoung Bum Kim; Yonghyun Jeong,~Jonghyun_Lee2; ~Hansam_Cho1; ~YoungJoon_Yoo1; ~Seoung_Bum_Kim1; ~Yonghyun_Jeong1,"{'value': ['Generative Models', 'Diffusion Models', 'Image Editing', 'Image Composition']}","{'value': 'Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, Compose and Conquer (CnC), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics.'}",https://openreview.net{'value': '/pdf/c1f91459054d4243a0b6b078ff6d6b6f0c1707f3.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=p4S5Z6Sah4,{'value': 'Traveling Waves Encode The Recent Past and Enhance Sequence Learning'},T. Anderson Keller; Lyle Muller; Terrence Sejnowski; Max Welling,~T._Anderson_Keller1; ~Lyle_Muller1; ~Terrence_Sejnowski2; ~Max_Welling1,"{'value': ['RNNs', 'Traveling Waves', 'Memory', 'Sequence Modeling']}","{'value': 'Traveling waves of neural activity have been observed throughout the brain at a diversity of regions and scales; however, their precise computational role is still debated. One physically inspired hypothesis suggests that the cortical sheet may act like a wave-propagating system capable of invertibly storing a short-term memory of sequential stimuli through induced waves traveling across the cortical surface, and indeed many experimental results from neuroscience correlate wave activity with memory tasks. To date, however, the computational implications of this idea have remained hypothetical due to the lack of a simple recurrent neural network architecture capable of exhibiting such waves. In this work, we introduce a model to fill this gap, which we denote the Wave-RNN (wRNN), and demonstrate how such an architecture indeed efficiently encodes the recent past through a suite of synthetic memory tasks where wRNNs learn faster and reach significantly lower error than wave-free counterparts. We further explore the implications of this memory storage system on more complex sequence modeling tasks such as sequential image classification and find that wave-based models not only again outperform comparable wave-free RNNs while using significantly fewer parameters, but additionally perform comparably to more complex gated architectures such as LSTMs and GRUs.'}",https://openreview.net{'value': '/pdf/875de4ed3f20200b62e1de9da79bf3a2aae7068f.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ojIJZDNIBj,{'value': 'Copula Conformal prediction for multi-step time series prediction'},Sophia Huiwen Sun; Rose Yu,~Sophia_Huiwen_Sun1; ~Rose_Yu1,"{'value': ['Conformal Prediction', 'time series', 'uncertainty quantification', 'calibration', 'RNN']}","{'value': 'Accurate uncertainty measurement is a key step in building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification framework popular for its ease of implementation, finite-sample coverage guarantees, and generality for underlying prediction algorithms. However, existing conformal prediction approaches for time series are limited to single-step prediction without considering the temporal dependency. In this paper, we propose the Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite-sample validity guarantee. On four synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and efficient confidence intervals for multi-step prediction tasks than existing techniques. Our code is open-sourced at https://github.com/Rose-STL-Lab/CopulaCPTS.'}",https://openreview.net{'value': '/pdf/76af3e4f5f8bb8b344fcb41b96512669856618c4.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=oZtt0pRnOl,{'value': 'Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation'},Xinyu Tang; Richard Shin; Huseyin A Inan; Andre Manoel; Fatemehsadat Mireshghallah; Zinan Lin; Sivakanth Gopi; Janardhan Kulkarni; Robert Sim,~Xinyu_Tang1; ~Richard_Shin1; ~Huseyin_A_Inan1; ~Andre_Manoel1; ~Fatemehsadat_Mireshghallah1; ~Zinan_Lin1; ~Sivakanth_Gopi1; ~Janardhan_Kulkarni2; ~Robert_Sim1,"{'value': ['differential privacy', 'large language models', 'in-context learning']}","{'value': 'We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. \nThis scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt.\nWe propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (DP) guarantees, and show empirically that it can achieve effective ICL.\nWe conduct extensive experiments on standard benchmarks and compare our algorithm with non-private ICL and zero-shot solutions. \nOur results demonstrate that our algorithm can achieve competitive performance with strong privacy levels.\nThese results open up new possibilities for ICL with privacy protection for a broad range of applications.'}",https://openreview.net{'value': '/pdf/09be8703beef4187bc79c928d6e2482368daaff7.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=oQKKlzxV1o,{'value': 'Online Information Acquisition: Hiring Multiple Agents'},Federico Cacciamani; Matteo Castiglioni; Nicola Gatti,~Federico_Cacciamani1; ~Matteo_Castiglioni1; ~Nicola_Gatti1,"{'value': ['online learning', 'information acquisition', 'mechanism design']}","{'value': ""We investigate the mechanism design problem faced by a principal who hires \\emph{multiple} agents to gather and report costly information. Then, the principal exploits the  information to make an informed decision.  We model this problem as a game, where the principal announces a mechanism consisting in action recommendations and a payment function, a.k.a. scoring rule. Then, each agent chooses an effort level and receives partial information about an underlying state of nature based on the effort. Finally, the agents report the information (possibly non-truthfully), the principal takes a decision based on this information, and the agents are paid according to the scoring rule. While previous work focuses on single-agent problems, we consider multi-agents settings. This poses the challenge of coordinating the agents' efforts and aggregating correlated information. Indeed, we show that optimal mechanisms must correlate agents' efforts, which introduces externalities among the agents, and hence complex incentive compatibility constraints and equilibrium selection problems. First, we design a polynomial-time algorithm to find an optimal incentive compatible mechanism. Then, we study an online problem, where the principal repeatedly interacts with a group of unknown agents. We design a no-regret algorithm that provides $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret with respect to an optimal mechanism, matching the state-of-the-art bound for single-agent settings.""}",https://openreview.net{'value': '/pdf/83fcb8ac23e715a1b18621cd00f0a8a7d023dc7a.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=oKn9c6ytLx,{'value': 'WebArena: A Realistic Web Environment for Building Autonomous Agents'},Shuyan Zhou; Frank F. Xu; Hao Zhu; Xuhui Zhou; Robert Lo; Abishek Sridhar; Xianyi Cheng; Tianyue Ou; Yonatan Bisk; Daniel Fried; Uri Alon; Graham Neubig,~Shuyan_Zhou1; ~Frank_F._Xu1; ~Hao_Zhu1; ~Xuhui_Zhou1; ~Robert_Lo1; ~Abishek_Sridhar1; ~Xianyi_Cheng1; ~Tianyue_Ou1; ~Yonatan_Bisk1; ~Daniel_Fried1; ~Uri_Alon1; ~Graham_Neubig1,{'value': ['language-guided agents; web automation; benchmark']},"{'value': 'With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting.  The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \\ours can be used to measure such progress.\\footnote{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.}'}",https://openreview.net{'value': '/pdf/384c500abb2b5adc4f3c40956a267477925c4b94.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=o7qhUMylLU,{'value': 'Sample-Efficient Multi-Agent RL: An Optimization Perspective'},Nuoya Xiong; Zhihan Liu; Zhaoran Wang; Zhuoran Yang,~Nuoya_Xiong1; ~Zhihan_Liu1; ~Zhaoran_Wang1; ~Zhuoran_Yang1,"{'value': ['multi-agent reinforcement learning', 'theory', 'general function approximation', 'general-sum Markov Games']}","{'value': 'We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under general function approximation. \n    In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.'}",https://openreview.net{'value': '/pdf/fe5b0154a5ddbf71df6a0cd9077a345a6fa6065c.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=n7Sr8SW4bn,{'value': 'Mayfly: a Neural Data Structure for Graph Stream Summarization'},Yuan Feng; Yukun Cao; Wang Hairu; Xike Xie; S Kevin Zhou,~Yuan_Feng10; ~Yukun_Cao1; ~Wang_Hairu2; ~Xike_Xie1; ~S_Kevin_Zhou1,{'value': ['Meta-Learning;Memory Augmented Neural Network; Deep Neural Network Application;Graph Summarization']},"{'value': 'A graph is a structure made up of vertices and edges used to represent complex relationships between entities, while a graph stream is a continuous flow of graph updates that convey evolving relationships between entities. The massive volume and high dynamism of graph streams promote research on data structures of graph summarization, which provides a concise and approximate view of graph streams with sub-linear space and linear construction time, enabling real-time graph analytics in various domains, such as social networking, financing, and cybersecurity.\nIn this work, we propose the Mayfly, the first neural data structure for summarizing graph streams. The Mayfly replaces handcrafted data structures with better accuracy and adaptivity.\nTo cater to practical applications, Mayfly incorporates two offline training phases.\nDuring the larval phase, the Mayfly learns basic summarization abilities from automatically and synthetically constituted meta-tasks, and in the metamorphosis phase, it rapidly adapts to real graph streams via meta-tasks.\nWith specific configurations of information pathways, the Mayfly enables flexible support for miscellaneous graph queries, including edge, node, and connectivity queries.\nExtensive empirical studies show that the Mayfly significantly outperforms its handcrafted competitors.'}",https://openreview.net{'value': '/pdf/e7d88ca4c807b194ba332ff83811bbd8c79934bc.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=mw1PWNSWZP,{'value': 'OctoPack: Instruction Tuning Code Large Language Models'},Niklas Muennighoff; Qian Liu; Armel Randy Zebaze; Qinkai Zheng; Binyuan Hui; Terry Yue Zhuo; Swayam Singh; Xiangru Tang; Leandro Von Werra; Shayne Longpre,~Niklas_Muennighoff1; ~Qian_Liu2; ~Armel_Randy_Zebaze1; ~Qinkai_Zheng2; ~Binyuan_Hui1; ~Terry_Yue_Zhuo1; ~Swayam_Singh1; ~Xiangru_Tang2; ~Leandro_Von_Werra1; ~Shayne_Longpre1,"{'value': ['large language models', 'large code models', 'instruction tuning']}","{'value': ""Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.""}",https://openreview.net{'value': '/pdf/2b332f4f4e9406870d019ed23d22f771fefbe70f.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=mM7VurbA4r,{'value': 'SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents'},Xuhui Zhou; Hao Zhu; Leena Mathur; Ruohong Zhang; Haofei Yu; Zhengyang Qi; Louis-Philippe Morency; Yonatan Bisk; Daniel Fried; Graham Neubig; Maarten Sap,~Xuhui_Zhou1; ~Hao_Zhu1; ~Leena_Mathur1; ~Ruohong_Zhang1; ~Haofei_Yu1; ~Zhengyang_Qi1; ~Louis-Philippe_Morency1; ~Yonatan_Bisk1; ~Daniel_Fried1; ~Graham_Neubig1; ~Maarten_Sap1,"{'value': ['Social', 'Interaction', 'Agent', 'Social intelligence', 'Large Language Models', 'Evaluation', 'Theory of Mind']}","{'value': ""*Humans are social beings*; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and *interact* under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.""}",https://openreview.net{'value': '/pdf/6aece1f9088fc415196df5830f1ca62e6dbf37d3.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=m7tJxajC3G,{'value': 'Federated Causal Discovery from Heterogeneous Data'},Loka Li; Ignavier Ng; Gongxu Luo; Biwei Huang; Guangyi Chen; Tongliang Liu; Bin Gu; Kun Zhang,~Loka_Li1; ~Ignavier_Ng1; ~Gongxu_Luo1; ~Biwei_Huang1; ~Guangyi_Chen1; ~Tongliang_Liu1; ~Bin_Gu1; ~Kun_Zhang1,"{'value': ['Causal Discovery', 'Structure Learning', 'Federated Learning', 'Heterogeneous Data']}","{'value': 'Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing summary statistics as a proxy of the raw data to protect data privacy. Owing to the nonparametric properties, FCIT and FICP make no assumption about particular functional forms, thereby facilitating the handling of arbitrary causal models. We conduct extensive experiments on synthetic and real datasets to show the efficacy of our method. The code is available at https://github.com/lokali/FedCDH.git.'}",https://openreview.net{'value': '/pdf/5312cb34c47985655944829b99e98dedd558745b.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ltZ9ianMth,{'value': 'RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies'},Hao Cheng; Qingsong Wen; Yang Liu; Liang Sun,~Hao_Cheng5; ~Qingsong_Wen2; ~Yang_Liu3; ~Liang_Sun2,{'value': ['Robust time series forecasting; learning with noisy labels']},"{'value': 'Time series forecasting is an important and forefront task whose techniques have been applied to electricity forecasting, trajectory prediction, labor planning, etc. However, most of time series forecasting techniques assume that the training data is clean without anomalies. This assumption is unrealistic since the collected time series data can be contaminated in practice. The forecasting model will be inferior if it is directly trained by time series with anomalies. Thus it is essential to develop methods to automatically learn a robust forecasting model from the contaminated data. In this paper, we first statistically define three types of anomalies, then theoretically and experimentally analyze the loss robustness and sample robustness when these anomalies exist. Based on our analyses, we propose a simple and efficient algorithm to learn a robust forecasting model. Extensive experiments show that our method is highly robust and outperforms all existing approaches. The code is available at https://github.com/haochenglouis/RobustTSF.'}",https://openreview.net{'value': '/pdf/26c803ecf9794f4827a2507c69698ecefa1c6960.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=lHasEfGsXL,{'value': 'LightHGNN: Distilling Hypergraph Neural Networks into MLPs for 100x Faster Inference'},Yifan Feng; Yihe Luo; Shihui Ying; Yue Gao,~Yifan_Feng1; ~Yihe_Luo1; ~Shihui_Ying1; ~Yue_Gao4,"{'value': ['Hypergraph', 'HGNN', 'Knowledge Distillation', 'MLPs', 'Reliale Learning']}","{'value': 'Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. \nHowever, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment.\nIn practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference.\nIn this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed. \nSpecifically, we introduce LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN directly distills the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN$^+$ further explicitly injects reliable high-order correlations into the student MLPs to achieve topology-aware distillation and resistance to over-smoothing.\nExperiments on eight hypergraph datasets demonstrate that even without hypergraph dependency, the proposed LightHGNNs can still achieve competitive or even better performance than HGNNs and outperform vanilla MLPs by $16.3$ on average. Extensive experiments on three graph datasets further show the average best performance of our LightHGNNs compared with all other methods.\nExperiments on synthetic hypergraphs with 5.5w vertices indicate LightHGNNs can run $100\\times$ faster than HGNNs, showcasing their ability for latency-sensitive deployments.'}",https://openreview.net{'value': '/pdf/a56c441a5107b922f1268eb91721d0b5ddb37f0c.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=kPrxk6tUcg,{'value': 'Neuron-Enhanced AutoEncoder Matrix Completion and Collaborative Filtering: Theory and Practice'},Jicong Fan; Rui Chen; Zhao Zhang; Chris Ding,~Jicong_Fan2; ~Rui_Chen10; ~Zhao_Zhang3; ~Chris_Ding1,"{'value': ['autoencoder', 'matrix completion', 'collaborative filtering']}","{'value': 'Neural networks have shown promising performance in collaborative filtering and matrix completion but the theoretical analysis is limited and there is still room for improvement in terms of the accuracy of recovering missing values. This paper presents a neuron-enhanced autoencoder matrix completion (AEMC-NE) method and applies it to collaborative filtering. Our AEMC-NE adds an element-wise autoencoder to each output of the main autoencoder to enhance the reconstruction capability. Thus it can adaptively learn an activation function for the output layer to approximate possibly complicated response functions in real data. We provide theoretical analysis for AEMC-NE as well as AEMC to investigate the generalization ability of autoencoder and deep learning in matrix completion, considering both missing completely at random and missing not at random. We show that the element-wise neural network has the potential to reduce the generalization error bound, the data sparsity can be useful, and the prediction performance is closely related to the difference between the numbers of variables and samples. The numerical results on synthetic data and benchmark datasets demonstrated the effectiveness of AEMC-NE in comparison to many baselines.'}",https://openreview.net{'value': '/pdf/26bfcbd64a1cb9585e84baa8bd9d7a5f020f52a6.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=jenyYQzue1,{'value': 'MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning'},Zayne Rea Sprague; Xi Ye; Kaj Bostrom; Swarat Chaudhuri; Greg Durrett,~Zayne_Rea_Sprague1; ~Xi_Ye2; ~Kaj_Bostrom1; ~Swarat_Chaudhuri1; ~Greg_Durrett1,"{'value': ['Large Language Models', 'Chain-of-Thought', 'Textual Reasoning']}","{'value': 'While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our data instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.'}",https://openreview.net{'value': '/pdf/0fd545b50f3dd67f4d965d2b37b07aa5d08aba77.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=j511LaqEeP,{'value': 'Non-Exchangeable Conformal Risk Control'},António Farinhas; Chrysoula Zerva; Dennis Thomas Ulmer; Andre Martins,~António_Farinhas1; ~Chrysoula_Zerva1; ~Dennis_Thomas_Ulmer1; ~Andre_Martins1,"{'value': ['conformal prediction', 'conformal risk control', 'non-exchangeable data', 'uncertainty']}","{'value': 'Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best $F_1$-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing non-exchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. Our framework is flexible, makes very few assumptions, and allows weighting the data based on its relevance for a given test example; a careful choice of weights may result in tighter bounds, making our framework useful in the presence of change points, time series, or other forms of distribution drift. Experiments with both synthetic and real world data show the usefulness of our method.'}",https://openreview.net{'value': '/pdf/ab193b1a0d582e0dc8095741222c1b4912c9d193.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ix7rLVHXyY,{'value': 'Learning Performance-Improving Code Edits'},Alexander G Shypula; Aman Madaan; Yimeng Zeng; Uri Alon; Jacob R. Gardner; Yiming Yang; Milad Hashemi; Graham Neubig; Parthasarathy Ranganathan; Osbert Bastani; Amir Yazdanbakhsh,~Alexander_G_Shypula1; ~Aman_Madaan1; ~Yimeng_Zeng1; ~Uri_Alon1; ~Jacob_R._Gardner1; ~Yiming_Yang1; ~Milad_Hashemi1; ~Graham_Neubig1; ~Parthasarathy_Ranganathan1; ~Osbert_Bastani1; ~Amir_Yazdanbakhsh1,"{'value': ['Large Language Models', 'Retrieval Augmented Generation', 'Program Synthesis', 'Program Optimization', 'Fine-Tuning', 'Goal-Conditioning', 'Data Augmentation', 'Self-Play', 'Synthetic Dataset', 'Performance Optimization', 'Machine Learning for Code Optimization', 'Dataset']}","{'value': 'With the decline of Moore\'s law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. Simultaneously, pretrained large language models (LLMs) have demonstrated strong capabilities at solving a wide range of programming tasks. To that end, we introduce a framework for adapting LLMs to high-level program optimization. First, we curate a dataset of performance-improving edits made by human programmers of over 77,000 competitive C++ programming submission pairs, accompanied by extensive unit tests. A major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious ""improvements."" To isolate and reliably evaluate the impact of program optimizations, we design an environment based on the gem5 full system simulator, the de facto simulator used in academia and industry. Next, we propose a broad range of adaptation strategies for code optimization; for prompting, these include retrieval-based few-shot prompting and chain-of-thought, and for finetuning, these include performance-conditioned generation and synthetic data augmentation based on self-play. A combination of these techniques achieves a mean speedup of 6.86$\\times$ with eight generations, higher than average optimizations from individual programmers (3.66$\\times$). Using our model\'s fastest generations, we set a new upper limit on the fastest speedup possible for our dataset at 9.64$\\times$ compared to using the fastest human submissions available (9.56$\\times$).'}",https://openreview.net{'value': '/pdf/e94f139ce25197392e4252dc56ed557ff74094fd.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ikwEDva1JZ,{'value': 'How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations'},Tianyu Guo; Wei Hu; Song Mei; Huan Wang; Caiming Xiong; Silvio Savarese; Yu Bai,~Tianyu_Guo4; ~Wei_Hu1; ~Song_Mei1; ~Huan_Wang1; ~Caiming_Xiong1; ~Silvio_Savarese1; ~Yu_Bai1,"{'value': ['in-context learning', 'transformers', 'representation learning', 'learning theory', 'mechanistic understanding']}","{'value': 'While large language models based on the transformer architecture have demonstrated remarkable in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where existing theory and mechanistic understanding focus mostly on simple scenarios such as learning simple function classes. This paper takes initial steps on understanding ICL in more complex scenarios, by studying learning with \\emph{representations}. Concretely, we construct synthetic in-context learning problems with a compositional structure, where the label depends on the input through a possibly complex but \\emph{fixed} representation function, composed with a linear function that \\emph{differs} in each instance. By construction, the optimal ICL algorithm first transforms the inputs by the representation function, and then performs linear ICL on top of the transformed dataset. We show theoretically the existence of transformers that approximately implement such algorithms with mild depth and size.  Empirically, we find trained transformers consistently achieve near-optimal ICL performance in this setting, and exhibit the desired dissection where lower layers transforms the dataset and upper layers perform linear ICL. Through extensive probing and a new pasting experiment, we further reveal several mechanisms within the trained transformers, such as concrete copying behaviors on both the inputs and the representations, linear ICL capability of the upper layers alone, and a post-ICL representation selection mechanism in a harder mixture setting. These observed mechanisms align well with our theory and may shed light on how transformers perform ICL in more realistic scenarios.'}",https://openreview.net{'value': '/pdf/5d585a41f28be94e8dd4315c71f8f5ee62908b9d.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=igfDXfMvm5,{'value': 'USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields'},Moyang Li; Peng Wang; Lingzhe Zhao; Bangyan Liao; Peidong Liu,~Moyang_Li1; ~Peng_Wang29; ~Lingzhe_Zhao1; ~Bangyan_Liao1; ~Peidong_Liu3,"{'value': ['Neural Radiance Fields', 'Bundle Adjustment', 'Rolling Shutter']}","{'value': 'Neural Radiance Fields (NeRF) has received much attention recently due to its impressive capability to represent 3D scene and synthesize novel view images. Existing works usually assume that the input images are captured by a global shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter effect would also affect the accuracy of the camera pose estimation (e.g. via COLMAP), which further prevents the success of NeRF algorithm with RS images.\nIn this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and recover accurate camera motion trajectory simultaneously under the framework of NeRF, by modeling the physical image formation process of a RS camera.\nExperimental results demonstrate that USB-NeRF achieves better performance compared to prior works, in terms of RS effect removal, novel view image synthesis as well as camera motion estimation. Furthermore, our algorithm can also be used to recover high-fidelity high frame-rate global shutter video from a sequence of RS images.'}",https://openreview.net{'value': '/pdf/4612afe3654a547a44dc4e5e94071ad6b9f1e4b2.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=iad1yyyGme,{'value': 'CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery'},Yuxiao Cheng; Ziqian Wang; Tingxiong Xiao; Qin Zhong; Jinli Suo; Kunlun He,~Yuxiao_Cheng1; ~Ziqian_Wang1; ~Tingxiong_Xiao1; ~Qin_Zhong1; ~Jinli_Suo1; ~Kunlun_He1,"{'value': ['Time series', 'Causal discovery', 'Neural networks', 'Benchmark', 'Dataset']}","{'value': ""Time-series causal discovery (TSCD) is a fundamental problem of machine learning. However,  existing synthetic datasets cannot properly evaluate or predict the algorithms' performance on real data. This study introduces the CausalTime pipeline to generate time-series that highly resemble the real data and with ground truth causal graphs for quantitative performance evaluation. The pipeline starts from real observations in a specific scenario and produces a matching benchmark dataset. Firstly, we harness deep neural networks along with normalizing flow to accurately capture realistic dynamics. Secondly, we extract hypothesized causal graphs by performing importance analysis on the neural network or leveraging prior knowledge. Thirdly, we derive the ground truth causal graphs by splitting the causal model into causal term, residual term, and noise term. Lastly, using the fitted network and the derived causal graph, we generate corresponding versatile time-series proper for algorithm assessment. In the experiments, we validate the fidelity of the generated data through qualitative and quantitative experiments, followed by a benchmarking of existing TSCD algorithms using these generated datasets. CausalTime offers a feasible solution to evaluating TSCD algorithms in real applications and can be generalized to a wide range of fields. For easy use of the proposed approach, we also provide a user-friendly website, hosted on www.causaltime.cc.""}",https://openreview.net{'value': '/pdf/58ab28fc32a66d671eae2d327b9bf9ff5767b383.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ia9fKO1Vjq,{'value': 'Identifiable Latent Polynomial Causal Models through the Lens of Change'},Yuhang Liu; Zhen Zhang; Dong Gong; Mingming Gong; Biwei Huang; Anton van den Hengel; Kun Zhang; Javen Qinfeng Shi,~Yuhang_Liu1; ~Zhen_Zhang2; ~Dong_Gong1; ~Mingming_Gong1; ~Biwei_Huang1; ~Anton_van_den_Hengel1; ~Kun_Zhang1; ~Javen_Qinfeng_Shi1,"{'value': ['Latent Causal Models', 'Causal Representations', 'Identifiability', 'Representation Learning']}","{'value': 'Causal representation learning aims to unveil latent high-level causal representations from observed low-level data. One of its primary tasks is to provide reliable assurance of identifying these latent causal models, known as \\textit{identifiability}. A recent breakthrough explores identifiability by leveraging the change of causal influences among latent causal variables across multiple environments \\citep{liu2022identifying}. However, this progress rests on the assumption that the causal relationships among latent causal variables adhere strictly to linear Gaussian models. In this paper, we extend the scope of latent causal models to involve nonlinear causal relationships, represented by polynomial models, and general noise distributions conforming to the exponential family. Additionally, we investigate the necessity of imposing changes on all causal parameters and present partial identifiability results when part of them remains unchanged. Further, we propose a novel empirical estimation method, grounded in our theoretical finding, that enables learning consistent latent causal representations. Our experimental results, obtained from both synthetic and real-world data, validate our theoretical contributions concerning identifiability and consistency.'}",https://openreview.net{'value': '/pdf/d3926b6a4489ce07972dbf8ea063435da922e861.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=i9Vs5NGDpk,"{'value': 'Asymptotically Free Sketched Ridge Ensembles: Risks, Cross-Validation, and Tuning'}",Pratik Patil; Daniel LeJeune,~Pratik_Patil1; ~Daniel_LeJeune1,"{'value': ['asymptotic freeness', 'sketching', 'ensembles', 'ridge regression', 'generalized cross-validation', 'tuning']}","{'value': 'We employ random matrix theory to establish consistency of generalized cross validation (GCV) for estimating prediction risks of sketched ridge regression ensembles, enabling efficient and consistent tuning of regularization and sketching parameters. Our results hold for a broad class of asymptotically free sketches under very mild data assumptions. For squared prediction risk, we provide a decomposition into an unsketched equivalent implicit ridge bias and a sketching-based variance, and prove that the risk can be globally optimized by only tuning sketch size in infinite ensembles. For general subquadratic prediction risk functionals, we extend GCV to construct consistent risk estimators, and thereby obtain distributional convergence of the GCV-corrected predictions in Wasserstein-2 metric. This in particular allows construction of prediction intervals with asymptotically correct coverage conditional on the training data. We also propose an ""ensemble trick"" whereby the risk for unsketched ridge regression can be efficiently estimated via GCV using small sketched ridge ensembles. We empirically validate our theoretical results using both synthetic and real large-scale datasets with practical sketches including CountSketch and subsampled randomized discrete cosine transforms.'}",https://openreview.net{'value': '/pdf/28ddc17b08b8df3bdcb23d2393cdb2d882b39eef.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=i8PjQT3Uig,{'value': 'Locality Sensitive Sparse Encoding for Learning World Models Online'},Zichen Liu; Chao Du; Wee Sun Lee; Min Lin,~Zichen_Liu1; ~Chao_Du1; ~Wee_Sun_Lee1; ~Min_Lin1,"{'value': ['model-based rl', 'online learning', 'incremental learning', 'catastrophic forgetting']}","{'value': 'Acquiring an accurate world model $\\textit{online}$ for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even with very high dimensional nonlinear features. We validate the representation power of our encoding and verify that it allows efficient online learning under data covariate shift. We also show, in the Dyna MBRL setting, that our world models learned online using a $\\textit{single pass}$ of trajectory data either surpass or match the performance of deep world models trained with replay and other continual learning methods.'}",https://openreview.net{'value': '/pdf/0149a804bccf18bec867875f9e9fb664c4288f4b.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=huGECz8dPp,{'value': 'Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression'},Ivan Butakov; Alexander Tolmachev; Sofia Malanchuk; Anna Neopryatnaya; Alexey Frolov; Kirill Andreev,~Ivan_Butakov1; ~Alexander_Tolmachev1; ~Sofia_Malanchuk1; ~Anna_Neopryatnaya1; ~Alexey_Frolov1; ~Kirill_Andreev1,"{'value': ['deep learning', 'information bottleneck principle', 'stochastic neural networks', 'lossy compression']}","{'value': 'The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs). Its essence lies in tracking the dynamics of two mutual information (MI) values: between the hidden layer output and the DNN input/target. According to the hypothesis put forth by Shwartz-Ziv & Tishby (2017), the training process consists of two distinct phases: fitting and compression. The latter phase is believed to account for the good generalization performance exhibited by DNNs. Due to the challenging nature of estimating MI between high-dimensional random vectors, this hypothesis was only partially verified for NNs of tiny sizes or specific types, such as quantized NNs. In this paper, we introduce a framework for conducting IB analysis of general NNs. Our approach leverages the stochastic NN method proposed by Goldfeld et al. (2019) and incorporates a compression step to overcome the obstacles associated with high dimensionality. In other words, we estimate the MI between the compressed representations of high-dimensional random vectors. The proposed method is supported by both theoretical and practical justifications. Notably, we demonstrate the accuracy of our estimator through synthetic experiments featuring predefined MI values and comparison with MINE (Belghazi et al., 2018). Finally, we perform IB analysis on a close-to-real-scale convolutional DNN, which reveals new features of the MI dynamics.'}",https://openreview.net{'value': '/pdf/30faaa47d472289085b65d4e2599be690c2b0a2b.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=hWS4MueyzC,{'value': 'Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World'},Rujie Wu; Xiaojian Ma; Zhenliang Zhang; Wei Wang; Qing Li; Song-Chun Zhu; Yizhou Wang,~Rujie_Wu2; ~Xiaojian_Ma1; ~Zhenliang_Zhang2; ~Wei_Wang4; ~Qing_Li1; ~Song-Chun_Zhu1; ~Yizhou_Wang1,"{'value': ['Few-shot learning', 'Visual reasoning', 'Open world learning']}","{'value': 'We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2)  real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme. We even conceived a neuro-symbolic reasoning approach that reconciles LLMs & VLMs with logical reasoning to emulate the human problem-solving process for Bongard Problems. However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%. We hope Bongard-OpenWorld can help us better understand the limitations of current visual intelligence and facilitate future research on visual agents with stronger few-shot visual reasoning capabilities.'}",https://openreview.net{'value': '/pdf/ffa49aeaac1a4b4b4b728c20ae009f257bbcf630.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=hNhwSmtXRh,{'value': 'Lemur: Harmonizing Natural Language and Code for Language Agents'},Yiheng Xu; Hongjin SU; Chen Xing; Boyu Mi; Qian Liu; Weijia Shi; Binyuan Hui; Fan Zhou; Yitao Liu; Tianbao Xie; Zhoujun Cheng; Siheng Zhao; Lingpeng Kong; Bailin Wang; Caiming Xiong; Tao Yu,~Yiheng_Xu1; ~Hongjin_SU1; ~Chen_Xing2; ~Boyu_Mi1; ~Qian_Liu2; ~Weijia_Shi1; ~Binyuan_Hui1; ~Fan_Zhou6; ~Yitao_Liu2; ~Tianbao_Xie1; ~Zhoujun_Cheng1; ~Siheng_Zhao1; ~Lingpeng_Kong1; ~Bailin_Wang3; ~Caiming_Xiong1; ~Tao_Yu5,"{'value': ['large language model', 'agent', 'code generation', 'reasoning', 'decision making']}","{'value': 'We introduce Lemur and Lemur-Chat, openly accessible language models optimized\nfor both natural language and coding capabilities to serve as the backbone\nof versatile language agents. The evolution from language chat models to\nfunctional language agents demands that models not only master human interaction,\nreasoning, and planning but also ensure grounding in the relevant environments.\nThis calls for a harmonious blend of language and coding capabilities\nin the models. Lemur and Lemur-Chat are proposed to address this necessity,\ndemonstrating balanced proficiencies in both domains, unlike existing\nopen-source models that tend to specialize in either. Through meticulous pretraining\nusing a code-intensive corpus and instruction fine-tuning on text and code\ndata, our models achieve state-of-the-art averaged performance across diverse\ntext and coding benchmarks. Comprehensive experiments demonstrate Lemur’s\nsuperiority over existing open-source models and its proficiency across various\nagent tasks involving human communication, tool usage, and interaction under\nfully- and partially- observable environments. The harmonization between natural\nand programming languages enables Lemur-Chat to significantly narrow the\ngap with proprietary models on agent abilities, providing key insights into developing\nadvanced open-source agents adept at reasoning, planning, and operating\nseamlessly across environments. Our model and code have been open-sourced at\nhttps://github.com/OpenLemur/Lemur.'}",https://openreview.net{'value': '/pdf/8ef62990871ebf2cac77dc6ea498085f167f070a.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=h7DGnWGeos,{'value': 'Active Retrosynthetic Planning Aware of Route Quality'},Luotian Yuan; Yemin Yu; Ying Wei; Yongwei Wang; Zhihua Wang; Fei Wu,~Luotian_Yuan1; ~Yemin_Yu1; ~Ying_Wei1; ~Yongwei_Wang1; ~Zhihua_Wang4; ~Fei_Wu1,"{'value': ['Retrosynthetic planning', 'route evaluation', 'reinforcement learning']}","{'value': 'Retrosynthetic planning is a sequential decision-making process of identifying synthetic routes from the available building block materials to reach a desired target molecule.\nThough existing planning approaches show promisingly high solving rates and low costs, the trivial route cost evaluation via pre-trained forward reaction prediction models certainly falls short of real-world chemical practice.\nAn alternative option is to annotate the actual cost of a route, such as yield, through chemical experiments or input from chemists, while \nthis often leads to substantial query costs.\nIn order to strike the balance between query costs and route quality evaluation, we propose an Active Retrosynthetic Planning (ARP) framework that remains compatible with the established retrosynthetic planners.\nOn one hand, the proposed ARP trains an actor that decides whether to query the cost of a reaction; on the other hand, it resorts to a critic to estimate the value of a molecule with its preceding reaction cost as input. \nThose molecules with low reaction costs are preferred to expand first.\nWe apply our framework to different existing approaches on both the benchmark and an expert dataset and demonstrate that it outperforms the existing state-of-the-art approach by 6.2\\% in route quality while reducing the query cost by 12.8\\%.\nIn addition, \nARP consistently plans \nhigh-quality routes with either abundant or sparse annotations.'}",https://openreview.net{'value': '/pdf/0eba4d55c6b2d267c53e120ccbcbe1c0a441de5a.pdf'},{'title_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=h57gkDO2Yg,{'value': 'Self-Supervised Dataset Distillation for Transfer Learning'},Dong Bok Lee; Seanie Lee; Joonho Ko; Kenji Kawaguchi; Juho Lee; Sung Ju Hwang,~Dong_Bok_Lee1; ~Seanie_Lee1; ~Joonho_Ko1; ~Kenji_Kawaguchi1; ~Juho_Lee2; ~Sung_Ju_Hwang1,"{'value': ['Dataset Distillation', 'Self-supervised Learning', 'Kernel Ridge Regression']}","{'value': ""Dataset distillation aims to optimize a small set so that a model trained on the set achieves performance similar to that of a model trained on the full dataset. While many supervised methods have achieved remarkable success in distilling a large dataset into a small set of representative samples, however, \nthey are not designed to produce a distilled dataset that can be effectively used to facilitate self-supervised pre-training. To this end, we propose a novel problem of distilling an unlabeled dataset into a set of small synthetic samples for efficient self-supervised learning (SSL). We first prove that a gradient of synthetic samples with respect to a SSL objective in naive bilevel optimization is \\textit{biased} due to the randomness originating from data augmentations or masking for inner optimization. To address this issue, we propose to minimize the mean squared error (MSE) between a model's representations of the synthetic examples and their corresponding learnable target feature representations for the inner objective, which does not introduce any randomness. Our primary motivation is that the model obtained by the proposed inner optimization can mimic the \\textit{self-supervised target model}. To achieve this, we also introduce the MSE between representations of the inner model and the self-supervised target model on the original full dataset for outer optimization. We empirically validate the effectiveness of our method on transfer learning. Our code is available at \nhttps://github.com/db-Lee/selfsup_dd""}",https://openreview.net{'value': '/pdf/8e0804ccc9e156127635c129ed31835d637a188e.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=gwbQ2YwLhD,{'value': 'Learning Large DAGs is Harder than you Think: Many Losses are Minimal for the Wrong DAG'},Jonas Seng; Matej Zečević; Devendra Singh Dhami; Kristian Kersting,~Jonas_Seng1; ~Matej_Zečević1; ~Devendra_Singh_Dhami1; ~Kristian_Kersting1,"{'value': ['Directed Acyclic Graphs', 'Structure Learning']}","{'value': 'Structure learning is a crucial task in science, especially in fields such as medicine and biology, where the wrong identification of (in)dependencies among random variables can have significant implications. The primary objective of structure learning is to learn a Directed Acyclic Graph (DAG) that represents the underlying probability distribution of the data. Many prominent DAG learners rely on least square losses or log-likelihood losses for optimization. It is well-known from regression models that least square losses are heavily influenced by the scale of the variables. Recently it has been demonstrated that the scale of data also affects performance of structure learning algorithms, though with a strong focus on linear 2-node systems and simulated data. Moving beyond these results, we provide conditions under which square-based losses are minimal for wrong DAGs in $d$-dimensional cases. Furthermore, we also show that scale can impair performance of structure learners if relations among variables are non-linear for both square based and log-likelihood based losses. We confirm our theoretical findings through extensive experiments on synthetic and real-world data.'}",https://openreview.net{'value': '/pdf/e06479f1378de21f8c0a0bf06e6768aa6a939ab0.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=gwDuW7Ok5f,{'value': 'Dual Associated Encoder for Face Restoration'},YU-JU TSAI; Yu-Lun Liu; Lu Qi; Kelvin C.K. Chan; Ming-Hsuan Yang,~YU-JU_TSAI1; ~Yu-Lun_Liu2; ~Lu_Qi1; ~Kelvin_C.K._Chan1; ~Ming-Hsuan_Yang1,"{'value': ['Blind Face Restoration', 'Vector Quantization', 'Codebook Prior', 'Feature Encoding', 'Feature Representation']}","{'value': 'Restoring facial details from low-quality (LQ) images has remained challenging due to the nature of the problem caused by various degradations in the wild. \nThe codebook prior has been proposed to address the ill-posed problems by leveraging an autoencoder and learned codebook of high-quality (HQ) features, achieving remarkable quality.\nHowever, existing approaches in this paradigm frequently depend on a single encoder pre-trained on HQ data for restoring HQ images, disregarding the domain gap and distinct feature representations between LQ and HQ images.\nAs a result, encoding LQ inputs with the same encoder could be insufficient, resulting in imprecise feature representation and leading to suboptimal performance.\nTo tackle this problem, we propose a novel dual-branch framework named $\\textit{DAEFR}$. Our method introduces an auxiliary LQ branch that extracts domain-specific information from the LQ inputs. \nAdditionally, we incorporate association training to promote effective synergy between the two branches, enhancing code prediction and restoration quality.\nWe evaluate the effectiveness of DAEFR on both synthetic and real-world datasets, demonstrating its superior performance in restoring facial details.\nProject page: https://liagm.github.io/DAEFR/'}",https://openreview.net{'value': '/pdf/385339f234d79a0ddfed8a993d63bd3833de1bfc.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=guRNebwZBb,{'value': 'CausalLM is not optimal for in-context learning'},Nan Ding; Tomer Levinboim; Jialin Wu; Sebastian Goodman; Radu Soricut,~Nan_Ding3; ~Tomer_Levinboim1; ~Jialin_Wu1; ~Sebastian_Goodman1; ~Radu_Soricut2,"{'value': ['in-context learning', 'causalLM', 'prefixLM']}","{'value': 'Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.'}",https://openreview.net{'value': '/pdf/2532c5a044ed2dc8f0ebf02dcac4ffabcb17d685.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=glwwbaeKm2,{'value': 'VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks'},Zhaomin Wu; Junyi Hou; Bingsheng He,~Zhaomin_Wu1; ~Junyi_Hou1; ~Bingsheng_He1,"{'value': ['Vertical federated learning', 'benchmark', 'feature correlation', 'feature importance']}","{'value': 'Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.'}",https://openreview.net{'value': '/pdf/a8cf2e01c93c35a2ae7c336331cf9de773a11782.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=gd0lAEtWso,{'value': 'OmniControl: Control Any Joint at Any Time for Human Motion Generation'},Yiming Xie; Varun Jampani; Lei Zhong; Deqing Sun; Huaizu Jiang,~Yiming_Xie2; ~Varun_Jampani2; ~Lei_Zhong1; ~Deqing_Sun2; ~Huaizu_Jiang1,"{'value': ['3D human motion generation', 'diffusion models', 'conditional generation']}","{'value': 'We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model. Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals. At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion. Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism. By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints. Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints. Project page: https://neu-vi.github.io/omnicontrol/.'}",https://openreview.net{'value': '/pdf/ccde3adf1de96ef348db1adc995af579e407bada.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=gctmyMiPHH,{'value': 'Feature Collapse'},Thomas Laurent; James von Brecht; Xavier Bresson,~Thomas_Laurent1; ~James_von_Brecht1; ~Xavier_Bresson6,"{'value': ['deep learning theory', 'representation learning', 'neural collapse']}","{'value': ""We formalize and study a phenomenon called *feature collapse* that makes precise the intuitive idea that entities playing a similar role in a learning task receive similar representations. As feature collapse requires a notion of task, we leverage a synthetic task in which a learner must classify  `sentences' constituted of $L$ tokens. We start by showing experimentally that feature collapse goes hand in hand with generalization. We then prove that, in the large sample limit,  distinct tokens that play identical roles in the task receive identical local feature representations in the first layer of the network. This analysis shows that a neural network trained on this task provably learns interpretable and meaningful representations in its first layer.""}",https://openreview.net{'value': '/pdf/a0cce1aa4b612c7319f3c67cc9659b27b469a8a3.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=gFR4QwK53h,{'value': 'Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View'},Haoyue Dai; Ignavier Ng; Gongxu Luo; Peter Spirtes; Petar Stojanov; Kun Zhang,~Haoyue_Dai1; ~Ignavier_Ng1; ~Gongxu_Luo1; ~Peter_Spirtes1; ~Petar_Stojanov2; ~Kun_Zhang1,"{'value': ['Gene regulatory network', 'Single-cell RNA-sequencing', 'Dropout', 'Zero-inflated data', 'Causal model', 'Causal discovery', 'Nonparametric']}","{'value': 'Gene regulatory network inference (GRNI) is a challenging problem, particularly owing to the presence of zeros in single-cell RNA sequencing data: some are biological zeros representing no gene expression, while some others are technical zeros arising from the sequencing procedure (aka dropouts), which may bias GRNI by distorting the joint distribution of the measured gene expressions. Existing approaches typically handle dropout error via imputation, which may introduce spurious relations as the true joint distribution is generally unidentifiable. To tackle this issue, we introduce a causal graphical model to characterize the dropout mechanism, namely, Causal Dropout Model. We provide a simple yet effective theoretical result: interestingly, the conditional independence (CI) relations in the data with dropouts, after deleting the samples with zero values (regardless if technical or not) for the conditioned variables, are asymptotically identical to the CI relations in the original data without dropouts. This particular test-wise deletion procedure, in which we perform CI tests on the samples without zeros for the conditioned variables, can be seamlessly integrated with existing structure learning approaches including constraint-based and greedy score-based methods, thus giving rise to a principled framework for GRNI in the presence of dropouts. We further show that the causal dropout model can be validated from data, and many existing statistical models to handle dropouts fit into our model as specific parametric instances. Empirical evaluation on synthetic, curated, and real-world experimental transcriptomic data comprehensively demonstrate the efficacy of our method.'}",https://openreview.net{'value': '/pdf/69813094585730931fce711a92e4bb53d955e2dd.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=gEwKAZZmSw,{'value': 'Efficient Backpropagation with Variance Controlled Adaptive Sampling'},Ziteng Wang; Jianfei Chen; Jun Zhu,~Ziteng_Wang3; ~Jianfei_Chen1; ~Jun_Zhu2,"{'value': ['efficient training algorithms', 'stochastic gradient descent', 'importance sampling', 'variance reduction']}","{'value': 'Sampling-based algorithms, which eliminate ""unimportant"" computations during forward and/or backpropagation (BP), offer potential solutions to accelerate neural network training. However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks. In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to minimize the computational load of BP. VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation. To preserve accuracy, we control the additional variance introduced by learning the sample ratio jointly with model parameters during training. We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains. On all the tasks, VCAS can preserve the original training loss trajectory and validation accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction of the whole training process. The implementation is available at https://github.com/thu-ml/VCAS.'}",https://openreview.net{'value': '/pdf/e97e89b7aa6ea85d256619491b323e36ff1165b2.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=g6eCbercEc,{'value': 'InfoCon: Concept Discovery with Generative and Discriminative Informativeness'},Ruizhe Liu; Qian Luo; Yanchao Yang,~Ruizhe_Liu1; ~Qian_Luo1; ~Yanchao_Yang1,"{'value': ['concept discovery', 'robotic manipulation', 'self-supervised learning']}","{'value': 'We focus on the self-supervised discovery of manipulation concepts that can be adapted and reassembled to address various robotic tasks. We propose that the decision to conceptualize a physical procedure should not depend on how we name it (semantics) but rather on the significance of the informativeness in its representation regarding the low-level physical state and state changes. We model manipulation concepts -- discrete symbols -- as generative and discriminative goals and derive metrics that can autonomously link them to meaningful sub-trajectories from noisy, unlabeled demonstrations. Specifically, we employ a trainable codebook containing encodings --symbols -- capable of synthesizing the end-state of a sub-trajectory given the current state (generative informativeness). Moreover, the encoding corresponding to a particular sub-trajectory should differentiate the state within and outside it and confidently predict the subsequent action based on the gradient of its discriminative score (discriminative informativeness). These metrics, which do not rely on human annotation, can be seamlessly integrated into a VQ-VAE framework, enabling the partitioning of demonstrations into semantically consistent sub-trajectories, fulfilling the purpose of discovering manipulation concepts and the corresponding (sub)-goal states. We evaluate the effectiveness of the learned concepts by training policies that utilize them as guidance, demonstrating superior performance compared to other baselines. Additionally, our discovered manipulation concepts compare favorably to human-annotated ones, while saving much manual effort. The code and trained models will be made public.'}",https://openreview.net{'value': '/pdf/a4b3a7d01c5dc3f4d140f8da45983050a3301975.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=f1xnBr4WD6,{'value': 'Cycle Consistency Driven Object Discovery'},Aniket Rajiv Didolkar; Anirudh Goyal; Yoshua Bengio,~Aniket_Rajiv_Didolkar1; ~Anirudh_Goyal1; ~Yoshua_Bengio1,"{'value': ['cycle consistency', 'object discovery', 'downstream RL']}","{'value': ""Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. While these approaches have shown promise in certain scenarios, they still exhibit certain limitations. First, they rely on architectural priors which can be unreliable and usually require meticulous engineering to identify the correct objects. Second, there has been a notable gap in investigating the practical utility of these representations in downstream tasks. To address the first limitation, we introduce a method that explicitly optimizes the constraint that each object in a scene should be associated with a distinct slot. We formalize this constraint by introducing  consistency objectives which are cyclic in nature. By integrating these consistency objectives into various existing slot-based object-centric methods, we showcase substantial improvements in object-discovery performance. These enhancements consistently hold true across both synthetic and real-world scenes, underscoring the effectiveness and adaptability of the proposed approach. To tackle the second limitation, we apply the learned object-centric representations from the proposed method to two downstream reinforcement learning tasks, demonstrating considerable performance enhancements compared to conventional slot-based and monolithic representation learning methods. Our results suggest that the proposed approach not only improves object discovery, but also provides richer features for downstream tasks.""}",https://openreview.net{'value': '/pdf/18ace982ecbf580ad919f876edd9b3a6e1652550.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=eiF7TU1E8E,{'value': 'SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer'},Yuhta Takida; Masaaki Imaizumi; Takashi Shibuya; Chieh-Hsin Lai; Toshimitsu Uesaka; Naoki Murata; Yuki Mitsufuji,~Yuhta_Takida1; ~Masaaki_Imaizumi1; ~Takashi_Shibuya1; ~Chieh-Hsin_Lai2; ~Toshimitsu_Uesaka1; ~Naoki_Murata1; ~Yuki_Mitsufuji1,"{'value': ['Generative adversarial network', 'optimal transport', 'sliced Wasserstein distance']}","{'value': 'Generative adversarial networks (GANs) learn a target probability distribution by optimizing a generator and a discriminator with minimax objectives. This paper addresses the question of whether such optimization actually provides the generator with gradients that make its distribution close to the target distribution. We derive *metrizable conditions*, sufficient conditions for the discriminator to serve as the distance between the distributions, by connecting the GAN formulation with the concept of sliced optimal transport. Furthermore, by leveraging these theoretical results, we propose a novel GAN training scheme called the Slicing Adversarial Network (SAN). With only simple modifications, a broad class of existing GANs can be converted to SANs. Experiments on synthetic and image datasets support our theoretical results and the effectiveness of SAN as compared to the usual GANs. We also apply SAN to StyleGAN-XL, which leads to a state-of-the-art FID score amongst GANs for class conditional generation on CIFAR10 and ImageNet 256$\\times$256. The code is available at https://github.com/sony/san.'}",https://openreview.net{'value': '/pdf/e24f00c0e961a02784afb5aaa4e6f670cb1640ab.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=efeBC1sQj9,{'value': 'SEPT: Towards Efficient Scene Representation Learning for Motion Prediction'},Zhiqian Lan; Yuxuan Jiang; Yao Mu; Chen Chen; Shengbo Eben Li,~Zhiqian_Lan1; ~Yuxuan_Jiang1; ~Yao_Mu1; ~Chen_Chen42; ~Shengbo_Eben_Li2,"{'value': ['motion prediction', 'autonomous driving', 'self-supervised learning']}","{'value': ""Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse 2 motion forecasting benchmarks, outperforming previous methods on all main metrics by a large margin.""}",https://openreview.net{'value': '/pdf/8087e5328150c9164f4ed4ea5d61cb9eadb1c690.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=eY7sLb0dVF,{'value': 'Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs'},Ilan Naiman; N. Benjamin Erichson; Pu Ren; Michael W. Mahoney; Omri Azencot,~Ilan_Naiman1; ~N._Benjamin_Erichson1; ~Pu_Ren1; ~Michael_W._Mahoney1; ~Omri_Azencot1,"{'value': ['Time Series Generation', 'Koopman Theory; Variational Autoencoder; Generative Modeling']}","{'value': 'Generating realistic time series data is important for many engineering and scientific applications. \nExisting work tackles this problem using generative adversarial networks (GANs).\nHowever, GANs are unstable during training, and they can suffer from mode collapse. \nWhile variational autoencoders (VAEs) are known to be more robust to the these issues, they are (surprisingly) less considered for time series generation. \nIn this work, we introduce Koopman VAE (KoVAE), a new generative framework that is based on a novel design for the model prior, and that can be optimized for either regular and irregular training data. \nInspired by Koopman theory, we represent the latent conditional prior dynamics using a linear map. \nOur approach enhances generative modeling with two desired features: (i) incorporating domain knowledge can be achieved by leveraging spectral tools that prescribe constraints on the eigenvalues of the linear map; and (ii) studying the qualitative behavior and stability of the system can be performed using tools from dynamical systems theory. \nOur results show that KoVAE outperforms state-of-the-art GAN and VAE methods across several challenging synthetic and real-world time series generation benchmarks. \nWhether trained on regular or irregular data, KoVAE generates time series that improve both discriminative and predictive metrics. \nWe also present visual evidence suggesting that KoVAE learns probability density functions that better approximate the empirical ground truth distribution.'}",https://openreview.net{'value': '/pdf/fb39133e3e34567def62064e0567dfe5b9e688b3.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ePOjNlOjLC,{'value': 'Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation'},Ruoyu Wang; Yongqi Yang; Zhihao Qian; Ye Zhu; Yu Wu,~Ruoyu_Wang6; ~Yongqi_Yang1; ~Zhihao_Qian1; ~Ye_Zhu3; ~Yu_Wu3,{'value': ['Diffusion Models；text-vision Condition；']},"{'value': 'Originating from the diffusion phenomenon in physics that describes particle movement, the diffusion generative models inherit the characteristics of stochastic random walk in the data space along the denoising trajectory. However, the intrinsic mutual interference among image regions contradicts the need for practical downstream application scenarios where the preservation of low-level pixel information from given conditioning is desired (e.g., customization tasks like personalized generation and inpainting based on a user-provided single image). In this work, we investigate the diffusion (physics) in diffusion (machine learning) properties and propose our Cyclic One-Way Diffusion (COW) method to control the direction of diffusion phenomenon given a pre-trained frozen diffusion model for versatile customization application scenarios, where the low-level pixel information from the conditioning needs to be preserved. Notably, unlike most current methods that incorporate additional conditions by fine-tuning the base text-to-image diffusion model or learning auxiliary networks, our method provides a novel perspective to understand the task needs and is applicable to a wider range of customization scenarios in a learning-free manner. Extensive experiment results show that our proposed COW can achieve more flexible customization based on strict visual conditions in different application settings. Project page: https://wangruoyu02.github.io/cow.github.io/.'}",https://openreview.net{'value': '/pdf/1ed153e30901516c66436cf1fc6202ddeebe5ce3.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=eMNN0wIyVw,{'value': 'On Trajectory Augmentations for Off-Policy Evaluation'},Ge Gao; Qitong Gao; Xi Yang; Song Ju; Miroslav Pajic; Min Chi,~Ge_Gao4; ~Qitong_Gao1; ~Xi_Yang11; ~Song_Ju1; ~Miroslav_Pajic2; ~Min_Chi1,"{'value': ['Trajectory augmentation', 'Off-policy evaluation', 'Sub-trajectory mining from offline dataset']}","{'value': 'In the realm of reinforcement learning (RL), off-policy evaluation (OPE) holds a pivotal position, especially in high-stake human-involved scenarios such as e-learning and healthcare. Applying OPE to these domains is often challenging with scarce and underrepresentative offline training trajectories. Data augmentation has been a successful technique to enrich training data. However, directly employing existing data augmentation methods to OPE may not be feasible, due to the Markovian nature within the offline trajectories and the desire for generalizability across diverse target policies. In this work, we propose an offline trajectory augmentation approach to specifically facilitate OPE in human-involved scenarios. We propose sub-trajectory mining to extract potentially valuable sub-trajectories from offline data, and diversify the behaviors within those sub-trajectories by varying coverage of the state-action space. Our work was empirically evaluated in a wide array of environments, encompassing both simulated scenarios and real-world domains like robotic control, healthcare, and e-learning, where the training trajectories include varying levels of coverage of the state-action space. By enhancing the performance of a variety of OPE methods, our work offers a promising path forward for tackling OPE challenges in situations where data may be limited or underrepresentative.'}",https://openreview.net{'value': '/pdf/ec9484bf2975dae1326d6ba008edb1cc61c0556c.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=dl0u4ODCuW,{'value': 'Retro-fallback: retrosynthetic planning in an uncertain world'},Austin Tripp; Krzysztof Maziarz; Sarah Lewis; Marwin Segler; José Miguel Hernández-Lobato,~Austin_Tripp1; ~Krzysztof_Maziarz1; ~Sarah_Lewis1; ~Marwin_Segler2; ~José_Miguel_Hernández-Lobato1,"{'value': ['Retrosynthesis', 'planning', 'chemistry', 'search']}","{'value': 'Retrosynthesis is the task of planning a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by algorithms may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.'}",https://openreview.net{'value': '/pdf/801c10026da69b3fde1f98639a39ec3e43286187.pdf'},{'title_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=djM3WzpOmK,{'value': 'Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent Geometries'},Haitz Sáez de Ocáriz Borde; Anastasis Kratsios,~Haitz_Sáez_de_Ocáriz_Borde1; ~Anastasis_Kratsios1,"{'value': ['Latent Graph Inference', 'Representation Learning', 'Metric Embedding', 'Geometric Deep Learning', 'Graph Neural Networks']}","{'value': ""The inductive bias of a graph neural network (GNN) is largely encoded in its specified graph. Latent graph inference relies on latent geometric representations to dynamically rewire or infer a GNN's graph to maximize the GNN's predictive downstream performance, but it lacks solid theoretical foundations in terms of embedding-based representation guarantees. This paper addresses this issue by introducing a trainable deep learning architecture, coined \\textit{neural snowflake}, that can adaptively implement fractal-like metrics on $\\mathbb{R}^d$. We prove that any given finite weights graph can be isometrically embedded by a standard MLP encoder. Furthermore, when the latent graph can be represented in the feature space of a sufficiently regular kernel, we show that the combined neural snowflake and MLP encoder do not succumb to the curse of dimensionality by using only a low-degree polynomial number of parameters in the number of nodes. This implementation enables a low-dimensional isometric embedding of the latent graph. We conduct synthetic experiments to demonstrate the superior metric learning capabilities of neural snowflakes when compared to more familiar spaces like Euclidean space.  Additionally, we carry out latent graph inference experiments on graph benchmarks. Consistently, the neural snowflake model achieves predictive performance that either matches or surpasses that of the state-of-the-art latent graph inference models. Importantly, this performance improvement is achieved without requiring random search for optimal latent geometry. Instead, the neural snowflake model achieves this enhancement in a differentiable manner.""}",https://openreview.net{'value': '/pdf/60e6db71b5bef3a528d29bef36ec6ca6e6fbbcff.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=dcjtMYkpXx,{'value': 'Reward Model Ensembles Help Mitigate Overoptimization'},Thomas Coste; Usman Anwar; Robert Kirk; David Krueger,~Thomas_Coste1; ~Usman_Anwar1; ~Robert_Kirk1; ~David_Krueger1,"{'value': ['ensembles', 'overoptimization', 'RLHF', 'reinforcement learning from human feedback', 'language models', 'uncertainty weighted optimization']}","{'value': 'Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the “true” reward, these learned reward models are susceptible to overoptimization. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger “gold” reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real-world conditions. Both with and without label noise we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.'}",https://openreview.net{'value': '/pdf/ef2bb264ca333c06129a9c23f5038f5566619ecb.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=dTpbEdN9kr,{'value': 'Human Motion Diffusion as a Generative Prior'},Yoni Shafir; Guy Tevet; Roy Kapon; Amit Haim Bermano,~Yoni_Shafir1; ~Guy_Tevet1; ~Roy_Kapon1; ~Amit_Haim_Bermano2,"{'value': ['diffusion', 'prior', 'composition', 'generation', 'motion', 'generative']}","{'value': 'Recent work has demonstrated the significant potential of denoising diffusion models\nfor generating human motion, including text-to-motion capabilities.\nHowever, these methods are restricted by the paucity of annotated motion data,\na focus on single-person motions, and a lack of detailed control.\nIn this paper, we introduce three forms of composition based on diffusion priors:\nsequential, parallel, and model composition.\nUsing sequential composition, we tackle the challenge of long sequence\ngeneration. We introduce DoubleTake, an inference-time method with which\nwe generate long animations consisting of sequences of prompted intervals\nand their transitions, using a prior trained only for short clips.\nUsing parallel composition, we show promising steps toward two-person generation.\nBeginning with two fixed priors as well as a few two-person training examples, we learn a slim\ncommunication block, ComMDM, to coordinate interaction between the two resulting motions.\nLastly, using model composition, we first train individual priors\nto complete motions that realize a prescribed motion for a given joint.\nWe then introduce DiffusionBlending, an interpolation mechanism to effectively blend several\nsuch models to enable flexible and efficient fine-grained joint and trajectory-level control and editing.\nWe evaluate the composition methods using an off-the-shelf motion diffusion model,\nand further compare the results to dedicated models trained for these specific tasks.'}",https://openreview.net{'value': '/pdf/713790c2a80588d36bcaaefffcf9b8f9c4117583.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=dPHLbUqGbr,"{'value': 'Fast, Expressive $\\mathrm{SE}(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space'}",Erik J Bekkers; Sharvaree Vadgama; Rob Hesselink; Putri A Van der Linden; David W. Romero,~Erik_J_Bekkers1; ~Sharvaree_Vadgama2; ~Rob_Hesselink1; ~Putri_A_Van_der_Linden1; ~David_W._Romero1,"{'value': ['Equivariance', 'Point Clouds', 'Message Passing Neural Network', 'Molecules', 'Diffusion Model']}","{'value': 'Based on the theory of homogeneous spaces we derive *geometrically optimal edge attributes* to be used within the flexible message-passing framework. We formalize the notion of weight sharing in convolutional networks as the sharing of message functions over point-pairs that should be treated equally. We define equivalence classes of point-pairs that are identical up to a transformation in the group and derive attributes that uniquely identify these classes. Weight sharing is then obtained by conditioning message functions on these attributes. As an application of the theory, we develop an efficient equivariant group convolutional network for processing 3D point clouds. The theory of homogeneous spaces tells us how to do group convolutions with feature maps over the homogeneous space of positions $\\mathbb{R}^3$, position and orientations $\\mathbb{R}^3 {\\times} S^2$, and the group $SE(3)$ itself. Among these, $\\mathbb{R}^3 {\\times} S^2$ is an optimal choice due to the ability to represent directional information, which $\\mathbb{R}^3$ methods cannot, and it significantly enhances computational efficiency compared to indexing features on the full $SE(3)$ group. We support this claim with state-of-the-art results —in accuracy and speed— on five different benchmarks in 2D and 3D, including interatomic potential energy prediction, trajectory forecasting in N-body systems, and generating molecules via equivariant diffusion models.\n\n*Code available at [https://github.com/ebekkers/ponita](https://github.com/ebekkers/ponita)*'}",https://openreview.net{'value': '/pdf/45bbf79dbc027921114b248f761f76649c8b04d7.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=dN4vpVTvWX,{'value': 'TUVF: Learning Generalizable Texture UV Radiance Fields'},An-Chieh Cheng; Xueting Li; Sifei Liu; Xiaolong Wang,~An-Chieh_Cheng1; ~Xueting_Li2; ~Sifei_Liu2; ~Xiaolong_Wang3,"{'value': ['texture synthesis', 'generative model', 'NeRF', '3D AIGC']}","{'value': 'Textures are a vital aspect of creating visually appealing and realistic 3D models. In this paper, we study the problem of generating high-fidelity texture given shapes of 3D assets, which has been relatively less explored compared with generic 3D shape modeling. Our goal is to facilitate a controllable texture generation process, such that one texture code can correspond to a particular appearance style independent of any input shapes from a category. We introduce Texture UV Radiance Fields (TUVF) that generate textures in a learnable UV sphere space rather than directly on the 3D shape. This allows the texture to be disentangled from the underlying shape and transferable to other shapes that share the same UV space, i.e., from the same category. We integrate the UV sphere space with the radiance field, which provides a more efficient and accurate representation of textures than traditional texture maps. We perform our experiments on synthetic and real-world object datasets where we achieve not only realistic synthesis but also substantial improvements over state-of-the-arts on texture controlling and editing.'}",https://openreview.net{'value': '/pdf/e3ed9ed07f700f15cfa92ddec1c528a47e174a1a.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=dGH4kHFKFj,{'value': 'GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models'},Haitao Yang; Xiangru Huang; Bo Sun; Chandrajit L. Bajaj; Qixing Huang,~Haitao_Yang1; ~Xiangru_Huang1; ~Bo_Sun6; ~Chandrajit_L._Bajaj1; ~Qixing_Huang1,{'value': ['Joint shape matching;shape generator;implicit representation;geometric regularization']},"{'value': 'This paper introduces GenCorres, a novel unsupervised joint shape matching (JSM) approach. Our key idea is to learn a mesh generator to fit an unorganized deformable shape collection while constraining deformations between adjacent synthetic shapes to preserve geometric structures such as local rigidity and local conformality. GenCorres presents three appealing advantages over existing JSM techniques. First, GenCorres performs JSM among a synthetic shape collection whose size is much bigger than the input shapes and fully leverages the datadriven power of JSM. Second, GenCorres unifies consistent shape matching and pairwise matching (i.e., by enforcing deformation priors between adjacent synthetic shapes). Third, the generator provides a concise encoding of consistent shape correspondences. However, learning a mesh generator from an unorganized shape collection is challenging, requiring a good initialization. GenCorres addresses this issue by learning an implicit generator from the input shapes, which provides intermediate shapes between two arbitrary shapes. We introduce a novel approach for computing correspondences between adjacent implicit surfaces, which we use to regularize the implicit generator. Synthetic shapes of the implicit generator then guide initial fittings (i.e., via template-based deformation) for learning the mesh generator. Experimental results show that GenCorres considerably outperforms state-of-the-art JSM techniques. The synthetic shapes of GenCorres also achieve salient performance gains against state-of-the-art deformable shape generators.'}",https://openreview.net{'value': '/pdf/3d4025afa45f3e8ebf52c557a2cb56df069072ab.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=cuAxSHcsSX,{'value': 'On Differentially Private Federated Linear Contextual Bandits'},Xingyu Zhou; Sayak Ray Chowdhury,~Xingyu_Zhou2; ~Sayak_Ray_Chowdhury1,"{'value': ['linear contextual bandits', 'federated learning', 'differential privacy']}","{'value': ""We consider cross-silo federated linear contextual bandit (LCB) problem under differential privacy, where multiple silos interact with their respective local users and communicate via a central server to realize collaboration without sacrificing each user's privacy. We identify three issues in the state-of-the-art~\\citep{dubey2020differentially}: (i) failure of claimed privacy protection, (ii) incorrect regret bound due to noise miscalculation and (iii) ungrounded communication cost. \nTo resolve these issues, we take a two-step approach. First, we design an algorithmic framework consisting of a generic federated LCB algorithm and flexible privacy protocols. Then, leveraging the proposed framework, we study federated LCBs under two different privacy constraints. We first establish privacy and regret guarantees under silo-level local differential privacy, which fix the issues present in state-of-the-art algorithm.\nTo further improve the regret performance, we next consider shuffle model of differential privacy, under which we show that our algorithm can achieve nearly ``optimal'' regret without a trusted server. \nWe accomplish this via two different schemes --  one relies on a new result on privacy amplification via shuffling for DP mechanisms and another one leverages the integration of a shuffle protocol for vector sum into the tree-based mechanism, both of which might be of independent interest. Finally, we support our theoretical results with\nnumerical evaluations over contextual bandit instances generated from both synthetic and real-life data.""}",https://openreview.net{'value': '/pdf/01e3e3b782a27d1e25f072db1cdadb1ac80b628d.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=coIaBY8EVF,{'value': 'Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces'},Omer Nahum; Gali Noti; David C. Parkes; Nir Rosenfeld,~Omer_Nahum1; ~Gali_Noti1; ~David_C._Parkes1; ~Nir_Rosenfeld2,"{'value': ['congestion', 'decongestion', 'online marketplaces', 'learning in economic settings', 'efficient allocation']}","{'value': 'Congestion is a common failure mode of markets, where consumers compete inefficiently on the same subset of goods (e.g.,  chasing the same small set of properties on a vacation rental platform). The typical economic story is that prices  decongest by balancing supply and demand. But in modern online marketplaces, prices are typically set in a decentralized way by sellers, and the information about items is inevitably partial. The power of a platform  is limited to controlling *representations*---the subset of information about items presented by default to users. This motivates the present study of *decongestion by representation*, where a platform seeks to learn representations that reduce  congestion and thus improve social welfare. The technical challenge is twofold: relying only on revealed preferences from the choices of consumers, rather than true preferences; and the combinatorial problem associated with representations that  determine the  features to reveal in the default view.  We tackle both challenges by proposing a *differentiable proxy of welfare* that can be trained end-to-end on consumer choice data. We develop sufficient conditions for when decongestion promotes welfare, and present the results of extensive experiments on both synthetic and real data that demonstrate the utility of our approach.'}",https://openreview.net{'value': '/pdf/547355cdf3a48450f2d99fc17cdc87ddd505a059.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=cUSNs8nGaV,{'value': 'GlucoBench: Curated List of Continuous Glucose Monitoring Datasets with Prediction Benchmarks'},Renat Sergazinov; Elizabeth Chun; Valeriya Rogovchenko; Nathaniel J Fernandes; Nicholas Kasman; Irina Gaynanova,~Renat_Sergazinov1; ~Elizabeth_Chun1; ~Valeriya_Rogovchenko1; ~Nathaniel_J_Fernandes1; ~Nicholas_Kasman1; ~Irina_Gaynanova1,"{'value': ['diabetes management', 'continuous glucose monitors (CGM)', 'glucose trajectory prediction', 'artificial pancreas systems', 'public datasets', 'standardized tasks', 'benchmark models', 'glycemic control']}","{'value': ""The rising rates of diabetes necessitate innovative methods for its management. Continuous glucose monitors (CGM) are small medical devices that measure blood glucose levels at regular intervals providing insights into daily patterns of glucose variation. Forecasting of glucose trajectories based on CGM data  holds the potential to substantially improve diabetes management, by both refining artificial pancreas systems and enabling individuals to make adjustments based on  predictions to maintain optimal glycemic range. Despite numerous methods proposed for CGM-based glucose trajectory prediction, these methods are typically evaluated on small, private datasets, impeding reproducibility, further research, and practical adoption. The absence of standardized prediction tasks and systematic comparisons between methods has led to uncoordinated research efforts, obstructing the identification of optimal tools for tackling specific challenges. As a result, only a limited number of prediction methods have been implemented in clinical practice.  \n\nTo address these challenges, we present a comprehensive resource that provides (1) a consolidated repository of curated publicly available CGM datasets to foster reproducibility and accessibility; (2) a standardized task list to unify research objectives and facilitate coordinated efforts; (3) a set of benchmark models with established baseline performance, enabling the research community to objectively gauge new methods' efficacy; and (4) a detailed analysis of performance-influencing factors for model development. We anticipate these resources to propel collaborative research endeavors in the critical domain of CGM-based glucose predictions. Our code is available online at github.com/IrinaStatsLab/GlucoBench.""}",https://openreview.net{'value': '/pdf/43f545b8172aa5500f599f53c7466c1b3897e5d0.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=bWNJFD1l8M,{'value': 'Transferring Learning Trajectories of Neural Networks'},Daiki Chijiwa,~Daiki_Chijiwa1,"{'value': ['neural networks', 'learning dynamics', 'permutation symmetry', 'loss landscape']}","{'value': 'Training deep neural networks (DNNs) is computationally expensive, which is problematic especially when performing duplicated or similar training runs in model ensemble or fine-tuning pre-trained models, for example. Once we have trained one DNN on some dataset, we have its learning trajectory (i.e., a sequence of intermediate parameters during training) which may potentially contain useful information for learning the dataset. However, there has been no attempt to utilize such information of a given learning trajectory for another training. In this paper, we formulate the problem of ""transferring"" a given learning trajectory from one initial parameter to another one (named *learning transfer problem*) and derive the first algorithm to approximately solve it by matching gradients successively along the trajectory via permutation symmetry. We empirically show that the transferred parameters achieve non-trivial accuracy before any direct training, and can be trained significantly faster than training from scratch.'}",https://openreview.net{'value': '/pdf/350c40175cd737693c26d330f15e330d29ae3452.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=bTMMNT7IdW,{'value': 'Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time'},QIUHAO Zeng; Changjian Shui; Long-Kai Huang; Peng Liu; Xi Chen; Charles Ling; Boyu Wang,~QIUHAO_Zeng1; ~Changjian_Shui2; ~Long-Kai_Huang1; ~Peng_Liu20; ~Xi_Chen32; ~Charles_Ling1; ~Boyu_Wang3,"{'value': ['Distribution Shift', 'Temporal Distribution Shift']}","{'value': 'Distribution shifts over time are common in real-world machine-learning applications. This scenario is formulated as Evolving Domain Generalization (EDG), where models aim to generalize well to unseen target domains in a time-varying system by learning and leveraging the underlying evolving pattern of the distribution shifts across domains. However, existing methods encounter challenges due to the limited number of timestamps (every domain corresponds to a timestamp) in EDG datasets, leading to difficulties in capturing evolving dynamics and risking overfitting to the sparse timestamps, which hampers their generalization and adaptability to new tasks. To address this limitation, we propose a novel approach SDE-EDG that collects the Infinitely Fined-Grid Evolving Trajectory (IFGET) of the data distribution with continuous-interpolated samples to bridge temporal gaps (intervals between two successive timestamps). Furthermore, by leveraging the inherent capacity of Stochastic Differential Equations (SDEs) to capture continuous trajectories, we propose their use to align SDE-modeled trajectories with IFGET across domains, thus enabling the capture of evolving distribution trends. We evaluate our approach on several benchmark datasets and demonstrate that it can achieve superior performance compared to existing state-of-the-art methods.'}",https://openreview.net{'value': '/pdf/2d6728dfffe50fd8e8627061ece7a1f07abc5462.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ZWyZeqE928,{'value': 'Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data'},Shikai Fang; Xin Yu; Zheng Wang; Shibo Li; Mike Kirby; Shandian Zhe,~Shikai_Fang2; ~Xin_Yu4; ~Zheng_Wang2; ~Shibo_Li1; ~Mike_Kirby1; ~Shandian_Zhe1,"{'value': ['Bayesian model', 'Tensor decomposition', 'Gaussian Processes']}","{'value': 'Tucker decomposition is a powerful tensor model to handle multi-aspect data. It demonstrates the low-rank property by decomposing the grid-structured data as interactions between a core tensor and a set of object representations (factors).  A fundamental assumption of such decomposition is that there are finite objects in each aspect or mode, corresponding to discrete indexes of data entries. However,  real-world data is often not naturally posed in this setting. For example, geographic data is represented as continuous indexes of latitude and longitude coordinates, and cannot fit tensor models directly. To generalize Tucker decomposition to such scenarios, we propose Functional Bayesian Tucker Decomposition (FunBaT). We treat the continuous-indexed data as the interaction between the Tucker core and a group of latent functions. We use Gaussian processes (GP)  as functional priors to model the latent functions. Then, we convert each GP into a state-space prior by constructing an equivalent stochastic differential equation (SDE)  to reduce computational cost. An efficient inference algorithm is developed for scalable posterior approximation based on advanced message-passing techniques. The advantage of our method is shown in both synthetic data and several real-world applications. We release the code of FunBaT at {https://github.com/xuangu-fang/Functional-Bayesian-Tucker-Decomposition}.'}",https://openreview.net{'value': '/pdf/28ad57ce7aedc545ef81092035c69c2b59d2baba.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ZULjcYLWKe,{'value': 'DMBP: Diffusion model-based predictor for robust offline reinforcement learning against state observation perturbations'},Zhihe YANG; Yunjian Xu,~Zhihe_YANG1; ~Yunjian_Xu1,"{'value': ['Robust Reinforcement Learning', 'Offline Reinforcement Learning', 'Diffusion Models']}","{'value': 'Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of ran- dom noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP.'}",https://openreview.net{'value': '/pdf/c9ff6dbf668267f62054398a9367479a749d5fbf.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ZFMiHfZwIf,{'value': 'Skill or Luck? Return Decomposition via Advantage Functions'},Hsiao-Ru Pan; Bernhard Schölkopf,~Hsiao-Ru_Pan1; ~Bernhard_Schölkopf1,"{'value': ['reinforcement learning', 'off-policy', 'advantage function']}","{'value': 'Learning from off-policy data is essential for sample-efficient reinforcement learning. In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent’s actions (skill) and parts outside of the agent’s control (luck). Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The resulting method can learn\nfrom off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions. We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important. Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to suboptimal policy optimization performance.'}",https://openreview.net{'value': '/pdf/669ebbb6ad5925d0675f9ecb3a2fefc005bf85ae.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Z59Rb5bPPP,{'value': 'Trajeglish: Traffic Modeling as Next-Token Prediction'},Jonah Philion; Xue Bin Peng; Sanja Fidler,~Jonah_Philion1; ~Xue_Bin_Peng1; ~Sanja_Fidler1,"{'value': ['self-driving', 'traffic modeling', 'autonomous vehicles', 'simulation', 'motion prediction', 'transformer']}","{'value': 'A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.'}",https://openreview.net{'value': '/pdf/6a1754e5443464338e361380403b4fec2f5d94b2.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=YcW8i9VCf5,{'value': 'Adversarial Causal Bayesian Optimization'},Scott Sussex; Pier Giuseppe Sessa; Anastasia Makarova; Andreas Krause,~Scott_Sussex1; ~Pier_Giuseppe_Sessa1; ~Anastasia_Makarova1; ~Andreas_Krause1,"{'value': ['causality', 'bayesian optimization']}","{'value': ""In Causal Bayesian Optimization (CBO), an agent intervenes on a structural causal model with known graph but unknown mechanisms to maximize a downstream reward variable. In this paper, we consider the generalization where other agents or external events also intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.""}",https://openreview.net{'value': '/pdf/70ae95d2b3d8a042800cb337082f488983922e18.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=YZrg56G0JV,{'value': 'Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks'},Ziping Xu; Zifan Xu; Runxuan Jiang; Peter Stone; Ambuj Tewari,~Ziping_Xu1; ~Zifan_Xu1; ~Runxuan_Jiang1; ~Peter_Stone1; ~Ambuj_Tewari1,{'value': ['Reinforcement Learning; Multitask Learning; Exploration']},"{'value': 'Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic  policy-sharing algorithm with myopic exploration design like $\\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the ""exploration benefits"" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control environments, where the diverse task set aligns with the task selection by automatic curriculum learning, which is empirically shown to improve sample-efficiency.'}",https://openreview.net{'value': '/pdf/5c003b03ad1f26d11e28a80a2ac2718e4b0660a5.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=YIls9HEa52,{'value': 'Parsing neural dynamics with infinite recurrent switching linear dynamical systems'},Victor Geadah; International Brain Laboratory; Jonathan W. Pillow,~Victor_Geadah1; ~International_Brain_Laboratory1; ~Jonathan_W._Pillow1,"{'value': ['Markov switching processes', 'Neural data analysis', 'State-space models']}","{'value': 'Unsupervised methods for dimensionality reduction of neural activity and behavior have provided unprecedented insights into the underpinnings of neural information processing. One popular approach involves the recurrent switching linear dynamical system (rSLDS) model, which describes the latent dynamics of neural spike train data using discrete switches between a finite number of low-dimensional linear dynamical systems. However, a few properties of rSLDS model limit its deployability on trial-varying data, such as a fixed number of states over trials, and no latent structure or organization of states. Here we overcome these limitations by endowing the rSLDS model with a semi-Markov discrete state process, with latent geometry, that captures key properties of stochastic processes over partitions with flexible state cardinality. We leverage partial differential equations (PDE) theory to derive an efficient, semi-parametric formulation for dynamical sufficient statistics to the discrete states. This process, combined with switching dynamics, defines our infinite recurrent switching linear dynamical system (irSLDS) model class. We first validate and demonstrate the capabilities of our model on synthetic data. Next, we turn to the analysis of mice electrophysiological data during decision-making, and uncover strong non-stationary processes underlying both within-trial and trial-averaged neural activity.'}",https://openreview.net{'value': '/pdf/69049199784ba1584b80a5be627c8e848d397879.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=YEhQs8POIo,{'value': 'Differentially Private Synthetic Data via Foundation Model APIs 1: Images'},Zinan Lin; Sivakanth Gopi; Janardhan Kulkarni; Harsha Nori; Sergey Yekhanin,~Zinan_Lin1; ~Sivakanth_Gopi1; ~Janardhan_Kulkarni2; ~Harsha_Nori1; ~Sergey_Yekhanin1,"{'value': ['synthetic data', 'differential privacy', 'model API', 'foundation models']}","{'value': 'Generating differentially private (DP) synthetic data that closely resembles the original private data is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are only accessible via their inference APIs. However, this comes with greater challenges due to strictly more restrictive model access and the need to protect privacy from the API provider. \n\nIn this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its initial promise on synthetic images. Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methods without any model training. For example, on CIFAR10 (with ImageNet as the public data), we achieve FID ≤ 7.9 with privacy cost ε = 0.67, significantly improving the previous SOTA from ε = 32. We further demonstrate the promise of applying PE on large foundation models such as Stable Diffusion to tackle challenging private datasets with a small number of high-resolution images. The code and data are released at https://github.com/microsoft/DPSDA.'}",https://openreview.net{'value': '/pdf/234b2f2be9197c43fbf0e155d4e1d8eaa11cfb64.pdf'},{'title_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=WNQjN5HzXt,{'value': 'AUGCAL: Improving Sim2Real Adaptation by Uncertainty Calibration on Augmented Synthetic Images'},Prithvijit Chattopadhyay; Bharat Goyal; Boglarka Ecsedi; Viraj Uday Prabhu; Judy Hoffman,~Prithvijit_Chattopadhyay1; ~Bharat_Goyal1; ~Boglarka_Ecsedi1; ~Viraj_Uday_Prabhu1; ~Judy_Hoffman1,"{'value': ['Unsupervised Domain Adaptation', 'Sim2Real']}","{'value': 'Synthetic data (Sim) drawn from simulators have emerged as a popular alternativefor training models where acquiring annotated real-world images is difficult. However, transferring models trained on synthetic images to real-world applicationscan be challenging due to appearance disparities. A commonly employed solution to counter this Sim2Real gap is unsupervised domain adaptation, where models are trained using labeled Sim data and unlabeled Real data. Mispredictions made by such Sim2Real adapted models are often associated with miscalibration – stemming from overconfident predictions on real data. In this paper, we introduce AUGCAL, a simple training-time patch for unsupervised adaptation that improves Sim2Real adapted models by – (1) reducing overall miscalibration, (2) reducing overconfidence in incorrect predictions and (3) improving confidence score reliability by better guiding misclassification detection – all while retaining or improving Sim2Real performance. Given a base Sim2Real adaptation algorithm, at training time, AUGCAL involves replacing vanilla Sim images with strongly augmented views (AUG intervention) and additionally optimizing for a training time calibration loss on augmented Sim predictions (CAL intervention). We motivate AUGCAL using a brief analytical justification of how to reduce miscalibration on unlabeled REAL data. Through our experiments, we empirically show the efficacy of AUGCAL across multiple adaptation methods, backbones, tasks and shifts.'}",https://openreview.net{'value': '/pdf/4f20af9c38d0b7f42f702485207a3f1ec51ee183.pdf'},{'title_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=WIzzXCVYiH,{'value': 'GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries'},Xiaoqi Wang; Han Wei Shen,~Xiaoqi_Wang2; ~Han_Wei_Shen1,"{'value': ['AI Interpretability', 'Graph Neural Networks', 'Model-Level Explanation of Neural Networks', 'Decision Boundaries']}","{'value': 'While Graph Neural Networks (GNNs) have achieved remarkable performance on various machine learning tasks on graph data, they also raised questions regarding their transparency and interpretability. Recently, there have been extensive research efforts to explain the decision-making process of GNNs. These efforts often focus on explaining why a certain prediction is made for a particular instance, or what discriminative features the GNNs try to detect for each class. However, to the best of our knowledge, there is no existing study on understanding the decision boundaries of GNNs, even though the decision-making process of GNNs is directly determined by the decision boundaries. To bridge this research gap, we propose a model-level explainability method called GNNBoundary, which attempts to gain deeper insights into the decision boundaries of graph classifiers. Specifically, we first develop an algorithm to identify the pairs of classes whose decision regions are adjacent. For an adjacent class pair, the near-boundary graphs between them are effectively generated by optimizing a novel objective function specifically designed for boundary graph generation. Thus, by analyzing the nearboundary graphs, the important characteristics of decision boundaries can be uncovered. To evaluate the efficacy of GNNBoundary, we conduct experiments on both synthetic and public real-world datasets. The results demonstrate that, via the analysis of faithful near-boundary graphs generated by GNNBoundary, we can thoroughly assess the robustness and generalizability of the explained GNNs. The official implementation can be found at https://github.com/yolandalalala/GNNBoundary.'}",https://openreview.net{'value': '/pdf/4c88af58e7fea802a541ca1a850768e448700110.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=W3VsHuga3j,{'value': 'Modeling Boundedly Rational Agents with Latent Inference Budgets'},Athul Paul Jacob; Abhishek Gupta; Jacob Andreas,~Athul_Paul_Jacob1; ~Abhishek_Gupta1; ~Jacob_Andreas1,"{'value': ['neurosymbolic', 'planning', 'rationality']}","{'value': 'We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating constrained inference. In this work, we introduce a latent inference budget model (L-IBM) that models these constraints explicitly, via a latent variable (inferred jointly with a model of agents’ goals) that controls the runtime of an iterative inference algorithm. L-IBMs make it possible to learn agent models using data from diverse populations of suboptimal actors. In three modeling tasks—inferring navigation goals from routes, inferring communicative intents from human utterances, and predicting next moves in human chess games—we show that L-IBMs match or outperforms Boltzmann models of decision-making under uncertainty. Moreover, the inferred inference budgets are themselves meaningful, efficient to compute, and correlated with measures of player skill, partner skill and task difficulty.'}",https://openreview.net{'value': '/pdf/b6fcabc7a4521fa9b8ff72f6d09fbafc1e118951.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=W2tCmRrj7H,{'value': 'A Flexible Generative Model for Heterogeneous Tabular EHR with Missing Modality'},Huan He; William hao; Yuanzhe Xi; Yong Chen; Bradley Malin; Joyce Ho,~Huan_He2; william.hao@emory.edu; ~Yuanzhe_Xi1; ~Yong_Chen10; ~Bradley_Malin1; ~Joyce_Ho1,"{'value': ['Generative Model', 'Synthetic EHR']}","{'value': 'Realistic synthetic electronic health records (EHRs) can be leveraged to acceler- ate methodological developments for research purposes while mitigating privacy concerns associated with data sharing. However, the training of Generative Ad- versarial Networks remains challenging, often resulting in issues like mode col- lapse. While diffusion models have demonstrated progress in generating qual- ity synthetic samples for tabular EHRs given ample denoising steps, their perfor- mance wanes when confronted with missing modalities in heterogeneous tabular EHRs data. For example, some EHRs contain solely static measurements, and some contain only contain temporal measurements, or a blend of both data types. To bridge this gap, we introduce FLEXGEN-EHR– a versatile diffusion model tai- lored for heterogeneous tabular EHRs, equipped with the capability of handling missing modalities in an integrative learning framework. We define an optimal transport module to align and accentuate the common feature space of hetero- geneity of EHRs. We empirically show that our model consistently outperforms existing state-of-the-art synthetic EHR generation methods both in fidelity by up to 3.10% and utility by up to 7.16%. Additionally, we show that our method can be successfully used in privacy-sensitive settings, where the original patient-level data cannot be shared.'}",https://openreview.net{'value': '/pdf/4a599e95342021d11914b38bfcbd9f85362f685b.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=VtmBAGCN7o,{'value': 'MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework'},Sirui Hong; Mingchen Zhuge; Jonathan Chen; Xiawu Zheng; Yuheng Cheng; Jinlin Wang; Ceyao Zhang; Zili Wang; Steven Ka Shing Yau; Zijuan Lin; Liyang Zhou; Chenyu Ran; Lingfeng Xiao; Chenglin Wu; Jürgen Schmidhuber,~Sirui_Hong1; ~Mingchen_Zhuge2; ~Jonathan_Chen3; ~Xiawu_Zheng1; ~Yuheng_Cheng1; ~Jinlin_Wang1; ~Ceyao_Zhang1; ~Zili_Wang1; ~Steven_Ka_Shing_Yau1; ~Zijuan_Lin1; ~Liyang_Zhou2; ~Chenyu_Ran1; ~Lingfeng_Xiao1; ~Chenglin_Wu2; ~Jürgen_Schmidhuber1,"{'value': ['Autonomous Agent', 'Meta Programming', 'Multi-Agent Society', 'Group Intelligence']}","{'value': 'Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.'}",https://openreview.net{'value': '/pdf/474fc6dad3bd9bf7fdb97c7cd72b2cc0649a9647.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=VrHiF2hsrm,{'value': 'Understanding Catastrophic Forgetting in Language Models via Implicit Inference'},Suhas Kotha; Jacob Mitchell Springer; Aditi Raghunathan,~Suhas_Kotha1; ~Jacob_Mitchell_Springer1; ~Aditi_Raghunathan1,"{'value': ['implicit inference in language models', 'fine-tuning', 'catastrophic forgetting']}","{'value': 'We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.'}",https://openreview.net{'value': '/pdf/fba453c1032d524bc23b1106ab3a598979c31704.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=VXDPXuq4oG,{'value': 'Order-Preserving GFlowNets'},Yihang Chen; Lukas Mauch,~Yihang_Chen1; ~Lukas_Mauch1,"{'value': ['probabilistic sampling', 'multi-objective optimization', 'GFlowNet']}","{'value': ""Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. However, GFlowNets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly accessible, in the case of multi-objective optimization (MOO) tasks for example. Moreover, to prioritize identifying high-reward candidates, the conventional practice is to raise the reward to a higher exponent, the optimal choice of which may vary across different environments. To address these issues, we propose Order-Preserving GFlowNets (OP-GFNs), which sample with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates, thus eliminating the need for an explicit formulation of the reward function. We theoretically prove that the training process of OP-GFNs gradually sparsifies the learned reward landscape in single-objective maximization tasks. The sparsification concentrates on candidates of a higher hierarchy in the ordering, ensuring exploration at the beginning and exploitation towards the end of the training. We demonstrate OP-GFN's state-of-the-art performance in single-objective maximization (totally ordered) and multi-objective Pareto front approximation (partially ordered) tasks, including synthetic datasets, molecule generation, and neural architecture search.""}",https://openreview.net{'value': '/pdf/7db994e2b6453eeefbbd759a3887758bb0241f5f.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=V1GM9xDvIY,{'value': 'Neural structure learning with stochastic differential equations'},Benjie Wang; Joel Jennings; Wenbo Gong,~Benjie_Wang1; ~Joel_Jennings1; ~Wenbo_Gong1,"{'value': ['Structure Learning', 'Causal Discovery', 'Generative Model', 'Variational Inference', 'Differential Equation']}","{'value': 'Discovering the underlying relationships among variables from temporal observations has been a longstanding challenge in numerous scientific disciplines, including biology, finance, and climate science. The dynamics of such systems are often best described using continuous-time stochastic processes. Unfortunately, most existing structure learning approaches assume that the underlying process evolves in discrete-time and/or observations occur at regular time intervals. These mismatched assumptions can often lead to incorrect learned structures and models. In this work, we introduce a novel structure learning method, SCOTCH, which combines neural stochastic differential equations (SDE) with variational inference to infer a posterior distribution over possible structures. This continuous-time approach can naturally handle both learning from and predicting observations at arbitrary time points. Theoretically, we establish sufficient conditions for an SDE and SCOTCH to be structurally identifiable, and prove its consistency under infinite data limits. Empirically, we demonstrate that our approach leads to improved structure learning performance on both synthetic and real-world datasets compared to relevant baselines under regular and irregular sampling intervals.'}",https://openreview.net{'value': '/pdf/5ff1f06834df4a615749f6e89f07285cdc60b2ad.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=UyGWafcopT,{'value': 'Meaning Representations from Trajectories in Autoregressive Models'},Tian Yu Liu; Matthew Trager; Alessandro Achille; Pramuditha Perera; Luca Zancato; Stefano Soatto,~Tian_Yu_Liu2; ~Matthew_Trager2; ~Alessandro_Achille1; ~Pramuditha_Perera3; ~Luca_Zancato1; ~Stefano_Soatto3,"{'value': ['definitions of meaning', 'semantic similarity', 'sentence embeddings']}","{'value': 'We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover, unlike vector-based representations,  distribution-based representations can also model asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym relations) by using algebraic operations between likelihood functions. These ideas are grounded in distributional perspectives on semantics and are connected to standard constructions in automata theory, but to our knowledge they have not been applied to modern language models. We empirically show that the representations obtained from large models align well with human annotations, outperform other zero-shot and prompt-free methods on semantic similarity tasks, and can be used to solve more complex entailment and containment tasks that standard embeddings cannot handle. Finally, we extend our method  to represent data from different modalities (e.g., image and text) using multimodal autoregressive models. Our code is available at: https://github.com/tianyu139/meaning-as-trajectories'}",https://openreview.net{'value': '/pdf/7b0c96b0676b9a4c87008b8719d7e36c9049f57e.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=UfBIxpTK10,{'value': 'Deep Confident Steps to New Pockets: Strategies for Docking Generalization'},Gabriele Corso; Arthur Deng; Nicholas Polizzi; Regina Barzilay; Tommi S. Jaakkola,~Gabriele_Corso1; ~Arthur_Deng1; ~Nicholas_Polizzi1; ~Regina_Barzilay1; ~Tommi_S._Jaakkola1,"{'value': ['generalization', 'molecular docking', 'protein-ligand binding', 'diffusion models', 'benchmark', 'bootstrapping', 'self-training']}","{'value': 'Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, docking methods must generalize well across the proteome. Existing benchmarks, however, fail to rigorously assess generalizability. Therefore, we develop DockGen, a new benchmark based on the ligand-binding domains of proteins, and we show that existing machine learning-based docking models have very weak generalization abilities. We carefully analyze the scaling laws of ML-based docking and show that, by scaling data and model size, as well as integrating synthetic data strategies, we are able to significantly increase the generalization capacity and set new state-of-the-art performance across benchmarks.  Further, we propose Confidence Bootstrapping, a new training paradigm that solely relies on the interaction between diffusion and confidence models and exploits the multi-resolution generation process of diffusion models. We demonstrate that Confidence Bootstrapping significantly improves the ability of ML-based docking methods to dock to unseen protein classes, edging closer to accurate and generalizable blind docking methods.'}",https://openreview.net{'value': '/pdf/6482d88857d6eb089f7b4a5808e73ee7180fa42e.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=UBVNwD3hPN,{'value': 'CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents'},Siyuan Qi; Shuo Chen; Yexin Li; Xiangyu Kong; Junqi Wang; Bangcheng Yang; Pring Wong; Yifan Zhong; Xiaoyuan Zhang; Zhaowei Zhang; Nian Liu; Yaodong Yang; Song-Chun Zhu,~Siyuan_Qi1; ~Shuo_Chen3; ~Yexin_Li1; ~Xiangyu_Kong1; ~Junqi_Wang1; ~Bangcheng_Yang1; ~Pring_Wong2; ~Yifan_Zhong2; ~Xiaoyuan_Zhang3; ~Zhaowei_Zhang2; ~Nian_Liu4; ~Yaodong_Yang1; ~Song-Chun_Zhu1,"{'value': ['Interactive Environments', 'Benchmark', 'Reinforcement Learning', 'Language Agent', 'Multi-agent']}","{'value': 'The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization’s profound alignment with human society requires sophisticated learning and prior knowledge, while its ever-changing space and action space demand robust reasoning for generalization. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. Overall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.'}",https://openreview.net{'value': '/pdf/8e9b4884f752883a17e58942608d9cc769610a01.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=TzoHLiGVMo,{'value': 'ODEFormer: Symbolic Regression of Dynamical Systems with Transformers'},Stéphane d'Ascoli; Sören Becker; Philippe Schwaller; Alexander Mathis; Niki Kilbertus,~Stéphane_d'Ascoli1; ~Sören_Becker2; ~Philippe_Schwaller1; ~Alexander_Mathis1; ~Niki_Kilbertus1,"{'value': ['symbolic regression', 'dynamical systems', 'differential equations', 'transformer']}","{'value': 'We introduce ODEFormer, the first transformer able to infer multidimensional ordinary differential equation (ODE) systems in symbolic form from the observation of a single solution trajectory. We perform extensive evaluations on two datasets: (i) the existing ‘Strogatz’ dataset featuring two-dimensional systems; (ii) ODEBench, a collection of one- to four-dimensional systems that we carefully curated from the literature to provide a more holistic benchmark. ODEFormer consistently outperforms existing methods while displaying substantially improved robustness to noisy and irregularly sampled observations, as well as faster inference. We release our code, model and benchmark at https://github.com/sdascoli/odeformer.'}",https://openreview.net{'value': '/pdf/f91ca392113614803487b965a914d26be9910ccf.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Ts95eXsPBc,{'value': 'Spatially-Aware Transformers for Embodied Agents'},Junmo Cho; Jaesik Yoon; Sungjin Ahn,~Junmo_Cho1; ~Jaesik_Yoon1; ~Sungjin_Ahn1,"{'value': ['Episodic Memory', 'Spatial Inference', 'Prediction', 'Generation', 'Reinforcement Learning']}","{'value': 'Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning. The source code for our models and experiments will be available at \\href{https://github.com/spatially_aware_transformer}{https://github.com/spatially_aware_transformer}.'}",https://openreview.net{'value': '/pdf/1dc0ff45874e872749597c38bcfc2df0fa7ed0d6.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=TjhUtloBZU,{'value': 'Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks'},Hao Chen; Jindong Wang; Ankit Shah; Ran Tao; Hongxin Wei; Xing Xie; Masashi Sugiyama; Bhiksha Raj,~Hao_Chen15; ~Jindong_Wang1; ~Ankit_Shah1; ~Ran_Tao2; ~Hongxin_Wei1; ~Xing_Xie3; ~Masashi_Sugiyama1; ~Bhiksha_Raj1,"{'value': ['Pre training', 'Noisy model learning', 'Label noise', 'Noise mitigation']}","{'value': 'Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a light-weight black-box tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering one may not be able to fully fine-tune or even access the pre-trained models. We conduct practical experiments on popular vision and language models that are pre-trained on noisy data for evaluation of our approach. Our analysis and results show the importance of this interesting and novel research direction, which we term Noisy Model Learning.'}",https://openreview.net{'value': '/pdf/ae1d6d7106b28a7cb9cacfd2f2b5dd34c10f0ce0.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=TjGJFkU3xL,{'value': 'Doubly Robust Proximal Causal Learning for Continuous Treatments'},Yong Wu; Yanwei Fu; Shouyan Wang; Xinwei Sun,~Yong_Wu9; ~Yanwei_Fu2; ~Shouyan_Wang1; ~Xinwei_Sun1,"{'value': ['causal effect', 'continuous treatment', 'kernel function', 'doubly robust']}","{'value': 'Proximal causal learning is a powerful framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatments can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments for proximal causal learning. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We then provide a comprehensive convergence analysis in terms of the mean square error. We demonstrate the utility of our estimator on synthetic datasets and real-world applications.'}",https://openreview.net{'value': '/pdf/eb1c69e9a863e64be033eb56122a7002e9564167.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=SQpnEfv9WH,{'value': 'Social-Transmotion: Promptable Human Trajectory Prediction'},Saeed Saadatnejad; Yang Gao; Kaouther Messaoud; Alexandre Alahi,~Saeed_Saadatnejad1; ~Yang_Gao15; ~Kaouther_Messaoud1; ~Alexandre_Alahi3,"{'value': ['human trajectory prediction', 'robot navigation', 'autonomous driving', 'attention mechanism']}","{'value': 'Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space.\nTo address this, we introduce *Social-Transmotion*, a generic Transformer-based model that exploits diverse and numerous visual cues to predict human behavior. We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes in the image plane, or body pose keypoints in either 2D or 3D.  This, in turn, augments trajectory data, leading to enhanced human trajectory prediction.\nUsing masking technique, our model exhibits flexibility and adaptability by capturing spatiotemporal interactions between agents based on the available visual cues.\nWe delve into the merits of using 2D versus 3D poses, and a limited set of poses. Additionally, we investigate the spatial and temporal attention map to identify which keypoints and time-steps in the sequence are vital for optimizing human trajectory prediction.\nOur approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY.\nThe code is publicly available: [https://github.com/vita-epfl/social-transmotion](https://github.com/vita-epfl/social-transmotion).'}",https://openreview.net{'value': '/pdf/5dcf47a66eb9a88f6cda4a59dfb2dd17df12bc68.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=SEiuSzlD1d,{'value': 'Mask-Based Modeling for Neural Radiance Fields'},Ganlin Yang; Guoqiang Wei; Zhizheng Zhang; Yan Lu; Dong Liu,~Ganlin_Yang1; ~Guoqiang_Wei1; ~Zhizheng_Zhang1; ~Yan_Lu7; ~Dong_Liu6,"{'value': ['NeRF', 'Pretraining', 'Mask-Based Modeling']}","{'value': 'Most Neural Radiance Fields (NeRFs) exhibit limited generalization capabilities,which restrict their applicability in representing multiple scenes using a single model. To address this problem, existing generalizable NeRF methods simply condition the model on image features. These methods still struggle to learn precise global representations over diverse scenes since they lack an effective mechanism for interacting among different points and views. In this work, we unveil that 3D implicit representation learning can be significantly improved by mask-based modeling. Specifically, we propose **m**asked **r**ay and **v**iew **m**odeling for generalizable **NeRF** (**MRVM-NeRF**), which is a self-supervised pretraining target to predict complete scene representations from partially masked features along each ray. With this pretraining target, MRVM-NeRF enables better use of correlations across different rays and views as the geometry priors, which thereby strengthens the capability of capturing intricate details within the scenes and boosts the generalization capability across different scenes. Extensive experiments demonstrate the effectiveness of our proposed MRVM-NeRF on both synthetic and real-world datasets, qualitatively and quantitatively. Besides, we also conduct experiments to show the compatibility of our proposed method with various backbones and its superiority under few-shot cases.'}",https://openreview.net{'value': '/pdf/60aa67a90ce56bdf66d121eaf8fa35163fc77a76.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=SBoRhRCzM3,{'value': 'THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS'},Junchi Yu; Ran He; Zhitao Ying,~Junchi_Yu1; ~Ran_He1; ~Zhitao_Ying1,"{'value': ['LLM Complex Reasoning', 'Language Model Reasoning']}","{'value': 'Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. \nHowever, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}.\nTo address these issues, we propose \\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs.\nThese analogous problems are related to the input one, with reusable solutions and problem-solving strategies.\nThus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. \nTo achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. \nThen, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch.\nTP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. \nExperiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\\% improvement of human preference in Creative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent Planning.'}",https://openreview.net{'value': '/pdf/1cddcaa3f523f7ad1c9aea87d5180f2d173e4bb8.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=SBj2Qdhgew,{'value': 'Demystifying Local & Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition'},Faisal Hamman; Sanghamitra Dutta,~Faisal_Hamman1; ~Sanghamitra_Dutta2,"{'value': ['Fairness', 'Federated Learning', 'Machine Learning', 'Information Theory']}","{'value': 'This work presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either $\\textit{global fairness}$ (overall disparity of the model across all clients) or $\\textit{local fairness}$ (disparity of the model at each client), without always considering their trade-offs. There is a lack of understanding regarding the interplay between global and local fairness in FL, particularly under data heterogeneity, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID), which first identifies three sources of unfairness in FL, namely, $\\textit{Unique Disparity}$, $\\textit{Redundant  Disparity}$, and $\\textit{Masked Disparity}$. We demonstrate how these three disparities contribute to global and local fairness using canonical examples. This decomposition helps us derive fundamental limits on the trade-off between global and local fairness, highlighting where they agree or disagree.  We introduce the $\\textit{Accuracy and Global-Local Fairness Optimality Problem}$ (AGLFOP), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution. We also present experimental results on synthetic datasets and the ADULT dataset to support our theoretical findings.'}",https://openreview.net{'value': '/pdf/9e1c4b40878b3906288af7002c4dbe907d55b29b.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=S46Knicu56,{'value': 'A Variational Framework for Estimating Continuous Treatment Effects with Measurement Error'},Erdun Gao; Howard Bondell; Wei Huang; Mingming Gong,~Erdun_Gao1; ~Howard_Bondell2; ~Wei_Huang8; ~Mingming_Gong1,"{'value': ['treatment effect estimation', 'continuous treatment', 'measurement error']}","{'value': 'Estimating treatment effects has numerous real-world applications in various fields, such as epidemiology and political science. While much attention has been devoted to addressing the challenge using fully observational data, there has been comparatively limited exploration of this issue in cases when the treatment is not directly observed. In this paper, we tackle this problem by developing a general variational framework, which is flexible to integrate with advanced neural network-based approaches, to identify the average dose-response function (ADRF) with the continuously valued error-contaminated treatment. Our approach begins with the formulation of a probabilistic data generation model, treating the unobserved treatment as a latent variable. In this model, we leverage a learnable density estimation neural network to derive its prior distribution conditioned on covariates. This module also doubles as a generalized propensity score estimator, effectively mitigating selection bias arising from observed confounding variables. Subsequently, we calculate the posterior distribution of the treatment, taking into account the observed measurement and outcome. To mitigate the impact of treatment error, we introduce a re-parametrized treatment value, replacing the error-affected one, to make more accurate predictions regarding the outcome. To demonstrate the adaptability of our framework, we incorporate two state-of-the-art ADRF estimation methods and rigorously assess its efficacy through extensive simulations and experiments using semi-synthetic data.'}",https://openreview.net{'value': '/pdf/4af3e778b6fe1434c57052978c47fd214baef37c.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=S2oTVrlcp3,{'value': 'SmartPlay : A Benchmark for LLMs as Intelligent Agents'},Yue Wu; Xuan Tang; Tom Mitchell; Yuanzhi Li,~Yue_Wu17; ~Xuan_Tang2; ~Tom_Mitchell2; ~Yuanzhi_Li1,{'value': ['Large Language Models']},"{'value': ""Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately.\nSmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. \nWe release our benchmark at https://github.com/microsoft/SmartPlay""}",https://openreview.net{'value': '/pdf/f84669704e333d9a0d57f5fd1dfd011b03109021.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=RzNlECeoOB,"{'value': ""$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence""}",Juno Kim; Jaehyuk Kwon; Mincheol Cho; Hyunjong Lee; Joong-Ho Won,~Juno_Kim1; ~Jaehyuk_Kwon1; ~Mincheol_Cho1; ~Hyunjong_Lee1; ~Joong-Ho_Won1,"{'value': ['Variational autoencoder', 'Information geometry', 'Heavy-tail learning', 'Generative model']}","{'value': ""The variational autoencoder (VAE) typically employs a standard normal prior as a regularizer for the probabilistic latent encoder. However, the Gaussian tail often decays too quickly to effectively accommodate the encoded points, failing to preserve crucial structures hidden in the data. In this paper, we explore the use of heavy-tailed models to combat over-regularization. Drawing upon insights from information geometry, we propose $t^3$VAE, a modified VAE framework that incorporates Student's t-distributions for the prior, encoder, and decoder. This results in a joint model distribution of a power form which we argue can better fit real-world datasets. We derive a new objective by reformulating the evidence lower bound as joint optimization of KL divergence between two statistical manifolds and replacing with $\\gamma$-power divergence, a natural alternative for power families. $t^3$VAE demonstrates superior generation of low-density regions when trained on heavy-tailed synthetic data. Furthermore, we show that $t^3$VAE significantly outperforms other models on CelebA and imbalanced CIFAR-100 datasets.""}",https://openreview.net{'value': '/pdf/456160753ce7ca91fd5d14c5ba07b1183322d395.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=RvfPnOkPV4,"{'value': ""What's In My Big Data?""}",Yanai Elazar; Akshita Bhagia; Ian Helgi Magnusson; Abhilasha Ravichander; Dustin Schwenk; Alane Suhr; Evan Pete Walsh; Dirk Groeneveld; Luca Soldaini; Sameer Singh; Hannaneh Hajishirzi; Noah A. Smith; Jesse Dodge,~Yanai_Elazar1; ~Akshita_Bhagia1; ~Ian_Helgi_Magnusson1; ~Abhilasha_Ravichander2; ~Dustin_Schwenk1; ~Alane_Suhr1; ~Evan_Pete_Walsh1; ~Dirk_Groeneveld1; ~Luca_Soldaini1; ~Sameer_Singh1; ~Hannaneh_Hajishirzi1; ~Noah_A._Smith2; ~Jesse_Dodge1,"{'value': ['nlp', 'dataset', 'analaysis', 'data-statistics', 'data-quality', 'PII']}","{'value': ""Large text corpora are the backbone of language models.\nHowever, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination).\nIn this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities---count and search---*at scale*, which allows us to analyze more than 35 terabytes on a standard compute node. \nWe apply WIMBD to ten different corpora used to train popular language models, including *C4*, *The Pile*, and *RedPajama*.\nOur analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. \nFor instance, we find that about 50% of the documents in *RedPajama* and *LAION-2B-en* are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE.\nWe open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.""}",https://openreview.net{'value': '/pdf/8e645356fd6998459b2c368f65c8a2b3a44206af.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Rry1SeSOQL,{'value': 'MT-Ranker: Reference-free machine translation evaluation by inter-system ranking'},Ibraheem Muhammad Moosa; Rui Zhang; Wenpeng Yin,~Ibraheem_Muhammad_Moosa1; ~Rui_Zhang7; ~Wenpeng_Yin1,{'value': ['Machine Translation Evaluation']},"{'value': 'Traditionally, Machine Translation (MT) Evaluation has been treated as a regression problem -- producing an absolute translation-quality score. This approach has two limitations: i) the scores lack interpretability, and human annotators struggle with giving consistent scores; ii) most scoring methods are based on (reference, translation) pairs, limiting their applicability in real-world scenarios where references are absent. In practice, we often care about whether a new MT system is better or worse than some competitors. In addition, reference-free MT evaluation is increasingly practical and necessary. Unfortunately, these two practical considerations have yet to be jointly explored. In this work, we formulate the reference-free MT evaluation into a pairwise ranking problem. Given the source sentence and a pair of translations, our system predicts which translation is better. In addition to proposing this new formulation, we further show that this new paradigm can demonstrate superior correlation with human judgments by merely using indirect supervision from natural language inference and weak supervision from our synthetic data. In the context of reference-free evaluation, MT-Ranker, trained without any human annotations, achieves state-of-the-art results on the WMT Shared Metrics Task benchmarks DARR20, MQM20, and MQM21. On a more challenging benchmark, ACES, which contains fine-grained evaluation criteria such as addition, omission, and mistranslation errors, MT-Ranker marks state-of-the-art against reference-free as well as reference-based baselines.'}",https://openreview.net{'value': '/pdf/fa181eb3a2cd5c3485b73e3829ad16f3dffa5faa.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=RjYKTQ0L0W,{'value': 'Achieving Human Parity in Content-Grounded Datasets Generation'},Asaf Yehudai; Boaz Carmeli; Yosi Mass; Ofir Arviv; Nathaniel Mills; Eyal Shnarch; Leshem Choshen,~Asaf_Yehudai1; ~Boaz_Carmeli1; ~Yosi_Mass1; ~Ofir_Arviv1; ~Nathaniel_Mills1; ~Eyal_Shnarch1; ~Leshem_Choshen1,"{'value': ['Data Generation', 'content grounded long-form question-answering', 'summarization']}","{'value': 'The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data.\nIt consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained on human-generated data and consistently outperforming them in faithfulness. Finally, we applied our method to create LFQA data within the medical domain and compared a model trained on it with models trained on other domains.'}",https://openreview.net{'value': '/pdf/2fef137cbcc7f81f79f62fda8cc241f7b8b18580.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=RNGUbTYSjk,{'value': 'Weaker MVI Condition: Extragradient Methods with Multi-Step Exploration'},Yifeng Fan; Yongqiang Li; Bo Chen,~Yifeng_Fan3; ~Yongqiang_Li2; ~Bo_Chen13,"{'value': ['extragradient', 'minimax', 'nonconvex-nonconcave', 'variational inequilities', 'saddle point problem']}","{'value': 'This paper proposes a new framework of algorithms that is extended from the celebrated extragradient algorithm. The min-max problem has attracted increasing attention because of its applications in machine learning tasks such as generative adversarial networks (GANs) training. While there has been exhaustive research on convex-concave setting, problem of nonconvex-nonconcave setting faces many challenges, such as convergence to limit cycles. Given that general min-max optimization has been found to be intractable, recent research efforts have shifted towards tackling structured problems. One of these follows the weak Minty variational inequality (weak MVI), which is motivated by relaxing Minty variational inequality (mvi) without compromising convergence guarantee of extragradient algorithm. Existing extragradient-type algorithms involve one exploration step and one update step per iteration. We analyze the algorithms with multiple exploration steps and show that current assumption can be further relaxed when more exploration is introduced. Furthermore, we design an adaptive algorithm that explores until the optimal improvement is achieved. This process exploits information from the whole trajectory and effectively tackles cyclic behaviors.'}",https://openreview.net{'value': '/pdf/a5e7750cce50556fd7f15f3b6311bd2c33845c54.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Qwq4cpLtoX,{'value': 'Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability'},Ivan Lee; Nan Jiang; Taylor Berg-Kirkpatrick,~Ivan_Lee2; ~Nan_Jiang11; ~Taylor_Berg-Kirkpatrick1,"{'value': ['in-context learning', 'neural architectures']}","{'value': ""What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps toward answering this question. We evaluate thirteen model architectures capable of causal language modeling across a suite of synthetic in-context learning tasks. These selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, state space model inspired, and other emerging attention alternatives. We discover that all the considered architectures can perform in-context learning under a wider range of conditions than previously documented. Additionally, we observe stark differences in statistical efficiency and consistency by varying the number of in-context examples and task difficulty. We also measure each architecture's predisposition towards in-context learning when presented with the option to memorize rather than leverage in-context examples. Finally, and somewhat surprisingly, we find that several attention alternatives are sometimes competitive with or better in-context learners than transformers. However, no single architecture demonstrates consistency across all tasks, with performance either plateauing or declining when confronted with a significantly larger number of in-context examples than those encountered during gradient-based training.""}",https://openreview.net{'value': '/pdf/8b3a829acfd26790f582233e7748752863f7a744.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Qox9rO0kN0,{'value': 'Learning Multi-Agent Communication from Graph Modeling Perspective'},Shengchao Hu; Li Shen; Ya Zhang; Dacheng Tao,~Shengchao_Hu1; ~Li_Shen1; ~Ya_Zhang1; ~Dacheng_Tao1,"{'value': ['communication learning', 'multi-agent reinforcement learning']}","{'value': 'In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.'}",https://openreview.net{'value': '/pdf/b18ae005fc01451473ec4301d9802e5267831053.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=QQ6RgKYiQq,{'value': 'MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field'},Kaizhi Yang; Xiaoshuai Zhang; Zhiao Huang; Xuejin Chen; Zexiang Xu; Hao Su,~Kaizhi_Yang1; ~Xiaoshuai_Zhang1; ~Zhiao_Huang1; ~Xuejin_Chen1; ~Zexiang_Xu1; ~Hao_Su1,"{'value': ['NeRF', 'Dynamic', 'Motion', 'Part discovery']}","{'value': 'We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.'}",https://openreview.net{'value': '/pdf/5a09f0c81c6bf7abc3ea9cd5d6138c06f4812622.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Q3YaCghZNt,{'value': 'Lemur: Integrating Large Language Models in Automated Program Verification'},Haoze Wu; Clark Barrett; Nina Narodytska,~Haoze_Wu1; ~Clark_Barrett1; ~Nina_Narodytska1,"{'value': ['Large Language Models', 'Formal verification']}","{'value': 'The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.'}",https://openreview.net{'value': '/pdf/bac67630a66030e8126ecebec92444100f2cd9cf.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=PudduufFLa,{'value': 'Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks'},Marc Rußwurm; Konstantin Klemmer; Esther Rolf; Robin Zbinden; Devis Tuia,~Marc_Rußwurm1; ~Konstantin_Klemmer1; ~Esther_Rolf1; ~Robin_Zbinden1; ~Devis_Tuia1,"{'value': ['Location Encoding', 'Positional Encoding', 'Implicit Neural Representations', 'Species Distribution Modeling']}","{'value': 'Learning representations of geographical space is vital for any machine learning model that integrates geolocated data, spanning application domains such as remote sensing, ecology, or epidemiology. Recent work embeds coordinates using sine and cosine projections based on Double Fourier Sphere (DFS) features. These embeddings assume a rectangular data domain even on global data, which can lead to artifacts, especially at the poles. At the same time, little attention has been paid to the exact design of the neural network architectures with which these functional embeddings are combined. This work proposes a novel location encoder for globally distributed geographic data that combines spherical harmonic basis functions, natively defined on spherical surfaces, with sinusoidal representation networks (SirenNets) that can be interpreted as learned Double Fourier Sphere embedding. We systematically evaluate positional embeddings and neural network architectures across various benchmarks and synthetic evaluation datasets. In contrast to previous approaches that require the combination of both positional encoding and neural networks to learn meaningful representations, we show that both spherical harmonics and sinusoidal representation networks are competitive on their own but set state-of-the-art performances across tasks when combined. The model code and experiments are available at https://github.com/marccoru/locationencoder.'}",https://openreview.net{'value': '/pdf/11eead9eb25de1cd772e111da1b931604a8fe49a.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=PcxQgtHGj2,{'value': 'Pre-training with Synthetic Data Helps Offline Reinforcement Learning'},Zecheng Wang; Che Wang; Zixuan Dong; Keith W. Ross,~Zecheng_Wang1; ~Che_Wang1; ~Zixuan_Dong1; ~Keith_W._Ross1,"{'value': ['Deep Reinforcement Learning', 'Offline Reinforcement Learning', 'Pretraining']}","{'value': 'Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.'}",https://openreview.net{'value': '/pdf/318129413e1d04dac4e9ef949bff927064665bee.pdf'},{'title_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Pc8AU1aF5e,{'value': 'Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control'},Longtao Zheng; Rundong Wang; Xinrun Wang; Bo An,~Longtao_Zheng1; ~Rundong_Wang1; ~Xinrun_Wang1; ~Bo_An2,"{'value': ['AI Agents', 'Large Language Models', 'Prompting']}","{'value': 'Building agents with large language models (LLMs) for computer control is a burgeoning research area, where the agent receives computer states and performs actions to complete complex tasks. Previous computer agents have demonstrated the benefits of in-context learning (ICL); however, their performance is hindered by several issues. First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exemplars and overlook the similarity among tasks, resulting in poor generalization to novel tasks. To address these challenges, we introduce Synapse, a computer agent featuring three key components: i) state abstraction, which filters out task-irrelevant information from raw states, allowing more exemplars within the limited context, ii) trajectory-as-exemplar prompting, which prompts the LLM with complete trajectories of the abstracted states and actions to improve multi-step decision-making, and iii) exemplar memory, which stores the embeddings of exemplars and retrieves them via similarity search for generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse achieves a 99.2% average success rate (a 10% relative improvement) across 64 tasks using demonstrations from only 48 tasks. Notably, Synapse is the first ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a 56% relative improvement in average step success rate over the previous state-of-the-art prompting scheme in Mind2Web.'}",https://openreview.net{'value': '/pdf/90203c60566501d9a43d3c69c8f26d50c4e151bc.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=POFrdKvpea,{'value': 'ACRF: Compressing Explicit Neural Radiance Fields via Attribute Compression'},Guangchi Fang; Qingyong Hu; Longguang Wang; Yulan Guo,~Guangchi_Fang1; ~Qingyong_Hu1; ~Longguang_Wang1; ~Yulan_Guo3,{'value': ['NeRF; NeRF Compression; 3D Data Compression']},"{'value': 'In this work, we study the problem of explicit NeRF compression. Through analyzing recent explicit NeRF models, we reformulate the task of explicit NeRF compression as 3D data compression. We further introduce our NeRF compression framework, Attributed Compression of Radiance Field (ACRF), which focuses on the compression of the explicit neural 3D representation. The neural 3D structure is pruned and converted to points with features, which are further encoded using importance-guided feature encoding. Furthermore, we employ an importance-prioritized entropy model to estimate the probability distribution of transform coefficients, which are then entropy coded with an arithmetic coder using the predicted distribution. Within this framework, we present two models, ACRF and ACRF-F, to strike a balance between compression performance and encoding time budget. Our experiments, which include both synthetic and real-world datasets such as Synthetic-NeRF and Tanks&Temples, demonstrate the superior performance of our proposed algorithm.'}",https://openreview.net{'value': '/pdf/45a84e3c7941fa2cce2c41900b464e316f204092.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=PLoWVP7Mjc,{'value': 'Embarrassingly Simple Dataset Distillation'},Yunzhen Feng; Shanmukha Ramakrishna Vedantam; Julia Kempe,~Yunzhen_Feng1; ~Shanmukha_Ramakrishna_Vedantam1; ~Julia_Kempe1,"{'value': ['Dataset Distillation', 'Data Condensation']}","{'value': 'Dataset distillation extracts a small set of synthetic training samples from a large dataset with the goal of achieving competitive performance on test data when trained on this sample. In this work, we tackle dataset distillation at its core by treating it directly as a bilevel optimization problem. Re-examining the foundational back-propagation through time method, we study the pronounced variance in the gradients, computational burden, and long-term dependencies. We introduce an improved method: Random Truncated Backpropagation Through Time (RaT-BPTT) to address them. RaT-BPTT incorporates a truncation coupled with a random window, effectively stabilizing the gradients and speeding up the optimization while covering long dependencies. This allows us to establish new state-of-the-art for a variety of standard dataset benchmarks. A deeper dive into the nature of distilled data unveils pronounced intercorrelation. In particular, subsets of distilled datasets tend to exhibit much worse performance than directly distilled smaller datasets of the same size. Leveraging RaT-BPTT, we devise a boosting mechanism that generates distilled datasets that contain subsets with near optimal performance across different data budgets.'}",https://openreview.net{'value': '/pdf/b0969030abee0e93619a86992e7382060e91e81e.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Oxh5CstDJU,"{'value': 'TD-MPC2: Scalable, Robust World Models for Continuous Control'}",Nicklas Hansen; Hao Su; Xiaolong Wang,~Nicklas_Hansen1; ~Hao_Su1; ~Xiaolong_Wang3,"{'value': ['reinforcement learning', 'model-based reinforcement learning', 'world models']}","{'value': 'TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents.\n\nExplore videos, models, data, code, and more at https://tdmpc2.com'}",https://openreview.net{'value': '/pdf/d7962b8e764953c9c4db96aa85307e9e9b689881.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Od39h4XQ3Y,{'value': 'Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models'},Yili Wang; Kaixiong Zhou; Ninghao Liu; Ying Wang; Xin Wang,~Yili_Wang2; ~Kaixiong_Zhou1; ~Ninghao_Liu2; ~Ying_Wang13; ~Xin_Wang54,"{'value': ['Sharpness-Aware Minimization', 'Molecular Graph']}","{'value': 'Sharpness-aware minimization (SAM) has received increasing attention in computer vision since it can effectively eliminate the sharp local minima from the training trajectory and mitigate generalization degradation. However, SAM requires two sequential gradient computations during the optimization of each step: one to obtain the perturbation gradient and the other to obtain the updating gradient. \nCompared with the base optimizer (e.g., Adam), SAM doubles the time overhead due to the additional perturbation gradient. By dissecting the theory of SAM and observing the training gradient of the molecular graph transformer, we propose a new algorithm named GraphSAM,  which reduces the training cost of SAM and improves the generalization performance of graph transformer models. \nThere are two key factors that contribute to this result: (i) \\textit{gradient approximation}: we use the updating gradient of the previous step to approximate the perturbation gradient at the intermediate steps smoothly (\\textbf{increases efficiency}); (ii) \\textit{loss landscape approximation}: we theoretically prove that the loss landscape of GraphSAM is limited to a small range centered on the expected loss of SAM (\\textbf{guarantees generalization performance}). The extensive experiments on six datasets with different tasks demonstrate the superiority of GraphSAM, especially in optimizing the model update process.'}",https://openreview.net{'value': '/pdf/18456a69ecee34f68549d5dc59cbfb15f3b0ebd1.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=OOxotBmGol,{'value': 'Large Language Models to Enhance Bayesian Optimization'},Tennison Liu; Nicolás Astorga; Nabeel Seedat; Mihaela van der Schaar,~Tennison_Liu1; ~Nicolás_Astorga1; ~Nabeel_Seedat1; ~Mihaela_van_der_Schaar2,"{'value': ['bayesian optimization', 'LLMs']}","{'value': ""Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance remains a delicate process. In this light, we present \\texttt{LLAMBO}, a novel approach that integrates the capabilities of Large Language Models (LLM) within BO. At a high level, we frame the BO problem in natural language, enabling LLMs to iteratively \\emph{propose} and \\emph{evaluate} promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can improve model-based BO. Our findings illustrate that \\texttt{LLAMBO} is effective at zero-shot warmstarting, and enhances surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require LLM finetuning. Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method. We empirically validate \\texttt{LLAMBO}'s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks.""}",https://openreview.net{'value': '/pdf/b22b6ea9b84dd474cf4871acc78a784c45bac294.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=OIsahq1UYC,{'value': 'Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization'},Dinghuai Zhang; Ricky T. Q. Chen; Cheng-Hao Liu; Aaron Courville; Yoshua Bengio,~Dinghuai_Zhang1; ~Ricky_T._Q._Chen1; ~Cheng-Hao_Liu1; ~Aaron_Courville3; ~Yoshua_Bengio1,"{'value': ['probabilistic inference', 'sampling', 'stochastic optimal control', 'gflownets']}","{'value': ""We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. \nWe extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities.  \nThe main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time.\nIn this work, we present Diffusion Generative Flow Samplers (DGFS), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional ``flow function''.\nOur method takes inspiration from the theory developed for generative flow networks (GFlowNets), allowing us to make use of intermediate learning signals.\nThrough various challenging experiments, we demonstrate that DGFS achieves more accurate estimates of the normalization constant than closely-related prior methods.""}",https://openreview.net{'value': '/pdf/de8c85381bfe2bb45303e6b306010705d7a2f519.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=NltzxpG0nz,{'value': 'Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds'},Sipeng Zheng; jiazheng liu; Yicheng Feng; Zongqing Lu,~Sipeng_Zheng1; ~jiazheng_liu2; ~Yicheng_Feng1; ~Zongqing_Lu2,"{'value': ['large multimodal pre-training', 'open-world embodied agent', 'large language model']}","{'value': ""Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks and carry out experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. The project’s website and code can be found at https://sites.google.com/view/steve-eye.""}",https://openreview.net{'value': '/pdf/a46ce05ff44a87fada32b918f094df26df3d2f07.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=NLevOah0CJ,{'value': 'Hindsight PRIORs for Reward Learning from Human Preferences'},Mudit Verma; Katherine Metcalf,~Mudit_Verma2; ~Katherine_Metcalf1,"{'value': ['preference based reinforcement learning', 'world models', 'return redistribution']}","{'value': ""Preference based Reinforcement Learning (PbRL) removes the need to hand specify a reward function by learning one from preference feedback over policy behaviors. Current approaches to PbRL do not address the credit assignment problem inherent in determining which parts of a behavior most contributed to a preference resulting in data intensive approaches and subpar reward models. We address such limitations by introducing a credit assignment strategy (PRIOR) that uses a forward dynamics world model to approximate state importance within a trajectory and then guides rewards to be proportional to state importance through an auxiliary predicted return redistribution objective. Incorporating state importance into reward learning improves the speed of policy learning, overall policy performance, and reward recovery on both locomotion and manipulation tasks. For example, PRIOR achieves 80% success rate with half the amount of data compared to baselines. The performance gains and our ablations demonstrate the benefits even a simple credit assignment strategy can have on reward learning and that state importance in forward dynamics prediction is a strong proxy for a state's contribution to a preference decision.""}",https://openreview.net{'value': '/pdf/83a2844bd7349c71ea3533dc2bea9b7fa6d0018f.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=MrR3rMxqqv,{'value': 'Memorization Capacity of Multi-Head Attention in Transformers'},Sadegh Mahdavi; Renjie Liao; Christos Thrampoulidis,~Sadegh_Mahdavi1; ~Renjie_Liao1; ~Christos_Thrampoulidis1,"{'value': ['Learning Theory', 'Expressivity', 'Multi-Head Attention', 'Transformers']}","{'value': 'Transformers have become the go-to architecture for language and vision tasks, yet their theoretical properties, especially memorization capacity, remain elusive. This paper investigates the memorization abilities of multi-head attention mechanisms, examining how many example sequences they can memorize, as a function of the number of heads and sequence length. Motivated by experimental findings on vision transformers, we introduce novel assumptions about the linear independence of input data, distinct from the commonly used general-position assumption. Under these assumptions, we demonstrate that an attention layer with $H$ heads, dimension $d$, and context size $n < d,$ featuring $\\Theta(Hd^2)$ parameters, can memorize $\\Omega(Hn)$ examples. Our analysis sheds light on how different attention heads handle various example sequences, aided by the softmax operator’s saturation property. We validate our findings through experiments on synthetic data.'}",https://openreview.net{'value': '/pdf/30b070a1d5057982a67ece4fdb61fca629dea9e6.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=MnMWa94t12,{'value': 'DyST: Towards Dynamic Neural Scene Representations on Real-World Videos'},Maximilian Seitzer; Sjoerd van Steenkiste; Thomas Kipf; Klaus Greff; Mehdi S. M. Sajjadi,~Maximilian_Seitzer1; ~Sjoerd_van_Steenkiste1; ~Thomas_Kipf2; ~Klaus_Greff1; ~Mehdi_S._M._Sajjadi1,"{'value': ['neural scene representations', 'scene representations', 'representation learning', 'novel view synthesis']}","{'value': 'Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.'}",https://openreview.net{'value': '/pdf/cb1c5f7dc44ea3c18ca42146caaee182fe578c30.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=MVmT6uQ3cQ,{'value': 'The Need for Speed: Pruning Transformers with One Recipe'},Samir Khaki; Konstantinos N Plataniotis,~Samir_Khaki1; ~Konstantinos_N_Plataniotis1,"{'value': ['transformers', 'pruning', 're-training', 'latency', 'throughput', 'token reduction', 'one-shot']}","{'value': 'We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique for $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained transformer architectures $\\textit{without requiring re-training}$. Recent works have explored improving transformer efficiency, however often incur computationally expensive re-training procedures or depend on architecture-specific characteristics, thus impeding practical wide-scale adoption. \nTo address these shortcomings, the OPTIN framework leverages intermediate feature distillation, capturing the long-range dependencies of model parameters (coined $\\textit{trajectory}$), to produce state-of-the-art results on natural language, image classification, transfer learning, and semantic segmentation tasks $\\textit{without re-training}$. Given a FLOP constraint, the OPTIN framework will compress the network while maintaining competitive accuracy performance and improved throughput. Particularly, we show a $\\leq 2$% accuracy degradation from NLP baselines and a $0.5$% improvement from state-of-the-art methods on image classification at competitive FLOPs reductions. We further demonstrate the generalization of tasks and architecture with comparative performance using Mask2Former for semantic segmentation and cnn-style networks. OPTIN presents one of the first one-shot efficient frameworks for compressing transformer architectures that generalizes well across different class domains, in particular: natural language and image-related tasks, without $\\textit{re-training}$.'}",https://openreview.net{'value': '/pdf/8cbf2648dcd7299075e540b214c292ba1adde95f.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=MO5PiKHELW,"{'value': 'Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs'}",Angelica Chen; Ravid Shwartz-Ziv; Kyunghyun Cho; Matthew L Leavitt; Naomi Saphra,~Angelica_Chen1; ~Ravid_Shwartz-Ziv2; ~Kyunghyun_Cho1; ~Matthew_L_Leavitt1; ~Naomi_Saphra1,"{'value': ['interpretability', 'BERT', 'syntax', 'phase changes', 'simplicity bias', 'training dynamics']}","{'value': 'Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.'}",https://openreview.net{'value': '/pdf/8d2d2b9084da09d4b41f5ad2da660350019c5412.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=MNyOI3C7YB,{'value': 'SEABO: A Simple Search-Based Method for Offline Imitation Learning'},Jiafei Lyu; Xiaoteng Ma; Le Wan; Runze Liu; Xiu Li; Zongqing Lu,~Jiafei_Lyu1; ~Xiaoteng_Ma1; ~Le_Wan1; ~Runze_Liu2; ~Xiu_Li1; ~Zongqing_Lu2,{'value': ['offline imitation learning; reward learning; reinforcement learning']},"{'value': 'Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL algorithms with ground-truth rewards, given only a single expert trajectory, and can outperform prior reward learning and offline IL methods across many tasks. Moreover, we demonstrate that SEABO also works well if the expert demonstrations contain only observations. Our code is publicly available at https://github.com/dmksjfl/SEABO.'}",https://openreview.net{'value': '/pdf/8623a11c82d4d56bfa3ba1d11b5337ba469cf4c6.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=MHjigVnI04,{'value': 'High-dimensional SGD aligns with emerging outlier eigenspaces'},Gerard Ben Arous; Reza Gheissari; Jiaoyang Huang; Aukosh Jagannath,~Gerard_Ben_Arous1; ~Reza_Gheissari1; ~Jiaoyang_Huang1; ~Aukosh_Jagannath1,"{'value': ['stochastic gradient descent', 'Hessian', 'multi-layer neural networks', 'high-dimensional classification', 'Gaussian mixture model', 'XOR problem']}","{'value': ""We rigorously study the joint evolution of training dynamics via stochastic gradient descent (SGD) and the spectra of empirical Hessian and gradient matrices. We prove that in two canonical classification tasks for multi-class high-dimensional mixtures and either 1 or 2-layer neural networks, the SGD trajectory rapidly aligns with emerging low-rank outlier eigenspaces of the Hessian and gradient matrices. Moreover, in multi-layer settings this alignment occurs per layer, with the final layer's outlier eigenspace evolving over the course of training, and exhibiting rank deficiency when the SGD converges to sub-optimal classifiers. This establishes  some of the rich predictions that have arisen from extensive numerical studies in the last decade about the spectra of Hessian and information matrices over the course of training in overparametrized networks.""}",https://openreview.net{'value': '/pdf/c8dc4127b70cc66dd5a649a848ee32a1b553666a.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=MCNqgUFTHI,{'value': 'Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents'},Yang Deng; Wenxuan Zhang; Wai Lam; See-Kiong Ng; Tat-Seng Chua,~Yang_Deng4; ~Wenxuan_Zhang1; ~Wai_Lam1; ~See-Kiong_Ng1; ~Tat-Seng_Chua2,"{'value': ['Dialogue Policy Planning', 'Proactive Dialogue', 'Large Language Model']}","{'value': 'Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.'}",https://openreview.net{'value': '/pdf/30c7e22635f23140a4f37883885b6e80b5f7c750.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=M6XWoEdmwf,{'value': 'AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents'},Jake Grigsby; Linxi Fan; Yuke Zhu,~Jake_Grigsby1; ~Linxi_Fan2; ~Yuke_Zhu1,"{'value': ['Meta-RL', 'Generalization', 'Long-Term Memory', 'Transformers']}","{'value': ""We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments.""}",https://openreview.net{'value': '/pdf/6ffd1eb5dc0bd2b144d5d0309763b3ed5e114e8b.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=LjivA1SLZ6,{'value': 'Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning'},Hyungho Na; Yunkyeong Seo; Il-chul Moon,~Hyungho_Na1; ~Yunkyeong_Seo1; ~Il-chul_Moon1,"{'value': ['Multi-agent reinforcement learning', 'episodic control', 'episodic incentive', 'state embedding']}","{'value': 'In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.'}",https://openreview.net{'value': '/pdf/8b2d5ac5539754d00bf99458a60c63157c74fbdb.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=LemSSn8htt,{'value': 'Delta-AI: Local objectives for amortized inference in sparse graphical models'},Jean-Pierre René Falet; Hae Beom Lee; Nikolay Malkin; Chen Sun; Dragos Secrieru; Dinghuai Zhang; Guillaume Lajoie; Yoshua Bengio,~Jean-Pierre_René_Falet1; ~Hae_Beom_Lee1; ~Nikolay_Malkin1; ~Chen_Sun7; ~Dragos_Secrieru1; ~Dinghuai_Zhang1; ~Guillaume_Lajoie1; ~Yoshua_Bengio1,"{'value': ['amortized inference', 'variational inference', 'graphical models', 'Markov random fields', 'generative flow networks', 'GFlowNets']}","{'value': ""We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call $\\Delta$-amortized inference ($\\Delta$-AI). Our approach is based on the observation that when the sampling of variables in a PGM is seen as a sequence of actions taken by an agent, sparsity of the PGM enables local credit assignment in the agent's policy learning objective. This yields a local constraint that can be turned into a local loss in the style of generative flow networks (GFlowNets) that enables off-policy training but avoids the need to instantiate all the random variables for each parameter update, thus speeding up training considerably. The $\\Delta$-AI objective matches the conditional distribution of a variable given its Markov blanket in a tractable learned sampler, which has the structure of a Bayesian network, with the same conditional distribution under the target PGM. As such, the trained sampler recovers marginals and conditional distributions of interest and enables inference of partial subsets of variables. We illustrate $\\Delta$-AI's effectiveness for sampling from synthetic PGMs and training latent variable models with sparse factor structure. Code: https://github.com/GFNOrg/Delta-AI.""}",https://openreview.net{'value': '/pdf/05a032243d28184a3102f9fc03c775e53cc11f84.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=LYG6tBlEX0,{'value': 'H-GAP: Humanoid Control with a Generalist Planner'},zhengyao jiang; Yingchen Xu; Nolan Wagener; Yicheng Luo; Michael Janner; Edward Grefenstette; Tim Rocktäschel; Yuandong Tian,~zhengyao_jiang2; ~Yingchen_Xu2; ~Nolan_Wagener1; ~Yicheng_Luo1; ~Michael_Janner1; ~Edward_Grefenstette1; ~Tim_Rocktäschel1; ~Yuandong_Tian1,"{'value': ['Generative Modelling', 'Humanoid Control', 'Model Predictive Control', 'Model-based Reinforcement Learning', 'Offline Reinforcement Learning']}","{'value': 'Humanoid control is an important research challenge offering avenues for integration into human-centric infrastructures and enabling physics-driven humanoid animations.\nThe daunting challenges in this field stem from the difficulty of optimizing in high-dimensional action spaces and the instability introduced by the bipedal morphology of humanoids. \nHowever, the extensive collection of human motion-captured data and the derived datasets of humanoid trajectories, such as MoCapAct, paves the way to tackle these challenges. In this context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a state-action trajectory generative model trained on humanoid trajectories derived from human motion-captured data, capable of adeptly handling downstream control tasks with Model Predictive Control (MPC).\nFor 56 degrees of freedom humanoid, we empirically demonstrate that H-GAP learns to represent and generate a wide range of motor behaviors. Further, without any learning from online interactions, it can also flexibly transfer these behaviours to solve novel downstream control tasks via planning. Notably, H-GAP excels established MPC baselines with access to the ground truth model, and is superior or comparable to offline RL methods trained for individual tasks.\nFinally, we do a series of empirical studies on the scaling properties of H-GAP, showing the potential for performance gains via additional data but not computing.'}",https://openreview.net{'value': '/pdf/5572333360c59f829af61902b8f2157f4a2e4109.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=LY3ukUANko,{'value': 'Zoology: Measuring and Improving  Recall in Efficient Language Models'},Simran Arora; Sabri Eyuboglu; Aman Timalsina; Isys Johnson; Michael Poli; James Zou; Atri Rudra; Christopher Re,~Simran_Arora1; ~Sabri_Eyuboglu1; ~Aman_Timalsina1; ~Isys_Johnson1; ~Michael_Poli1; ~James_Zou1; ~Atri_Rudra1; ~Christopher_Re1,"{'value': ['nlp', 'language models', 'representation learning', 'in-context learning']}","{'value': 'Attention-free language models that combine gating and convolutions are growing in popularity due to their efficiency and increasingly competitive performance. To better understand these architectures, we pretrain a suite of 17 attention and gated-convolution language models, finding that SoTA gated-convolution architectures still underperform attention by up to 2.1 perplexity points on the Pile. In fine-grained analysis, we find 82% of the gap is explained by each model\'s ability to recall information that is previously mentioned in-context, e.g. ""Hakuna Matata means no worries Hakuna Matata it means no"" -> ??. On this task, termed ""associative recall"", we find that attention outperforms gated-convolutions by a large margin: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution model on associative recall. This is surprising because prior work shows gated convolutions can perfectly solve synthetic tests for AR capability.  To close the gap between synthetics and real language, we develop a new formalization of the task called multi-query associative recall (MQAR) that better reflects actual language. We perform an empirical and theoretical study of MQAR that elucidates differences in the parameter-efficiency of attention and gated-convolution recall. Informed by our analysis, we evaluate simple convolution-attention hybrids and show that hybrids with input-dependent sparse attention patterns can close 97.4% of the gap to attention, while maintaining sub-quadratic scaling. Code is at: https://github.com/HazyResearch/zoology.'}",https://openreview.net{'value': '/pdf/80fadc4fc3c3c1a12600b7f7e0e3d13e04ac334b.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=KsUh8MMFKQ,{'value': 'Thin-Shell Object Manipulations With Differentiable Physics Simulations'},Yian Wang; Juntian Zheng; Zhehuan Chen; Zhou Xian; Gu Zhang; Chao Liu; Chuang Gan,~Yian_Wang1; ~Juntian_Zheng1; ~Zhehuan_Chen1; ~Zhou_Xian1; ~Gu_Zhang1; ~Chao_Liu9; ~Chuang_Gan1,"{'value': ['differentiable physics simulation', 'thin-shell object manipulation']}","{'value': ""In this work, we aim to teach robots to manipulate various thin-shell materials. \nPrior works studying thin-shell object manipulation mostly rely on heuristic policies or learn policies from real-world video demonstrations, and only focus on limited material types and tasks (e.g., cloth unfolding). However, these approaches face significant challenges when extended to a wider variety of thin-shell materials and a diverse range of tasks.\nOn the other hand, while virtual simulations are shown to be effective in diverse robot skill learning and evaluation, prior thin-shell simulation environments only support a subset of thin-shell materials, which also limits their supported range of tasks. \nTo fill in this gap, we introduce ThinShellLab - a fully differentiable simulation platform tailored for robotic interactions with diverse thin-shell materials possessing varying material properties, enabling flexible thin-shell manipulation skill learning and evaluation. Building on top of our developed simulation engine, we design a diverse set of manipulation tasks centered around different thin-shell objects. Our experiments suggest that manipulating thin-shell objects presents several unique challenges: 1) thin-shell manipulation relies heavily on frictional forces due to the objects' co-dimensional nature, 2) the materials being manipulated are highly sensitive to minimal variations in interaction actions, and 3) the constant and frequent alteration in contact pairs makes trajectory optimization methods susceptible to local optima, and neither standard reinforcement learning algorithms nor trajectory optimization methods (either gradient-based or gradient-free) are able to solve the tasks alone. To overcome these challenges, we present an optimization scheme that couples sampling-based trajectory optimization and gradient-based optimization, boosting both learning efficiency and converged performance across various proposed tasks. In addition, the differentiable nature of our platform facilitates a smooth sim-to-real transition. By tuning simulation parameters with a minimal set of real-world data, we demonstrate successful deployment of the learned skills to real-robot settings.  ThinShellLab will be publicly available. Video demonstration and more information can be found on the project website https://vis-www.cs.umass.edu/ThinShellLab/.""}",https://openreview.net{'value': '/pdf/edbd3be2c1ca4369cdf41d2d892af284a0c21cc3.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=KpoQSgxbKH,{'value': 'Generative Pre-training for Speech with Flow Matching'},Alexander H. Liu; Matthew Le; Apoorv Vyas; Bowen Shi; Andros Tjandra; Wei-Ning Hsu,~Alexander_H._Liu1; ~Matthew_Le2; ~Apoorv_Vyas1; ~Bowen_Shi1; ~Andros_Tjandra1; ~Wei-Ning_Hsu2,"{'value': ['speech pre-training', 'speech generation', 'generative pre-training', 'flow matching']}","{'value': 'Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested a foundational model for generation tasks in speech can be built with generative pre-training.'}",https://openreview.net{'value': '/pdf/965802a3d520fc25d1998740667c0a9b3f841a8f.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=KdVvOA00Or,{'value': 'ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift'},Hwanwoo Kim; Xin Zhang; Jiwei Zhao; Qinglong Tian,~Hwanwoo_Kim1; ~Xin_Zhang16; ~Jiwei_Zhao1; ~Qinglong_Tian1,"{'value': ['label shift', 'target shift', 'distributional shift', 'domain adaptation', 'transfer learning', 'importance weight']}","{'value': 'The presence of distribution shifts poses a significant challenge for deploying modern machine learning models in real-world applications. This work focuses on the target shift problem in a regression setting  (Zhang et al., 2013; Nguyen et al., 2016). More specifically, the target variable $y$ (also known as the response variable), which is continuous, has different marginal distributions in the training source and testing domain, while the conditional distribution of features $\\boldsymbol{x}$ given $y$ remains the same. While most literature focuses on classification tasks with finite target space, the regression problem has an *infinite dimensional* target space, which makes many of the existing methods inapplicable. In this work, we show that the continuous target shift problem can be addressed by estimating the importance weight function from an ill-posed integral equation. We propose a nonparametric regularized approach named *ReTaSA* to solve the ill-posed integral equation and provide theoretical justification for the estimated importance weight function. The effectiveness of the proposed method has been demonstrated with extensive numerical studies on synthetic and real-world datasets.'}",https://openreview.net{'value': '/pdf/d1da110b25cfb405463528a072d97b9b1bf77c6c.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=KY8ZNcljVU,{'value': 'NetInfoF Framework: Measuring and Exploiting Network Usable Information'},Meng-Chieh Lee; Haiyang Yu; Jian Zhang; Vassilis N. Ioannidis; Xiang song; Soji Adeshina; Da Zheng; Christos Faloutsos,~Meng-Chieh_Lee1; ~Haiyang_Yu6; ~Jian_Zhang32; ~Vassilis_N._Ioannidis1; ~Xiang_song1; ~Soji_Adeshina1; ~Da_Zheng1; ~Christos_Faloutsos1,"{'value': ['Graph Neural Networks', 'Information Theory', 'Heterophily Graphs']}","{'value': 'Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network (GNN) will perform well? More specifically, do the graph structure and the node features carry enough usable information for the task? Our goals are\n(1) to develop a fast tool to measure how much information is in the graph structure and in the node features, and\n(2) to exploit the information to solve the task, if there is enough.\nWe propose NetInfoF, a framework including NetInfoF_Probe and NetInfoF_Act, for the measurement and the exploitation of network usable information (NUI), respectively. Given a graph data, NetInfoF_Probe measures NUI without any model training, and NetInfoF_Act solves link prediction and node classification, while two modules share the same backbone.\nIn summary, NetInfoF has following notable advantages:\n(a) General, handling both link prediction and node classification;\n(b) Principled, with theoretical guarantee and closed-form solution;\n(c) Effective, thanks to the proposed adjustment to node similarity;\n(d) Scalable, scaling linearly with the input size.\nIn our carefully designed synthetic datasets, NetInfoF correctly identifies the ground truth of NUI and is the only method being robust to all graph scenarios. Applied on real-world datasets, NetInfoF wins in 11 out of 12 times on link prediction compared to general GNN baselines.'}",https://openreview.net{'value': '/pdf/472fc3640b561e46ab9bb9189f262b42f5ef4114.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=KOZu91CzbK,{'value': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization'},Weiran Yao; Shelby Heinecke; Juan Carlos Niebles; Zhiwei Liu; Yihao Feng; Le Xue; Rithesh R N; Zeyuan Chen; Jianguo Zhang; Devansh Arpit; Ran Xu; Phil L Mui; Huan Wang; Caiming Xiong; Silvio Savarese,~Weiran_Yao1; ~Shelby_Heinecke1; ~Juan_Carlos_Niebles1; ~Zhiwei_Liu3; ~Yihao_Feng1; ~Le_Xue1; ~Rithesh_R_N1; ~Zeyuan_Chen1; ~Jianguo_Zhang3; ~Devansh_Arpit2; ~Ran_Xu1; ~Phil_L_Mui1; ~Huan_Wang1; ~Caiming_Xiong1; ~Silvio_Savarese1,"{'value': ['Language Agent', 'AI Agent', 'Reinforcement Learning']}","{'value': 'Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.'}",https://openreview.net{'value': '/pdf/d02f39256b41ec50ca8cd3a5b136065bc7a4caae.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=JORAfH2xFd,{'value': 'On the Stability of Iterative Retraining of Generative Models on their own Data'},Quentin Bertrand; Joey Bose; Alexandre Duplessis; Marco Jiralerspong; Gauthier Gidel,~Quentin_Bertrand1; ~Joey_Bose1; ~Alexandre_Duplessis1; ~Marco_Jiralerspong1; ~Gauthier_Gidel1,"{'value': ['Generative Models', 'Iterative Training', 'Diffusion']}","{'value': ""Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models will be trained on both clean and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets---from classical training on real data to self-consuming generative models trained on purely synthetic data. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion of clean training data (w.r.t. synthetic data) is large enough. We empirically validate our theory on both synthetic and natural images by iteratively training normalizing flows and state-of-the-art diffusion models on CIFAR10 and FFHQ.""}",https://openreview.net{'value': '/pdf/119c01132e3767a01a8777074185c4c7f8fb3824.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=IoKRezZMxF,{'value': 'Consistent Video-to-Video Transfer Using Synthetic Dataset'},Jiaxin Cheng; Tianjun Xiao; Tong He,~Jiaxin_Cheng1; ~Tianjun_Xiao1; ~Tong_He5,"{'value': ['Computer Vision', 'Video Editing', 'Diffusion Model']}","{'value': ""We introduce a novel and efficient approach for text-based video-to-video editing that eliminates the need for resource-intensive per-video-per-model finetuning. At the core of our approach is a synthetic paired video dataset tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix's image transfer via editing instruction, we adapt this paradigm to the video domain. Extending the Prompt-to-Prompt to videos, we efficiently generate paired samples, each with an input video and its edited counterpart. Alongside this, we introduce the Long Video Sampling Correction during sampling, ensuring consistent long videos across batches. Our method surpasses current methods like Tune-A-Video, heralding substantial progress in text-based video-to-video editing and suggesting exciting avenues for further exploration and deployment.""}",https://openreview.net{'value': '/pdf/e91267e5675cf150b685ada8cd0303644c45a25f.pdf'},{'title_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=I2mIxuXA72,{'value': 'Understanding Domain Generalization: A Noise Robustness Perspective'},Rui Qiao; Bryan Kian Hsiang Low,~Rui_Qiao3; ~Bryan_Kian_Hsiang_Low1,"{'value': ['out-of-distribution generalization', 'distribution shifts', 'spurious correlation', 'noise robustness']}","{'value': 'Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise.\nSpecifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. \nConversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present.\nSuch desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. \nHowever, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. \nWe conjecture that the failure mode of ERM arising from spurious correlations may be less pronounced in practice. Our code is available at https://github.com/qiaoruiyt/NoiseRobustDG'}",https://openreview.net{'value': '/pdf/a53902576d4d742fd5f4175a0e1c0ee4657327ad.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=HrRKc9ei7h,{'value': 'Oracle Efficient Algorithms for Groupwise Regret'},Krishna Acharya; Eshwar Ram Arunachaleswaran; Sampath Kannan; Aaron Roth; Juba Ziani,~Krishna_Acharya1; ~Eshwar_Ram_Arunachaleswaran2; ~Sampath_Kannan1; ~Aaron_Roth1; ~Juba_Ziani1,"{'value': ['Oracle efficiency', 'online learning', 'groupwise regret', 'sleeping experts']}","{'value': ""We study the problem of online prediction, in which at each time step $t \\in \\{1,2, \\cdots T\\}$, an individual $x_t$ arrives, whose label we must predict. Each individual is associated with various groups, defined based on their features such as age, sex, race etc., which may intersect. Our goal is to make predictions that have regret guarantees not just overall but also simultaneously on each sub-sequence comprised of the members of any single group. Previous work such as  [Blum & Lykouris][1] and [Lee et al][2] provide attractive regret guarantees for these problems; however, these are computationally intractable on large model classes (e.g., the set of all linear models, as used in linear regression). We show that a simple modification of the sleeping experts technique of [Blum & Lykouris][1] yields an efficient *reduction* to the well-understood problem of obtaining diminishing external regret *absent group considerations*. \nOur approach gives similar regret guarantees compared to [Blum & Lykouris][1]; however, we run in time linear in the number of groups, and are oracle-efficient in the hypothesis class. This in particular implies that our algorithm is efficient whenever the number of groups is  polynomially bounded and the external-regret problem can be solved efficiently, an improvement on [Blum & Lykouris][1]'s stronger condition that the model class must be small. Our approach can handle online linear regression and online combinatorial optimization problems like online shortest paths. Beyond providing theoretical regret bounds, we evaluate this algorithm with an extensive set of experiments on synthetic data and on two real data sets --- Medical costs and the Adult income dataset, both instantiated with intersecting groups defined in terms of race, sex, and other demographic characteristics. \nWe find that uniformly across groups, our algorithm gives substantial error improvements compared to running a standard online linear regression algorithm with no groupwise regret guarantees.""}",https://openreview.net{'value': '/pdf/92a9ae57ae609528668b1a1e1b9ac6640ac6b40f.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=HhfcNgQn6p,{'value': 'Towards a statistical theory of data selection under weak supervision'},Germain Kolossov; Andrea Montanari; Pulkit Tandon,~Germain_Kolossov1; ~Andrea_Montanari1; ~Pulkit_Tandon1,"{'value': ['Data Selection', 'Empirical Risk Minimization', 'Influence Functions', 'High dimensional asymptotics']}","{'value': ""Given a sample of size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning.  Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $x_{i}$, and to be given access to a  'surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by {$x_{i}$}$_{i\\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization. By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$ Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$ Certain popular choices in data selection methods (e.g. unbiased reweighted subsampling, or influence function-based subsampling) can be substantially suboptimal.""}",https://openreview.net{'value': '/pdf/3afcd7230f6f462e837b839132c8cdd6cfceb037.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=HSKaGOi7Ar,{'value': 'Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness'},Bohang Zhang; Jingchu Gai; Yiheng Du; Qiwei Ye; Di He; Liwei Wang,~Bohang_Zhang1; ~Jingchu_Gai1; ~Yiheng_Du1; ~Qiwei_Ye1; ~Di_He1; ~Liwei_Wang1,"{'value': ['Graph Neural Networks', 'Expressive Power', 'Homomorphism', 'Subgraph Counting', 'Weisfeiler-Lehman']}","{'value': 'Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in the graph learning community. So far, GNN expressiveness has been primarily assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse, qualitative, and may not well reflect practical requirements (e.g., the ability to encode substructures). In this paper, we introduce a novel framework for quantitatively studying the expressiveness of GNN architectures, addressing all the above limitations. Specifically, we identify a fundamental expressivity measure termed homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. Homomorphism expressivity offers a complete and practical assessment tool: the completeness enables direct expressivity comparisons between GNN models, while the practicality allows for understanding concrete GNN abilities such as subgraph counting. By examining four classes of prominent GNNs as case studies, we derive simple, unified, and elegant descriptions of their homomorphism expressivity for both invariant and equivariant settings. Our results provide novel insights into a series of previous work, unify the landscape of different subareas in the community, and settle several open questions. Empirically, extensive experiments on both synthetic and real-world tasks verify our theory, showing that the practical performance of GNN models aligns well with the proposed metric.'}",https://openreview.net{'value': '/pdf/1cdf9d7930ee08e1c02c2c2819a16e7a2cc56a4b.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=HMe5CJv9dQ,{'value': 'Efficiently Computing Similarities to Private Datasets'},Arturs Backurs; Zinan Lin; Sepideh Mahabadi; Sandeep Silwal; Jakub Tarnawski,~Arturs_Backurs1; ~Zinan_Lin1; ~Sepideh_Mahabadi1; ~Sandeep_Silwal1; ~Jakub_Tarnawski1,"{'value': ['differential privacy', 'distance estimation', 'sublinear algorithms', 'high dimensional data analysis', 'kernels']}","{'value': ""Many methods in differentially private model training rely on computing the similarity between a query point (such as public or synthetic data) and private data. We abstract out this common subroutine and study the following fundamental algorithmic problem: Given a similarity function $f$ and a large high-dimensional private dataset $X \\subset \\mathbb{R}^d$, output a differentially private (DP) data-structure which approximates $\\sum_{x \\in X} f(x,y)$ for any query $y$. We consider the cases where $f$ is a kernel function, such as $f(x,y) = e^{-\\|x-y\\|_2^2/\\sigma^2}$ (also known as DP kernel density estimation), or a distance function such as $f(x,y) = \\|x-y\\|_2$, among others. \n    \nOur theoretical results improve upon prior work and give better privacy-utility trade-offs as well as faster query times for a wide range of kernels and distance functions. The unifying approach behind our results is leveraging `low-dimensional structures' present in the specific functions $f$ that we study, using tools such as provable dimensionality reduction, approximation theory, and one-dimensional decomposition of the functions. Our algorithms empirically exhibit improved query times and accuracy over prior state of the art. We also present an application to DP classification. Our experiments demonstrate that the simple methodology of classifying based on average similarity is orders of magnitude faster than prior DP-SGD based approaches for comparable accuracy.""}",https://openreview.net{'value': '/pdf/ad160dc6a510cce51aef5d56a4241029739ada3c.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=HFtrXBfNru,{'value': 'Temporal Generalization Estimation in Evolving Graphs'},Bin Lu; Tingyan Ma; Xiaoying Gan; Xinbing Wang; Yunqiang Zhu; Chenghu Zhou; Shiyu Liang,~Bin_Lu2; ~Tingyan_Ma1; ~Xiaoying_Gan1; ~Xinbing_Wang1; ~Yunqiang_Zhu1; ~Chenghu_Zhou3; ~Shiyu_Liang1,"{'value': ['Generalization Estimation', 'Evolving Graph', 'Graph Neural Network']}","{'value': 'Graph Neural Networks (GNNs) are widely deployed in vast fields, but they often struggle to maintain accurate representations as graphs evolve. We theoretically establish a lower bound, proving that under mild conditions, representation distortion inevitably occurs over time. To estimate the temporal distortion without human annotation after deployment, one naive approach is to pre-train a recurrent model (e.g., RNN) before deployment and use this model afterwards, but the estimation is far from satisfactory. In this paper, we analyze the representation distortion from an information theory perspective, and attribute it primarily to inaccurate feature extraction during evolution. Consequently, we introduce Smart, a straightforward and effective baseline enhanced by an adaptive feature extractor through self-supervised graph reconstruction. In synthetic random graphs, we further refine the former lower bound to show the inevitable distortion over time and empirically observe that Smart achieves good estimation performance. Moreover, we observe that Smart consistently shows outstanding generalization estimation on four real-world evolving graphs. The ablation studies underscore the necessity of graph reconstruction. For example, on OGB-arXiv dataset, the estimation metric MAPE deteriorates from 2.19% to 8.00% without reconstruction.'}",https://openreview.net{'value': '/pdf/f6c382372adc5cbff0284513e93b4f110453dff0.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=HE9eUQlAvo,"{'value': '""What Data Benefits My Classifier?"" Enhancing Model Performance and Interpretability through Influence-Based Data Selection'}",Anshuman Chhabra; Peizhao Li; Prasant Mohapatra; Hongfu Liu,~Anshuman_Chhabra1; ~Peizhao_Li1; ~Prasant_Mohapatra1; ~Hongfu_Liu2,"{'value': ['Data Selection', 'Interpretability', 'Fairness', 'Robustness']}","{'value': ""Classification models are ubiquitously deployed in society and necessitate high utility, fairness, and robustness performance. Current research efforts mainly focus on improving model architectures and learning algorithms on fixed datasets to achieve this goal. In contrast, in this paper, we address an orthogonal yet crucial problem: given a fixed convex learning model (or a convex surrogate for a non-convex model) and a function of interest, we assess what data benefits the model by interpreting the feature space, and then aim to improve performance as measured by this function. To this end, we propose the use of influence estimation models for interpreting the classifier's performance from the perspective of the data feature space. Additionally, we propose data selection approaches based on influence that enhance model utility, fairness, and robustness. Through extensive experiments on synthetic and real-world datasets, we validate and demonstrate the effectiveness of our approaches not only for conventional classification scenarios, but also under more challenging scenarios such as distribution shifts, fairness poisoning attacks, utility evasion attacks, online learning, and active learning.""}",https://openreview.net{'value': '/pdf/c9c086d91e0480dcd349f7bb625a5031fabcc53a.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=GSBHKiw19c,{'value': 'Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning'},Fan-Ming Luo; Tian Xu; Xingchen Cao; Yang Yu,~Fan-Ming_Luo1; ~Tian_Xu2; ~Xingchen_Cao1; ~Yang_Yu5,"{'value': ['model-based offline reinforcement learning', 'dynamics reward', 'reward-consistent dynamics model learning']}","{'value': 'Learning a precise dynamics model can be crucial for offline reinforcement learning, which, unfortunately, has been found to be quite challenging. Dynamics models that are learned by fitting historical transitions often struggle to generalize to unseen transitions. In this study, we identify a hidden but pivotal factor termed dynamics reward that remains consistent across transitions, offering a pathway to better generalization. Therefore, we propose the idea of reward-consistent dynamics models: any trajectory generated by the dynamics model should maximize the dynamics reward derived from the data. We implement this idea as the MOREC (Model-based Offline reinforcement learning with Reward Consistency) method, which can be seamlessly integrated into previous offline model-based reinforcement learning (MBRL) methods. MOREC learns a generalizable dynamics reward function from offline data, which is subsequently employed as a transition filter in any offline MBRL method: when generating transitions, the dynamics model generates a batch of transitions and selects the one with the highest dynamics reward value. On a synthetic task, we visualize that MOREC has a strong generalization ability and can surprisingly recover some distant unseen transitions. On 21 offline tasks in D4RL and NeoRL benchmarks, MOREC improves the previous state-of-the-art performance by a significant margin, i.e., 4.6\\% on D4RL tasks and 25.9\\% on NeoRL tasks. Notably, MOREC is the first method that can achieve above 95\\% online RL performance in 6 out of 12 D4RL tasks and 3 out of 9 NeoRL tasks. Code is available at https://github.com/polixir/morec.'}",https://openreview.net{'value': '/pdf/b755072e1d772c902d9b57e9ad4a0ff78b0df063.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=GEcwtMk1uA,{'value': 'Identifying the Risks of LM Agents with an LM-Emulated Sandbox'},Yangjun Ruan; Honghua Dong; Andrew Wang; Silviu Pitis; Yongchao Zhou; Jimmy Ba; Yann Dubois; Chris J. Maddison; Tatsunori Hashimoto,~Yangjun_Ruan1; ~Honghua_Dong1; ~Andrew_Wang4; ~Silviu_Pitis1; ~Yongchao_Zhou1; ~Jimmy_Ba1; ~Yann_Dubois1; ~Chris_J._Maddison1; ~Tatsunori_Hashimoto1,"{'value': ['Language Model Agent', 'Tool Use', 'Evaluation', 'Safety', 'Language Model']}","{'value': 'Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes toolkits and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.'}",https://openreview.net{'value': '/pdf/d1601f78407737fc216de9e6ec0085038f8c885f.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=FvK2noilxT,{'value': 'GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion'},Xueyi Liu; Li Yi,~Xueyi_Liu1; ~Li_Yi2,"{'value': ['motion refinement', 'hand-object interaction', 'inverse problem', 'generative prior']}","{'value': ""In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence.  This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data samples from a whitened noise space to a clean data manifold and a ``denoising via diffusion'' strategy which can handle input trajectories with various noise patterns by first diffusing them to align with the whitened noise space and cleaning via the canonical denoiser. Extensive experiments on four benchmarks with significant domain variations demonstrate the superior effectiveness of our method. GeneOH Diffusion also shows promise for various downstream applications. We include [a website](https://meowuu7.github.io/GeneOH-Diffusion/) for introducing the work.""}",https://openreview.net{'value': '/pdf/758b44508c97b8e9709281ba88fb4e1cc4c92077.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Fj7Fzm5lWL,"{'value': ""Let's do the time-warp-attend: Learning topological invariants of dynamical systems""}",Noa Moriel; Matt Ricci; Mor Nitzan,~Noa_Moriel1; ~Matt_Ricci1; ~Mor_Nitzan1,"{'value': ['dynamical systems', 'bifurcations', 'topological invariance', 'Hopf bifurcation', 'physics-informed machine learning', 'augmentation', 'single-cell RNA-sequencing']}","{'value': ""Dynamical systems across the sciences, from electrical circuits to ecological networks, undergo qualitative and often catastrophic changes in behavior, called bifurcations, when their underlying parameters cross a threshold. Existing methods predict oncoming catastrophes in individual systems but are primarily time-series-based and struggle both to categorize qualitative dynamical regimes across diverse systems and to generalize to real data. To address this challenge, we propose a data-driven, physically-informed deep-learning framework for classifying dynamical regimes and characterizing bifurcation boundaries based on the extraction of topologically invariant features. We focus on the paradigmatic case of the supercritical Hopf bifurcation, which is used to model periodic dynamics across a wide range of applications. Our convolutional attention method is trained with data augmentations that encourage the learning of topological invariants which can be used to detect bifurcation boundaries in unseen systems and to design models of biological systems like oscillatory gene regulatory networks. We further demonstrate our method's use in analyzing real data by recovering distinct proliferation and differentiation dynamics along pancreatic endocrinogenesis trajectory in gene expression space based on single-cell data. Our method provides valuable insights into the qualitative, long-term behavior of a wide range of dynamical systems, and can detect bifurcations or catastrophic transitions in large-scale physical and biological systems.""}",https://openreview.net{'value': '/pdf/8dc9a0a901c8cc5a3348f7f9bb9b466a836c04e9.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=FhQSGhBlqv,{'value': 'A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables'},Xinshuai Dong; Biwei Huang; Ignavier Ng; Xiangchen Song; Yujia Zheng; Songyao Jin; Roberto Legaspi; Peter Spirtes; Kun Zhang,~Xinshuai_Dong1; ~Biwei_Huang1; ~Ignavier_Ng1; ~Xiangchen_Song1; ~Yujia_Zheng1; ~Songyao_Jin1; ~Roberto_Legaspi1; ~Peter_Spirtes1; ~Kun_Zhang1,{'value': ['Causal Discovery; Latent Variables']},"{'value': 'Most existing causal discovery methods rely on the assumption of no latent confounders, limiting their applicability in solving real-life problems. In this paper, we introduce a novel, versatile framework for causal discovery that accommodates the presence of causally-related hidden variables almost everywhere in the causal network (for instance, they can be effects of measured variables), based on rank information of covariance matrix over measured variables. We start by investigating the efficacy of rank in comparison to conditional independence and, theoretically, establish necessary and sufficient conditions for the identifiability of certain latent structural patterns. Furthermore, we develop a  Rank-based Latent Causal Discovery algorithm, RLCD, that can efficiently locate hidden variables, determine their cardinalities, and discover the entire causal structure over both measured and hidden ones. We also show that, under certain graphical conditions, RLCD correctly identifies the Markov Equivalence Class of the whole latent causal graph asymptotically. Experimental results on both synthetic and real-world personality data sets demonstrate the efficacy of the proposed approach in finite-sample cases. Our code will be publicly available.'}",https://openreview.net{'value': '/pdf/8f4691fbb70a9d84f611815c4c5f2456d66f1a30.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=FQepisCUWu,{'value': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate'},Chi-Min Chan; Weize Chen; Yusheng Su; Jianxuan Yu; Wei Xue; Shanghang Zhang; Jie Fu; Zhiyuan Liu,~Chi-Min_Chan1; ~Weize_Chen1; ~Yusheng_Su1; ~Jianxuan_Yu1; ~Wei_Xue5; ~Shanghang_Zhang4; ~Jie_Fu2; ~Zhiyuan_Liu1,"{'value': ['Large Language Model', 'Multi-Agent Debate', 'LLM evaluators']}","{'value': ""Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies.\nIn this paper, we construct a multi-agent referee team called $\\textbf{ChatEval}$ to autonomously discuss and evaluate the quality of different texts. \nOur experiments on two benchmarks illustrate that ChatEval delivers superior accuracy and correlation in alignment with human assessment. Furthermore, we find that the diverse role prompts (different personas) are essential in the multi-agent debate process; that is, utilizing the same role description in the prompts can lead to a degradation in performance. Our qualitative analysis also shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments.""}",https://openreview.net{'value': '/pdf/6950b70dde4f8d7425424dce9cc4063f505b2194.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=F1TKzG8LJO,{'value': 'RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches'},Jiayuan Gu; Sean Kirmani; Paul Wohlhart; Yao Lu; Montserrat Gonzalez Arenas; Kanishka Rao; Wenhao Yu; Chuyuan Fu; Keerthana Gopalakrishnan; Zhuo Xu; Priya Sundaresan; Peng Xu; Hao Su; Karol Hausman; Chelsea Finn; Quan Vuong; Ted Xiao,~Jiayuan_Gu1; ~Sean_Kirmani1; ~Paul_Wohlhart1; ~Yao_Lu13; ~Montserrat_Gonzalez_Arenas1; ~Kanishka_Rao1; ~Wenhao_Yu1; ~Chuyuan_Fu1; ~Keerthana_Gopalakrishnan1; ~Zhuo_Xu1; ~Priya_Sundaresan1; ~Peng_Xu9; ~Hao_Su1; ~Karol_Hausman2; ~Chelsea_Finn1; ~Quan_Vuong2; ~Ted_Xiao1,"{'value': ['robotics', 'robot learning', 'robot manipulation', 'task representation', 'behavior cloning', 'multitask imitation learning', 'goal conditioning']}","{'value': 'Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies -- they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.'}",https://openreview.net{'value': '/pdf/99c49fe414f0c5349b9a1f94d32198a847626df5.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EpVe8jAjdx,{'value': 'Privileged Sensing Scaffolds Reinforcement Learning'},Edward S. Hu; James Springer; Oleh Rybkin; Dinesh Jayaraman,~Edward_S._Hu1; ~James_Springer1; ~Oleh_Rybkin1; ~Dinesh_Jayaraman2,"{'value': ['reinforcement learning', 'model-based reinforcement learning', 'world models', 'robotics', 'privileged information', 'asymmetric learning', 'multimodality', 'perception', 'sensing']}","{'value': 'We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon “sensory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose “Scaffolder”, a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new “S3” suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/'}",https://openreview.net{'value': '/pdf/8ab7ec56b6e56ac45ce2b68e95b277283dbb8377.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Ep0TtjVoap,{'value': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving'},Zhibin Gou; Zhihong Shao; Yeyun Gong; yelong shen; Yujiu Yang; Minlie Huang; Nan Duan; Weizhu Chen,~Zhibin_Gou1; ~Zhihong_Shao1; ~Yeyun_Gong2; ~yelong_shen1; ~Yujiu_Yang2; ~Minlie_Huang1; ~Nan_Duan1; ~Weizhu_Chen1,"{'value': ['Large Language Models', 'Mathematical Reasoning', 'Tool Learning']}","{'value': ""Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.""}",https://openreview.net{'value': '/pdf/2b0b45b11d0f61912efb1a932fb494d36f7b88e6.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EnXJfQqy0K,{'value': 'Building Cooperative Embodied Agents Modularly with Large Language Models'},Hongxin Zhang; Weihua Du; Jiaming Shan; Qinhong Zhou; Yilun Du; Joshua B. Tenenbaum; Tianmin Shu; Chuang Gan,~Hongxin_Zhang1; ~Weihua_Du1; ~Jiaming_Shan1; ~Qinhong_Zhou1; ~Yilun_Du1; ~Joshua_B._Tenenbaum1; ~Tianmin_Shu1; ~Chuang_Gan1,"{'value': ['Large Language Models', 'Embodied Intelligence', 'Multi-Agent Cooperation', 'Human-AI Interaction', 'Communication']}","{'value': 'In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.'}",https://openreview.net{'value': '/pdf/9baa34313f1aea292b16e30697c20766afadddb1.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ElykcDu5YK,{'value': 'Meta-VBO: Utilizing Prior Tasks in Optimizing Risk Measures with Gaussian Processes'},Quoc Phong Nguyen; Bryan Kian Hsiang Low; Patrick Jaillet,~Quoc_Phong_Nguyen2; ~Bryan_Kian_Hsiang_Low1; ~Patrick_Jaillet1,"{'value': ['meta-learning', 'Bayesian optimization', 'risk measure', 'value-at-risk', 'conditional value-at-risk']}","{'value': 'Research on optimizing the risk measure of a blackbox function using Gaussian processes, especially Bayesian optimization (BO) of risk measures, has become increasingly important due to the inevitable presence of uncontrollable variables in real-world applications. Nevertheless, existing works on BO of risk measures start the optimization from scratch for every new task without considering the results of prior tasks. In contrast, its vanilla BO counterpart has received a thorough investigation on utilizing prior tasks to speed up the current task through the body of works on meta-BO which, however, have not considered risk measures. To bridge this gap, this paper presents the first algorithm for meta-BO of risk measures (i.e., value-at-risk (VaR) and the conditional VaR), namely meta-VBO, by introducing a novel adjustment to the upper confidence bound acquisition function. Our proposed algorithm exhibits two desirable properties: (i) invariance to scaling and vertical shifting of the blackbox function and (ii) robustness to prior harmful tasks. We provide a theoretical performance guarantee for our algorithm and empirically demonstrate its performance using several synthetic function benchmarks and real-world objective functions.'}",https://openreview.net{'value': '/pdf/438a0598bfd4e1457d158d87209039d77cfd4c53.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EcetCr4trp,{'value': 'Understanding Convergence and Generalization in Federated Learning through Feature Learning Theory'},Wei Huang; Ye Shi; Zhongyi Cai; Taiji Suzuki,~Wei_Huang6; ~Ye_Shi1; ~Zhongyi_Cai1; ~Taiji_Suzuki1,"{'value': ['Federated Learning', 'Feature Learning Theory', 'FedAvg']}","{'value': 'Federated Learning (FL) has attracted significant attention as an efficient privacy-preserving approach to distributed learning across multiple clients. Despite extensive empirical research and practical applications, a systematic way to theoretically understand the convergence and generalization properties in FL remains limited. This work aims to establish a unified theoretical foundation for understanding FL through feature learning theory. We focus on a scenario where each client employs a two-layer convolutional neural network (CNN) for local training on their own data. Many existing works analyze the convergence of Federated Averaging (FedAvg) under lazy training with linearizing assumptions in weight space. In contrast, our approach tracks the trajectory of signal learning and noise memorization in FL, eliminating the need for these assumptions. We further show that FedAvg can achieve near-zero test error by effectively increasing signal-to-noise ratio (SNR) in feature learning, while local training without communication achieves a large constant test error. This finding highlights the benefits of communication for generalization in FL. Moreover, our theoretical results suggest that a weighted FedAvg method, based on the similarity of input features across clients, can effectively tackle data heterogeneity issues in FL. Experimental results on both synthetic and real-world datasets verify our theoretical conclusions and emphasize the effectiveness of the weighted FedAvg approach.'}",https://openreview.net{'value': '/pdf/06cdb541958881f994ad89f992d7f9bae0d0cca7.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EPNEazJoAg,{'value': 'Exploring the cloud of feature interaction scores in a Rashomon set'},Sichao Li; Rong Wang; Quanling Deng; Amanda S Barnard,~Sichao_Li1; ~Rong_Wang3; ~Quanling_Deng1; ~Amanda_S_Barnard1,"{'value': ['Feature interaction', 'variable importance', 'Rashomon set', 'model interpretability']}","{'value': 'Interactions among features are central to understanding the behavior of machine learning models. Recent research has made significant strides in detecting and quantifying feature interactions in single predictive models. However, we argue that the feature interactions extracted from a single pre-specified model may not be trustworthy since: *a well-trained predictive model may not preserve the true feature interactions and there exist multiple well-performing predictive models that differ in feature interaction strengths*. Thus, we recommend exploring feature interaction strengths in a model class of approximately equally accurate predictive models. In this work, we introduce the feature interaction score (FIS) in the context of a Rashomon set, representing a collection of models that achieve similar accuracy on a given task. We propose a general and practical algorithm to calculate the FIS in the model class. We demonstrate the properties of the FIS via synthetic data and draw connections to other areas of statistics. Additionally, we introduce a Halo plot for visualizing the feature interaction variance in high-dimensional space and a swarm plot for analyzing FIS in a Rashomon set. Experiments with recidivism prediction and image classification illustrate how feature interactions can vary dramatically in importance for similarly accurate predictive models. Our results suggest that the proposed FIS can provide valuable insights into the nature of feature interactions in machine learning models.'}",https://openreview.net{'value': '/pdf/98ef5c378a28954f6fb388a13143645d433a52ac.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EHg5GDnyq1,{'value': 'AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors'},Weize Chen; Yusheng Su; Jingwei Zuo; Cheng Yang; Chenfei Yuan; Chi-Min Chan; Heyang Yu; Yaxi Lu; Yi-Hsin Hung; Chen Qian; Yujia Qin; Xin Cong; Ruobing Xie; Zhiyuan Liu; Maosong Sun; Jie Zhou,~Weize_Chen1; ~Yusheng_Su1; ~Jingwei_Zuo2; ~Cheng_Yang6; ~Chenfei_Yuan1; ~Chi-Min_Chan1; ~Heyang_Yu1; ~Yaxi_Lu1; ~Yi-Hsin_Hung1; ~Chen_Qian8; ~Yujia_Qin1; ~Xin_Cong1; ~Ruobing_Xie2; ~Zhiyuan_Liu1; ~Maosong_Sun1; ~Jie_Zhou8,"{'value': ['large language mode', 'agent', 'multi-agent']}","{'value': 'Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework AgentVerse that can effectively orchestrate a collaborative group of expert agents as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that AgentVerse can proficiently deploy multi-agent groups that outperform a single agent. Extensive experiments on text understanding, reasoning, coding, tool utilization, and embodied AI confirm the effectiveness of AgentVerse. Moreover, our analysis of agent interactions within AgentVerse reveals the emergence of specific collaborative behaviors, contributing to heightened group efficiency. We will release our codebase, AgentVerse, to further facilitate multi-agent research.'}",https://openreview.net{'value': '/pdf/7c9bd9a841a2ba0c11ea97abb5e982430c2fc95e.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=EG68RSznLT,{'value': 'Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation'},Zhilong Zhang; Yihao Sun; Junyin Ye; Tian-Shuo Liu; Jiaji Zhang; Yang Yu,~Zhilong_Zhang2; ~Yihao_Sun1; ~Junyin_Ye1; ~Tian-Shuo_Liu1; ~Jiaji_Zhang1; ~Yang_Yu5,"{'value': ['Preference-based Reinforcement Learning', 'Offline Reinforcement Learning', 'Conditional Generative Modeling', 'Diffusion Models']}","{'value': 'Offline preference-based reinforcement learning (PbRL) offers an effective solution to overcome the challenges associated with designing rewards and the high costs of online interactions. In offline PbRL, agents are provided with a fixed dataset containing human preferences between pairs of trajectories. Previous studies mainly focus on recovering the rewards from the preferences, followed by policy optimization with an off-the-shelf offline RL algorithm. However, given that preference label in PbRL is inherently trajectory-based, accurately learning transition-wise rewards from such label can be challenging, potentially leading to misguidance during subsequent offline RL training. To address this issue, we introduce our method named $\\textit{Flow-to-Better (FTB)}$, which leverages the pairwise preference relationship to guide a generative model in producing preferred trajectories, avoiding Temporal Difference (TD) learning with inaccurate rewards. Conditioning on a low-preference trajectory, $\\textit{FTB}$ uses a diffusion model to generate a better one with a higher preference, achieving high-fidelity full-horizon trajectory improvement. During diffusion training, we propose a technique called $\\textit{Preference Augmentation}$ to alleviate the problem of insufficient preference data. As a result, we surprisingly find that the model-generated trajectories not only exhibit increased preference and consistency with the real transition but also introduce elements of $\\textit{novelty}$ and $\\textit{diversity}$, from which we can derive a desirable policy through imitation learning. Experimental results on D4RL benchmarks demonstrate that FTB achieves a remarkable improvement compared to state-of-the-art offline PbRL methods. Furthermore, we show that FTB can also serve as an effective data augmentation method for offline RL.'}",https://openreview.net{'value': '/pdf/ad36303392c1744c9dfd19512e714d01be29d8a0.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=DuQkqSe9en,{'value': 'Adversarial Imitation Learning via Boosting'},Jonathan Daniel Chang; Dhruv Sreenivas; Yingbing Huang; Kianté Brantley; Wen Sun,~Jonathan_Daniel_Chang1; ~Dhruv_Sreenivas1; ~Yingbing_Huang1; ~Kianté_Brantley2; ~Wen_Sun1,"{'value': ['Adversarial Imitation Learning', 'Boosting', 'Reinforcement Learning']}","{'value': 'Adversarial imitation learning (AIL) has stood out as a dominant framework across various imitation learning (IL) applications, with Discriminator Actor Critic (DAC) demonstrating the effectiveness of off-policy learning algorithms in improving sample efficiency and scalability to higher-dimensional observations. Despite DAC’s empirical success, the original AIL objective is on-policy and DAC’s ad-hoc application of off-policy training does not guarantee successful imitation. Follow-up work such as ValueDICE tackles this issue by deriving a fully off-policy AIL objective. Instead in this work, we develop a novel and principled AIL algorithm via the framework of boosting. Like boosting, our new algorithm, AILBoost, maintains an ensemble of weighted weak learners (i.e., policies) and trains a discriminator that witnesses the maximum discrepancy between the distributions of the ensemble and the expert policy. We maintain a weighted replay buffer to represent the state-action distribution induced by the ensemble, allowing us to train discriminators using the entire data collected so far. Empirically, we evaluate our algorithm on both controller state-based and pixel-based environments from the DeepMind Control Suite. AILBoost outperforms DAC on both types of environments, demonstrating the benefit of properly weighting replay buffer data for off-policy training. On state-based environments, AILBoost outperforms ValueDICE and IQ-Learn, achieving state-of-the-art performance with as little as one expert trajectory.'}",https://openreview.net{'value': '/pdf/33d9d1a00b33bacdd6a00be9d8cf3889ea15ea05.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=D9rJdtmIG6,{'value': 'SpaCE: The Spatial Confounding Environment'},Mauricio Tec; Ana Trisovic; Michelle Audirac; Sophie Mirabai Woodward; Jie Kate Hu; Naeem Khoshnevis; Francesca Dominici,~Mauricio_Tec1; ~Ana_Trisovic1; ~Michelle_Audirac1; ~Sophie_Mirabai_Woodward1; ~Jie_Kate_Hu1; ~Naeem_Khoshnevis1; ~Francesca_Dominici2,"{'value': ['benchmarks', 'spatial confounding', 'graph neural networks', 'semi-synthetic data', 'causal inference']}","{'value': 'Spatial confounding poses a significant challenge in scientific studies involving spatial data, where unobserved spatial variables can influence both treatment and outcome, possibly leading to spurious associations. To address this problem, we introduce SpaCE: The Spatial Confounding Environment, the first toolkit to provide realistic benchmark datasets and tools for systematically evaluating causal inference methods designed to alleviate spatial confounding. Each dataset includes training data, true counterfactuals, a spatial graph with coordinates, and smoothness and confounding scores characterizing the effect of a missing spatial confounder. It also includes realistic semi-synthetic outcomes and counterfactuals, generated using state-of-the-art machine learning ensembles, following best practices for causal inference benchmarks. The datasets cover real treatment and covariates from diverse domains, including climate, health and social sciences. SpaCE facilitates an automated end-to-end pipeline, simplifying data loading, experimental setup, and evaluating machine learning and causal inference models. The SpaCE project provides several dozens of datasets of diverse sizes and spatial complexity. It is publicly available as a Python package, encouraging community feedback and contributions.'}",https://openreview.net{'value': '/pdf/eeea8a3c3f7c7a89a04d83b01ce69520da74f097.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=D4NJFfrqoq,{'value': 'Optimistic Bayesian Optimization with Unknown Constraints'},Quoc Phong Nguyen; Wan Theng Ruth Chew; Le Song; Bryan Kian Hsiang Low; Patrick Jaillet,~Quoc_Phong_Nguyen2; ~Wan_Theng_Ruth_Chew1; ~Le_Song3; ~Bryan_Kian_Hsiang_Low1; ~Patrick_Jaillet1,"{'value': ['Bayesian optimization', 'black box constraint', 'decoupled query']}","{'value': 'Though some research efforts have been dedicated to constrained Bayesian optimization (BO), there remains a notable absence of a principled approach with a theoretical performance guarantee in the decoupled setting. Such a setting involves independent evaluations of the objective function and constraints at different inputs, and is hence a relaxation of the commonly-studied coupled setting where functions must be evaluated together. As a result, the decoupled setting requires an adaptive selection between evaluating either the objective function or a constraint, in addition to selecting an input (in the coupled setting). This paper presents a novel constrained BO algorithm with a provable performance guarantee that can address the above relaxed setting. Specifically, it considers the fundamental trade-off between exploration and exploitation in constrained BO, and, interestingly, affords a noteworthy connection to active learning. The performance of our proposed algorithms is also empirically evaluated using several synthetic and real-world optimization problems.'}",https://openreview.net{'value': '/pdf/b9f8e9fc8f8a6b5a79bf0426ca34210e1c6552ff.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Cu5wJa5LGO,{'value': 'LLCP: Learning Latent Causal Processes for Reasoning-based Video Question Answer'},Guangyi Chen; Yuke Li; Xiao Liu; Zijian Li; Eman Al Suradi; Donglai Wei; Kun Zhang,~Guangyi_Chen1; ~Yuke_Li1; ~Xiao_Liu23; ~Zijian_Li1; ~Eman_Al_Suradi1; ~Donglai_Wei1; ~Kun_Zhang1,"{'value': ['Video Question Answer', 'Visual Reasoning', 'Causal Represetation Learning']}","{'value': 'Current approaches to Video Question Answering (VideoQA) primarily focus on cross-modality matching, which is limited by the requirement for extensive data annotations and the insufficient capacity for causal reasoning (e.g. attributing accidents). To address these challenges, we introduce a causal framework for video reasoning, termed Learning Latent Causal Processes (LLCP). At the heart of LLCP lies a multivariate generative model designed to analyze the spatial-temporal dynamics of objects within events. Leveraging the inherent modularity of causal mechanisms, we train the model through self-supervised local auto-regression eliminating the need for annotated question-answer pairs. During inference, the model is applied to answer two types of reasoning questions: accident attribution, which infers the cause from observed effects, and counterfactual prediction, which predicts the effects of counterfactual conditions given the factual evidence. In the first scenario, we identify variables that deviate from the established distribution by the learned model, signifying the root cause of accidents. In the second scenario, we replace embeddings of previous variables with counterfactual ones, enabling us to forecast potential developments. Once we have identified these cause/effect variables, natural language answers are derived through a combination of grammatical parsing and a pre-trained vision-language model. We assess the efficacy of LLCP on both synthetic and real-world data, demonstrating comparable performance to supervised methods despite our framework using no paired textual annotations.'}",https://openreview.net{'value': '/pdf/e4886678d1fb61b3be5fd3c9001c2465abbc8e48.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=CdjnzWsQax,{'value': 'Generative Learning for Financial Time Series with Irregular and Scale-Invariant Patterns'},Hongbin Huang; Minghua Chen; Xiao Qiao,~Hongbin_Huang1; ~Minghua_Chen1; ~Xiao_Qiao1,"{'value': ['generative model', 'time series pattern recognition', 'diffusion model', 'financial time series']}","{'value': 'Limited data availability poses a major obstacle in training deep learning models for financial applications. Synthesizing financial time series to augment real-world data is challenging due to the irregular and scale-invariant patterns uniquely associated with financial time series - temporal dynamics that repeat with varying duration and magnitude. Such dynamics cannot be captured by existing approaches, which often assume regularity and uniformity in the underlying data. We develop a novel generative framework called FTS-Diffusion to model irregular and scale-invariant patterns that consists of three modules. First, we develop a scale-invariant pattern recognition algorithm to extract recurring patterns that vary in duration and magnitude. Second, we construct a diffusion-based generative network to synthesize segments of patterns. Third, we model the temporal transition of patterns in order to aggregate the generated segments. Extensive experiments show that FTS-Diffusion generates synthetic financial time series highly resembling observed data, outperforming state-of-the-art alternatives. Two downstream experiments demonstrate that augmenting real-world data with synthetic data generated by FTS-Diffusion reduces the error of stock market prediction by up to 17.9%. To the best of our knowledge, this is the first work on generating intricate time series with irregular and scale-invariant patterns, addressing data limitation issues in finance.'}",https://openreview.net{'value': '/pdf/afa4bb323e04cbec65604b1a8df0f2eebc2962f3.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=BqEvdOS1Hs,{'value': 'Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain'},Yiming Gao; Feiyu Liu; Liang Wang; Dehua Zheng; Zhenjie Lian; Weixuan Wang; Wenjin Yang; Siqin Li; Xianliang Wang; Wenhui Chen; Jing Dai; QIANG FU; Yang Wei; Lanxiao Huang; Wei Liu,~Yiming_Gao4; ~Feiyu_Liu1; ~Liang_Wang10; ~Dehua_Zheng1; ~Zhenjie_Lian1; ~Weixuan_Wang1; ~Wenjin_Yang2; ~Siqin_Li1; ~Xianliang_Wang1; ~Wenhui_Chen1; ~Jing_Dai2; ~QIANG_FU8; ~Yang_Wei2; ~Lanxiao_Huang1; ~Wei_Liu3,"{'value': ['human enhancement', 'human-agent collaboration', 'game playing', 'deep reinforcement learning']}","{'value': 'Existing game AI research mainly focuses on enhancing agents\' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading to poor experiences for their human partners. In other words, most game AI agents are modeled in a ""self-centered"" manner. In this paper, we propose a ""human-centered"" modeling scheme for collaborative agents that aims to enhance the experience of humans. Specifically, we model the experience of humans as the goals they expect to achieve during the task. We expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents\' original abilities (e.g., winning games). To achieve this, we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG approach introduces a ""baseline"", which corresponds to the extent to which humans primitively achieve their goals, and encourages agents to learn behaviors that can effectively enhance humans in achieving their goals better. We evaluate the RLHG agent in the popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both objective performance and subjective preference results show that the RLHG agent provides participants better gaming experience.'}",https://openreview.net{'value': '/pdf/6ce678ca6108e4a85517e2526c83d7d968f7108b.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=Bo6GpQ3B9a,{'value': 'Out-Of-Domain Unlabeled Data Improves Generalization'},seyed amir hossein saberi; Amir Najafi; Alireza Heidari; Mohammad Hosein Movasaghinia; Abolfazl Motahari; Babak Khalaj,~seyed_amir_hossein_saberi1; ~Amir_Najafi1; ~Alireza_Heidari2; ~Mohammad_Hosein_Movasaghinia1; ~Abolfazl_Motahari1; ~Babak_Khalaj1,"{'value': ['Out-of-domain data', 'Semi-supervised learing', 'learning theory', 'generalization bound', 'adversarial robustness']}","{'value': 'We propose a novel framework for incorporating unlabeled data into semi-supervised classification problems, where scenarios involving the minimization of either i) adversarially robust or ii) non-robust loss functions have been considered. Notably, we allow the unlabeled samples to deviate slightly (in total variation sense) from the in-domain distribution. The core idea behind our framework is to combine Distributionally Robust Optimization (DRO) with self-supervised training. As a result, we also leverage efficient polynomial-time algorithms for the training stage. From a theoretical standpoint, we apply our framework on the classification problem of a mixture of two Gaussians in $\\mathbb{R}^d$, where in addition to the $m$ independent and labeled samples from the true distribution, a set of $n$ (usually with $n\\gg m$) out of domain and unlabeled samples are gievn as well. Using only the labeled data, it is known that the generalization error can be bounded by $\\propto\\left(d/m\\right)^{1/2}$. However, using our method on both isotropic and non-isotropic Gaussian mixture models, one can derive a new set of analytically explicit and non-asymptotic bounds which show substantial improvement on the generalization error compared ERM. Our results underscore two significant insights: 1) out-of-domain samples, even when unlabeled, can be harnessed to narrow the generalization gap, provided that the true data distribution adheres to a form of the ""cluster assumption"", and 2) the semi-supervised learning paradigm can be regarded as a special case of our framework when there are no distributional shifts. We validate our claims through experiments conducted on a variety of synthetic and real-world datasets.'}",https://openreview.net{'value': '/pdf/db772b639656e2c5123c187ceacfb10805558c17.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=BXY6fe7q31,{'value': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions'},Juncheng Li; Kaihang Pan; Zhiqi Ge; Minghe Gao; Wei Ji; Wenqiao Zhang; Tat-Seng Chua; Siliang Tang; Hanwang Zhang; Yueting Zhuang,~Juncheng_Li3; ~Kaihang_Pan1; ~Zhiqi_Ge1; ~Minghe_Gao1; ~Wei_Ji1; ~Wenqiao_Zhang1; ~Tat-Seng_Chua2; ~Siliang_Tang1; ~Hanwang_Zhang3; ~Yueting_Zhuang1,"{'value': ['Multimodal Large Language Models', 'Demonstrative Instruction']}","{'value': 'Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs’ underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. The code and models are available at https://github.com/DCDmllm/Cheetah.'}",https://openreview.net{'value': '/pdf/2c2260fc62d6a180e12e943725968e430205fe0a.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=AcSChDWL6V,{'value': 'Distinguished In Uniform: Self-Attention Vs. Virtual Nodes'},Eran Rosenbluth; Jan Tönshoff; Martin Ritzert; Berke Kisin; Martin Grohe,~Eran_Rosenbluth1; ~Jan_Tönshoff1; ~Martin_Ritzert1; ~Berke_Kisin1; ~Martin_Grohe1,"{'value': ['Graph Neural Networks', 'Message Passing', 'Graph Transformers', 'Virtual Nodes', 'Expressivity', 'Uniform Expressivity']}","{'value': 'Graph Transformers (GTs) such as SAN and GPS are graph processing models that combine Message-Passing GNNs (MPGNNs) with global Self-Attention. They were shown to be universal function approximators, with two reservations: 1. The initial node features must be augmented with certain positional encodings. 2. The approximation is non-uniform: Graphs of different sizes may require a different approximating network.\n\nWe first clarify that this form of universality is not unique to GTs: Using the same positional encodings, also pure MPGNNs and even 2-layer MLPs are non-uniform universal approximators. We then consider uniform expressivity: The target function is to be approximated by a single network for graphs of all sizes. There, we compare GTs to the more efficient MPGNN + Virtual Node architecture. The essential difference between the two model definitions is in their global computation method: Self-Attention Vs Virtual Node. We prove that none of the models is a uniform-universal approximator, before proving our main result: Neither model’s uniform expressivity subsumes the other’s. We demonstrate the theory with experiments on synthetic data. We further augment our study with real-world datasets, observing mixed results which indicate no clear ranking in practice as well.'}",https://openreview.net{'value': '/pdf/e1ac40eba4c49945d38feedf14666ad5b8b2974c.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=AJBkfwXh3u,{'value': 'Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks'},Kesen Zhao; Liang Zhang,~Kesen_Zhao1; ~Liang_Zhang17,"{'value': ['Dynamic Graph', 'Graph Explanation', 'Graph Neural Network', 'Causal Inference']}","{'value': 'Dynamic Graph Neural Networks (DyGNNs) have gained significant popularity in the research of dynamic graphs, but are limited by the low transparency, such that human-understandable insights can hardly be drawn from their predictions. Although a number of existing research have been devoted to investigating the interpretability of graph neural networks (GNNs), achieving the interpretability of DyGNNs is pivotally challenging due to the complex spatial-temporal correlations in dynamic graphs. To this end, we propose an innovative causality-inspired generative model based on structural causal model (SCM), which explores the underlying philosophies of DyGNN predictions by identifying the trivial, static, and dynamic causal relationships. To reach this goal, two critical tasks need to be accomplished including (1) disentangling the complex causal relationships, and (2) fitting the spatial-temporal explanations of DyGNNs in the SCM architecture. To tackle these challenges, the proposed method incorporates a contrastive learning module to disentangle trivial and causal relationships, and a dynamic correlating module to disentangle dynamic and static causal relationships, respectively. A dynamic VGAE-based framework is further developed, which generates causal-and-dynamic masks for spatial interpretability, and recognizes dynamic relationships along the time horizon through causal invention for temporal interpretability. Comprehensive experiments have been conducted on both synthetic and real-world datasets, where our approach yields substantial improvements, thereby demonstrating significant superiority.'}",https://openreview.net{'value': '/pdf/da061144568aad672fd37e90edc5b87a39b91ba9.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=ADSxCpCu9s,{'value': 'LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents'},Jae-Woo Choi; Youngwoo Yoon; Hyobin Ong; Jaehong Kim; Minsu Jang,~Jae-Woo_Choi1; ~Youngwoo_Yoon1; ~Hyobin_Ong1; ~Jaehong_Kim3; ~Minsu_Jang1,"{'value': ['task planning', 'language models', 'benchmarking', 'embodied agents', 'home robots']}","{'value': 'Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.'}",https://openreview.net{'value': '/pdf/35abce446ca9b6e7a9136f7c38556084c63538ec.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=A0HKeKl4Nl,{'value': 'Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks'},Samyak Jain; Robert Kirk; Ekdeep Singh Lubana; Robert P. Dick; Hidenori Tanaka; Tim Rocktäschel; Edward Grefenstette; David Krueger,~Samyak_Jain1; ~Robert_Kirk1; ~Ekdeep_Singh_Lubana1; ~Robert_P._Dick1; ~Hidenori_Tanaka1; ~Tim_Rocktäschel1; ~Edward_Grefenstette1; ~David_Krueger1,"{'value': ['Fine-Tuning', 'Interpretability', 'Mechanisms']}","{'value': ""Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters the underlying capabilities learned by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in synthetic, controlled settings where we can use mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an extensive analysis of the effects of fine-tuning in these settings, and show that: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a `wrapper', is typically learned on top of the underlying model capabilities, creating the illusion that they have been modified; and (iii) further fine-tuning on a task where such ``wrapped capabilities'' are relevant leads to sample-efficient revival of the capability, i.e., the model begins reusing these capabilities after only a few gradient steps. This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task. We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a more realistic setup.""}",https://openreview.net{'value': '/pdf/b46dce52717c9ba5bb6dc047b34dd04064101c1d.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=9j1RD9LlWH,{'value': 'Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data'},Yongsheng Mei; Mahdi Imani; Tian Lan,~Yongsheng_Mei1; ~Mahdi_Imani3; ~Tian_Lan4,"{'value': ['Bayesian optimization', 'Gaussian Cox process']}","{'value': 'Bayesian optimization (BO) has established itself as a leading strategy for efficiently optimizing expensive-to-evaluate functions. Existing BO methods mostly rely on Gaussian process (GP) surrogate models and are not applicable to (doubly-stochastic) Gaussian Cox processes, where the observation process is modulated by a latent intensity function modeled as a GP. In this paper, we propose a novel maximum *a posteriori* inference of Gaussian Cox processes. It leverages the Laplace approximation and change of kernel technique to transform the problem into a new reproducing kernel Hilbert space, where it becomes more tractable computationally. It enables us to obtain both a functional posterior of the latent intensity function and the covariance of the posterior, thus extending existing works that often focus on specific link functions or estimating the posterior mean. Using the result, we propose a BO framework based on the Gaussian Cox process model and further develop a Nyström approximation for efficient computation. Extensive evaluations on various synthetic and real-world datasets demonstrate significant improvement over state-of-the-art inference solutions for Gaussian Cox processes, as well as effective BO with a wide range of acquisition functions designed through the underlying Gaussian Cox process model.'}",https://openreview.net{'value': '/pdf/4ffbca1e4135f360cd7a821d180b18971c1d4025.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=9RIbNmx984,{'value': 'On Double Descent in Reinforcement Learning with LSTD and Random Features'},David Brellmann; Eloïse Berthier; David Filliat; Goran Frehse,~David_Brellmann1; ~Eloïse_Berthier1; ~David_Filliat1; ~Goran_Frehse1,"{'value': ['Regularized Least-Square Temporal Difference', 'double descent', 'over-parameterization', 'random features']}","{'value': 'Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive deterministic limits of both the empirical and the true Mean-Squared Bellman Error (MSBE) that feature correction terms responsible for the double descent. Correction terms vanish when the $l_2$-regularization is increased or the number of unvisited states goes to zero. Numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.'}",https://openreview.net{'value': '/pdf/d5505b5e401dbac544e0b1e9458e4101128c2b33.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=9JQtrumvg8,"{'value': 'A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis'}",Izzeddin Gur; Hiroki Furuta; Austin V Huang; Mustafa Safdari; Yutaka Matsuo; Douglas Eck; Aleksandra Faust,~Izzeddin_Gur1; ~Hiroki_Furuta1; ~Austin_V_Huang1; ~Mustafa_Safdari1; ~Yutaka_Matsuo1; ~Douglas_Eck1; ~Aleksandra_Faust1,"{'value': ['Web Navigation', 'Web Automation', 'Large Language Models', 'Language Model Agents', 'Tool Use', 'Program Synthesis']}","{'value': 'Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.\nHowever, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.\nWe introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.\nWebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.\nWe design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.\nWe empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.'}",https://openreview.net{'value': '/pdf/0b27823f96e3efd0ed6921aafc4fe4643d1aeec5.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=8p3fu56lKc,{'value': 'One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention'},Arvind V. Mahankali; Tatsunori Hashimoto; Tengyu Ma,~Arvind_V._Mahankali1; ~Tatsunori_Hashimoto1; ~Tengyu_Ma1,"{'value': ['Linear Self-Attention', 'In-context learning', 'Gradient Descent', 'Theoretical Understanding']}","{'value': 'Recent works have empirically analyzed in-context learning and shown that transformers trained on synthetic linear regression tasks can learn to implement ridge regression, which is the Bayes-optimal predictor, given sufficient capacity (Akyurek et al., 2023), while one-layer transformers with linear self-attention and no MLP layer will learn to implement one step of gradient descent (GD) on a least-squares linear regression objective (von Oswald et al., 2022). However, the theory behind these observations remains poorly understood. We theoretically study transformers with a single layer of linear self-attention, trained on synthetic noisy linear regression data. First, we mathematically show that when the covariates are drawn from a standard Gaussian distribution, the one-layer transformer which minimizes the pre-training loss will implement a single step of GD on the least-squares linear regression objective. Then, we find that changing the distribution of the covariates and weight vector to a non-isotropic Gaussian distribution has a strong impact on the learned algorithm: the global minimizer of the pre-training loss now implements a single step of $\\textit{pre-conditioned}$ GD. However, if only the distribution of the responses is changed, then this does not have a large effect on the learned algorithm: even when the response comes from a more general family of $\\textit{nonlinear}$ functions, the global minimizer of the pre-training loss still implements a single step of GD on a least-squares linear regression objective.'}",https://openreview.net{'value': '/pdf/c56e892fe43263e21b243db50afac6ff6b53fe4d.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=8g26Yv1EOu,{'value': 'Amortized Network Intervention to Steer the Excitatory Point Processes'},Zitao Song; Wendi Ren; Shuang Li,~Zitao_Song1; ~Wendi_Ren1; ~Shuang_Li3,"{'value': ['Time series application', 'Point Process', 'Amortized Learning']}","{'value': 'Excitatory point processes (i.e., event flows) occurring over dynamic graphs (i.e., evolving topologies) provide a fine-grained model to capture how discrete events may spread over time and space. How to effectively steer the event flows by modifying the dynamic graph structures presents an interesting problem, motivated by curbing the spread of infectious diseases through strategically locking down cities to mitigating traffic congestion via traffic light optimization. To address the intricacies of planning and overcome the high dimensionality inherent to such decision-making problems, we design an Amortized Network Interventions (ANI) framework, allowing for the pooling of optimal policies from history and other contexts while ensuring a permutation equivalent property. This property enables efficient knowledge transfer and sharing across diverse contexts. Each task is solved by an H-step lookahead model-based reinforcement learning, where neural ODEs are introduced to model the dynamics of the excitatory point processes. Instead of simulating rollouts from the dynamics model, we derive an analytical mean-field approximation for the event flows given the dynamics, making the online planning more efficiently solvable. We empirically illustrate that this ANI approach substantially enhances policy learning for unseen dynamics and exhibits promising outcomes in steering event flows through network intervention using synthetic and real COVID datasets.'}",https://openreview.net{'value': '/pdf/dcb3d9792c19cdee0046707146a520aa8b24bd53.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=8HCARN2hhw,{'value': 'Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction'},Guillaume Bono; Leonid Antsfeld; Assem Sadek; Gianluca Monaci; Christian Wolf,~Guillaume_Bono1; ~Leonid_Antsfeld1; ~Assem_Sadek1; ~Gianluca_Monaci1; ~Christian_Wolf5,"{'value': ['Navigation', 'Embodied AI', 'Perception']}","{'value': 'Agents navigating in 3D environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly defined interpretation, whereas classical robotics addresses this with scene reconstruction resulting in some form of map, usually estimated with geometry and sensor models and/or learning. In this work we propose to learn an actionable representation of the scene independently of the targeted downstream task and without explicitly optimizing reconstruction. The learned representation is optimized by a blind auxiliary agent trained to navigate with it on multiple short sub episodes branching out from a waypoint and, most importantly, without any direct visual observation. We argue and show that the blindness property is important and forces the (trained) latent representation to be the only means for planning. With probing experiments we show that the learned representation optimizes navigability and not reconstruction. On downstream tasks we show that it is robust to changes in distribution, in particular the sim2real gap, which we evaluate with a real physical robot in a real office building, significantly improving performance.'}",https://openreview.net{'value': '/pdf/2b5b56f45f498f5877f3de2ea54e150352e0a408.pdf'},{'abstract_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=8F6bws5JBy,{'value': 'Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication'},June Yong Yang; Geondo Park; Joowon Kim; Hyeongwon Jang; Eunho Yang,~June_Yong_Yang1; ~Geondo_Park1; ~Joowon_Kim1; ~Hyeongwon_Jang1; ~Eunho_Yang1,"{'value': ['Tabular data', 'imbalanced learning', 'language models']}","{'value': 'Tabular data in the wild are frequently afflicted with class-imbalance, biasing machine learning model predictions towards major classes. A data-centric solution to this problem is oversampling - where the classes are balanced by adding synthetic minority samples via generative methods. However, although tabular generative models are capable of generating synthetic samples under a balanced distribution, their integrity suffers when the number of minority samples is low. To this end, pre-trained generative language models with rich prior knowledge are a fitting candidate for the task at hand. Nevertheless, an oversampling strategy tailored for tabular data that utilizes the extensive capabilities of such language models is yet to emerge. In this paper, we propose a novel oversampling framework for tabular data to channel the abilities of generative language models. By leveraging its conditional sampling capabilities, we synthesize minority samples by progressively masking the important features of the majority class samples and imputing them towards the minority distribution. To reduce the inclusion of imperfectly converted samples, we utilize the power of the language model itself to self-authenticate the labels of the samples generated by itself, sifting out ill-converted samples. Extensive experiments on a variety of datasets and imbalance ratios reveal that the proposed method successfully generates reliable minority samples to boost the performance of machine learning classifiers, even under heavy imbalance ratios.'}",https://openreview.net{'value': '/pdf/a26f38f14c8805d676e8192ad38267dc304ffabe.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=7zY781bMDO,{'value': 'Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning'},Zhaoyi Zhou; Chuning Zhu; Runlong Zhou; Qiwen Cui; Abhishek Gupta; Simon Shaolei Du,~Zhaoyi_Zhou1; ~Chuning_Zhu1; ~Runlong_Zhou1; ~Qiwen_Cui1; ~Abhishek_Gupta1; ~Simon_Shaolei_Du1,"{'value': ['Offline Reinforcement Learning', 'Return-Conditioned Supervised Learning', 'Bellman Completeness', 'Trajectory Stitching']}","{'value': 'Off-policy dynamic programming (DP) techniques such as $Q$-learning have proven to be important in sequential decision-making problems. In the presence of function approximation, however, these techniques often diverge due to the absence of Bellman completeness in the function classes considered, a crucial condition for the success of DP-based methods. In this paper, we show how off-policy learning techniques based on return-conditioned supervised learning (RCSL) are able to circumvent these challenges of Bellman completeness, converging under significantly more relaxed assumptions inherited from supervised learning. We prove there exists a natural environment in which if one uses two-layer multilayer perceptron as the function approximator, the layer width needs to grow *linearly* with the state space size to satisfy Bellman completeness while a constant layer width is enough for RCSL. These findings take a step towards explaining the superior empirical performance of RCSL methods compared to DP-based methods in environments with near-optimal datasets. Furthermore, in order to learn from sub-optimal datasets, we propose a simple framework called MBRCSL, granting RCSL methods the ability of dynamic programming to stitch together segments from distinct trajectories. MBRCSL leverages learned dynamics models and forward sampling to accomplish trajectory stitching while avoiding the need for Bellman completeness that plagues all dynamic programming algorithms. We propose both theoretical analysis and experimental evaluation to back these claims, outperforming state-of-the-art model-free and model-based offline RL algorithms across several simulated robotics problems.'}",https://openreview.net{'value': '/pdf/92f088b50f95913a055282fd19d3ad41911fe50f.pdf'},{'title_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=7vVWiCrFnd,{'value': 'Rethinking and Extending the Probabilistic Inference Capacity of GNNs'},Tuo Xu; Lei Zou,~Tuo_Xu1; ~Lei_Zou2,"{'value': ['graph neural networks', 'expressiveness', 'approximate inference']}","{'value': ""Designing expressive Graph Neural Networks (GNNs) is an important topic in graph machine learning fields. Despite the existence of numerous approaches proposed to enhance GNNs based on Weisfeiler-Lehman (WL) tests, what GNNs can and cannot learn still lacks a deeper understanding. This paper adopts a fundamentally different approach to examine the expressive power of GNNs from a probabilistic perspective. By establishing connections between GNNs' predictions and the central inference problems of probabilistic graphical models (PGMs), we can analyze previous GNN variants with a novel hierarchical framework and gain new insights into their node-level and link-level behaviors. Additionally, we introduce novel methods that can provably enhance GNNs' ability to capture complex dependencies and make complex predictions. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approaches.""}",https://openreview.net{'value': '/pdf/20a211772e90fa923b10b62f56acff93f0b25cef.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=7VPTUWkiDQ,{'value': 'Provable Compositional Generalization for Object-Centric Learning'},Thaddäus Wiedemer; Jack Brady; Alexander Panfilov; Attila Juhos; Matthias Bethge; Wieland Brendel,~Thaddäus_Wiedemer1; ~Jack_Brady1; ~Alexander_Panfilov1; ~Attila_Juhos1; ~Matthias_Bethge1; ~Wieland_Brendel1,"{'value': ['compositional generalization', 'identifiability', 'object-centric learning', 'generalization', 'OOD generalization', 'unsupervised learning', 'slot attention', 'disentanglement', 'autoencoders', 'representation learning']}","{'value': 'Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical or empirical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data.'}",https://openreview.net{'value': '/pdf/70cd6e52cd58ee0e0b07dfea409db6acc228b343.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=7M0EzjugaN,{'value': 'Online Continual Learning for Interactive Instruction Following Agents'},Byeonghwi Kim; Minhyuk Seo; Jonghyun Choi,~Byeonghwi_Kim1; ~Minhyuk_Seo1; ~Jonghyun_Choi1,"{'value': ['Embodied AI', 'Continual Learning']}","{'value': 'In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic, since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous ‘data prior’ based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed challenging Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior arts in our empirical validations by noticeable margins.'}",https://openreview.net{'value': '/pdf/9bd1847dd40bdb80227f6ddd6480e1eee45a4999.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=7ERQPyR2eb,{'value': 'Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis'},Zhenhui Ye; Tianyun Zhong; Yi Ren; Jiaqi Yang; Weichuang Li; Jiawei Huang; Ziyue Jiang; Jinzheng He; Rongjie Huang; Jinglin Liu; Chen Zhang; Xiang Yin; Zejun MA; Zhou Zhao,~Zhenhui_Ye1; ~Tianyun_Zhong3; ~Yi_Ren2; ~Jiaqi_Yang8; ~Weichuang_Li1; ~Jiawei_Huang5; ~Ziyue_Jiang1; ~Jinzheng_He1; ~Rongjie_Huang1; ~Jinglin_Liu1; ~Chen_Zhang3; ~Xiang_Yin2; ~Zejun_MA1; ~Zhou_Zhao3,"{'value': ['One-shot Talking Face Generation', 'Neural Radiance Field']}","{'value': 'One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable talking face animation. Besides, while the existing works mainly focus on synthesizing the head part, it is also vital to generate natural torso and background segments to obtain a realistic talking portrait video. To address these limitations, we present Real3D-Potrait, a framework that (1) improves the one-shot 3D reconstruction power with a large image-to-plane model that distills 3D prior knowledge from a 3D face generative model; (2) facilitates accurate motion-conditioned animation with an efficient motion adapter; (3) synthesizes realistic video with natural torso movement and switchable background using a head-torso-background super-resolution model; and (4) supports one-shot audio-driven talking face generation with a generalizable audio-to-motion model. Extensive experiments show that Real3D-Portrait generalizes well to unseen identities and generates more realistic talking portrait videos compared to previous methods. Video samples are available at https://real3dportrait.github.io.'}",https://openreview.net{'value': '/pdf/c59a7b868258573cafe7e9a84243e3f096008b51.pdf'},{'title_filter': 'Data Synthesis'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=78iGZdqxYY,{'value': 'Mirage: Model-agnostic Graph Distillation for Graph Classification'},Mridul Gupta; Sahil Manchanda; HARIPRASAD KODAMANA; Sayan Ranu,~Mridul_Gupta2; ~Sahil_Manchanda1; ~HARIPRASAD_KODAMANA1; ~Sayan_Ranu2,"{'value': ['graph distillation', 'graph classification', 'frequent pattern mining']}","{'value': 'GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called MIRAGE for graph classification. MIRAGE is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation trees. Furthermore, the frequency distribution of computation trees is often skewed in nature, enabling us to condense this data into a concise distilled summary. By compressing the computation data itself, as opposed to emulating gradient flows on the original training set—a prevalent approach to date—MIRAGE transforms into an unsupervised and architecture-agnostic distillation algorithm. Extensive benchmarking on real-world datasets underscores MIRAGE’s superiority, showcasing enhanced generalization accuracy, data compression, and distillation efficiency when compared to state-of-the-art baselines.'}",https://openreview.net{'value': '/pdf/6734ad0e165c7017f3c6c0f400183a1bae684654.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=70IgE3tRbu,{'value': 'Continuous Invariance Learning'},LIN Yong; Fan Zhou; Lu Tan; Lintao Ma; Jianmeng Liu; Yansu HE; Yuan Yuan; Yu Liu; James Y. Zhang; Yujiu Yang; Hao Wang,~LIN_Yong1; ~Fan_Zhou10; ~Lu_Tan1; ~Lintao_Ma1; ~Jianmeng_Liu1; ~Yansu_HE1; ~Yuan_Yuan5; ~Yu_Liu28; ~James_Y._Zhang1; ~Yujiu_Yang2; ~Hao_Wang3,{'value': ['Causality; Domain Generalization; Invariance Learning']},"{'value': ""Invariance learning methods aim to learn invariant features in the hope that they generalize under distributional shift. Although many tasks are naturally characterized by continuous domains, current invariance learning techniques generally assume categorically indexed domains. For example, auto-scaling in cloud computing often needs a CPU utilization prediction model that generalizes across different times (e.g., time of a day and date of a year), where `time' is a continuous domain index. In this paper, we start by theoretically showing that existing invariance learning methods can fail for continuous domain problems. Specifically, the naive solution of splitting continuous domains into discrete ones ignores the underlying relationship among domains, and therefore potentially leads to suboptimal performance. To address this challenge, we then propose Continuous Invariance Learning (CIL), which extracts invariant features across continuously indexed domains. CIL is a novel adversarial procedure which measures and controls the conditional independence between the labels and continuous domain indices given the extracted features. Our theoretical analysis demonstrates that CIL learns features that satisfy the invariant constraint with infinite samples. Empirical results on both synthetic and real-world datasets (including data collected from production systems) show that CIL consistently outperforms strong baselines among all the tasks.""}",https://openreview.net{'value': '/pdf/a60576de00dc34e15ed15f5b0bb1ebce91045d11.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=6N8TW504aa,{'value': 'Graphical Multioutput Gaussian Process with Attention'},Yijue Dai; Wenzhong Yan; Feng Yin,~Yijue_Dai1; ~Wenzhong_Yan1; ~Feng_Yin1,"{'value': ['Gaussian process regression', 'Multioutput Gaussian process', 'Attention mechanism']}","{'value': 'Integrating information while recognizing dependence from multiple data sources and enhancing the predictive performance of the multi-output regression are challenging tasks. Multioutput Gaussian Process (MOGP) methods offer outstanding solutions with tractable predictions and uncertainty quantification. However, their practical applications are hindered by high computational complexity and storage demand. Additionally, there exist model mismatches in existing MOGP models when dealing with non-Gaussian data. To improve the model representation ability in terms of flexibility, optimality, and scalability, this paper introduces a novel multi-output regression framework, termed Graphical MOGP (GMOGP), which is empowered by: (i) Generating flexible Gaussian process priors consolidated from dentified parents, (ii) providing dependent processes with attention-based graphical representations, and (iii) achieving Pareto optimal solutions of kernel hyperparameters via a distributed learning framework. Numerical results confirm that the proposed GMOGP significantly outperforms state-of-the-art MOGP alternatives in predictive performance, as well as in time and memory efficiency, across various synthetic and real datasets.'}",https://openreview.net{'value': '/pdf/0b6ac06d4a4184388fc33af01e76741a7603c341.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=5EniAcsO7f,{'value': 'Weatherproofing Retrieval for Localization with Generative AI and Geometric Consistency'},Yannis Kalantidis; Mert Bülent Sarıyıldız; Rafael S. Rezende; Philippe Weinzaepfel; Diane Larlus; Gabriela Csurka,~Yannis_Kalantidis2; ~Mert_Bülent_Sarıyıldız1; ~Rafael_S._Rezende1; ~Philippe_Weinzaepfel1; ~Diane_Larlus1; ~Gabriela_Csurka2,"{'value': ['visual localization', 'image retrieval', 'synthetic data', 'domain shift', 'geometric consistency', 'long-term visual localization', 'ret4loc', 'image alteration']}","{'value': 'State-of-the-art visual localization approaches generally rely on a first image retrieval step whose role is crucial. Yet, retrieval often struggles when facing varying conditions, due to e.g. weather or time of day, with dramatic consequences on the visual localization accuracy. In this paper, we improve this retrieval step and tailor it to the final localization task. Among the several changes we advocate for, we propose to synthesize variants of the training set images, obtained from generative text-to-image models, in order to automatically expand the training set towards a number of nameable variations that particularly hurt visual localization. After expanding the training set, we propose a training approach that leverages the specificities and the underlying geometry of this mix of real and synthetic images. We experimentally show that those changes translate into large improvements for the most challenging visual localization datasets.'}",https://openreview.net{'value': '/pdf/ce9b20038af890c0d8df5a681c61f3851e4466aa.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=52fz5sUAy2,{'value': 'Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference'},Haoxuan Li; Chunyuan Zheng; Sihao Ding; Peng Wu; Zhi Geng; Fuli Feng; Xiangnan He,~Haoxuan_Li6; ~Chunyuan_Zheng1; ~Sihao_Ding2; ~Peng_Wu5; ~Zhi_Geng1; ~Fuli_Feng1; ~Xiangnan_He1,"{'value': ['Selection Bias', 'Neighborhood effect', 'Recommender system']}","{'value': 'Selection bias in recommender system arises from the recommendation process of system filtering and the interactive process of user selection. Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but ignore the fact that potential outcomes for a given user-item pair may vary with the treatments assigned to other user-item pairs, named neighborhood effect. To fill the gap, this paper formally formulates the neighborhood effect as an interference problem from the perspective of causal inference, and introduces a treatment representation to capture the neighborhood effect. On this basis, we propose a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood effect. We further develop two new estimators for estimating the proposed ideal loss. We theoretically establish the connection between the proposed and previous debiasing methods ignoring the neighborhood effect, showing that the proposed methods can achieve unbiased learning when both selection bias and neighborhood effects are present, while the existing methods are biased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed methods.'}",https://openreview.net{'value': '/pdf/9205f9cf9861ea57ce78d3007b54e1bcec2e5df6.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=4Ay23yeuz0,{'value': 'Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space'},Hengrui Zhang; Jiani Zhang; Zhengyuan Shen; Balasubramaniam Srinivasan; Xiao Qin; Christos Faloutsos; Huzefa Rangwala; George Karypis,~Hengrui_Zhang1; ~Jiani_Zhang2; ~Zhengyuan_Shen1; ~Balasubramaniam_Srinivasan1; ~Xiao_Qin3; ~Christos_Faloutsos1; ~Huzefa_Rangwala2; ~George_Karypis1,"{'value': ['Tabular data', 'tabular generation', 'diffusion models']}","{'value': 'Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces TabSyn, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed Tabsyn include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capturing inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data; (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that Tabsyn outperforms existing methods. Specifically, it reduces the error rates by 86% and 67% for column-wise distribution and pair-wise column correlation estimations compared with the most competitive baselines. The code has been made available at https://github.com/amazon-science/tabsyn.'}",https://openreview.net{'value': '/pdf/a916d9616f8be0fc9c47c323b6afe8398acf898d.pdf'},{'title_filter': 'Data Synthesis'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=3zvB14IF6D,{'value': 'DORSal: Diffusion for Object-centric Representations of Scenes $\\textit{et al.}$'},Allan Jabri; Sjoerd van Steenkiste; Emiel Hoogeboom; Mehdi S. M. Sajjadi; Thomas Kipf,~Allan_Jabri2; ~Sjoerd_van_Steenkiste1; ~Emiel_Hoogeboom1; ~Mehdi_S._M._Sajjadi1; ~Thomas_Kipf2,"{'value': ['novel view synthesis', 'object-centric scene representations', 'camera control', 'scene editing', '3D', 'diffusion', 'generative models']}","{'value': 'Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on frozen object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View dataset, we show that DORSal enables scalable neural rendering of 3D scenes with object-level editing and improves upon existing approaches.'}",https://openreview.net{'value': '/pdf/396246ccfcbff898d518e14028ef788f763c9476.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=3xHDeA8Noi,{'value': 'Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training'},Hong Liu; Zhiyuan Li; David Leo Wright Hall; Percy Liang; Tengyu Ma,~Hong_Liu5; ~Zhiyuan_Li2; ~David_Leo_Wright_Hall1; ~Percy_Liang1; ~Tengyu_Ma1,"{'value': ['large language models', 'pretraining', 'optimization in deep learning']}","{'value': 'Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50\\% fewer steps, less total compute, and reduced wall-clock time.'}",https://openreview.net{'value': '/pdf/394bc531aa1bd41f10457a74817768d87b04566b.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=3oTPsORaDH,{'value': 'SEGNO: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases'},Yang Liu; Jiashun Cheng; Haihong Zhao; Tingyang Xu; Peilin Zhao; Fugee Tsung; Jia Li; Yu Rong,~Yang_Liu21; ~Jiashun_Cheng1; ~Haihong_Zhao2; ~Tingyang_Xu1; ~Peilin_Zhao2; ~Fugee_Tsung1; ~Jia_Li4; ~Yu_Rong1,"{'value': ['Equivariant Graph Neural Network', 'Graph Neural Network']}","{'value': 'Graph Neural Networks (GNNs) with equivariant properties have emerged as powerful tools for modeling complex dynamics of multi-object physical systems. However, their generalization ability is limited by the inadequate consideration of physical inductive biases: (1) Existing studies overlook the continuity of transitions among system states, opting to employ several discrete transformation layers to learn the direct mapping between two adjacent states; (2) Most models only account for first-order velocity information, despite the fact that many physical systems are governed by second-order motion laws. To incorporate these inductive biases, we propose the Second-order Equivariant Graph Neural Ordinary Differential Equation (SEGNO). Specifically, we show how the second-order continuity can be incorporated into GNNs while maintaining the equivariant property. Furthermore, we offer theoretical insights into SEGNO, highlighting that it can learn a unique trajectory between adjacent states, which is crucial for model generalization. Additionally, we prove that the discrepancy between this learned trajectory of SEGNO and the true trajectory is bounded. Extensive experiments on complex dynamical systems including molecular dynamics and motion capture demonstrate that our model yields a significant improvement over the state-of-the-art baselines.'}",https://openreview.net{'value': '/pdf/4acee75d595246c171ea11abc7187435b10862ea.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=3mnWvUZIXt,{'value': 'Towards Principled Representation Learning from Videos for Reinforcement Learning'},Dipendra Misra; Akanksha Saran; Tengyang Xie; Alex Lamb; John Langford,~Dipendra_Misra1; ~Akanksha_Saran1; ~Tengyang_Xie1; ~Alex_Lamb1; ~John_Langford1,"{'value': ['Reinforcement Learning', 'Representation Learning']}","{'value': 'We study pre-training representations for decision-making using video data, which is abundantly available for tasks such as game agents and software testing. Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent. We initiate the theoretical investigation into principled approaches for representation learning and focus on learning the latent state representations of the underlying MDP using video data. We study two types of settings: one where there is iid noise in the observation, and a more challenging setting where there is also the presence of exogenous noise, which is non-iid noise that is temporally correlated, such as the motion of people or cars in the background. We study three commonly used approaches: autoencoding, temporal contrastive learning, and forward modeling. We prove upper bounds for temporal contrastive learning and forward modeling in the presence of only iid noise. We show that these approaches can learn the latent state and use it to do efficient downstream RL with polynomial sample complexity. When exogenous noise is also present, we establish a lower bound result showing that the sample complexity of learning from video data can be exponentially worse than learning from action-labeled trajectory data. This partially explains why reinforcement learning with video pre-training is hard. We evaluate these representational learning methods in two visual domains, yielding results that are consistent with our theoretical findings.'}",https://openreview.net{'value': '/pdf/030d9ee1692e81df35ac143b32eb5beb1c384730.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=3cuJwmPxXj,{'value': 'Identifying Representations for Intervention Extrapolation'},Sorawit Saengkyongam; Elan Rosenfeld; Pradeep Kumar Ravikumar; Niklas Pfister; Jonas Peters,~Sorawit_Saengkyongam1; ~Elan_Rosenfeld1; ~Pradeep_Kumar_Ravikumar1; ~Niklas_Pfister1; ~Jonas_Peters2,"{'value': ['causality', 'extrapolation', 'exogenous variables', 'causal representation learning', 'identifiable representation learning', 'control functions', 'instrumental variables', 'invariance']}","{'value': 'The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods for downstream tasks are needed. In this paper, we consider the task of intervention extrapolation: predicting how interventions affect an outcome, even when those interventions are not observed at training time, and show that identifiable representations can provide an effective solution to this task even if the interventions affect the outcome non-linearly. Our setup includes an outcome variable $Y$, observed features $X$, which are generated as a non-linear transformation of latent features $Z$, and exogenous action variables $A$, which influence $Z$. The objective of intervention extrapolation is then to predict how interventions on $A$ that lie outside the training support of $A$ affect $Y$. Here, extrapolation becomes possible if the effect of $A$ on $Z$ is linear and the residual when regressing Z on A has full support. As $Z$ is latent, we combine the task of intervention extrapolation with identifiable representation learning, which we call $\\texttt{Rep4Ex}$: we aim to map the observed features $X$ into a subspace that allows for non-linear extrapolation in $A$. We show that the hidden representation is identifiable up to an affine transformation in $Z$-space, which, we prove, is sufficient for intervention extrapolation. The identifiability is characterized by a novel constraint describing the linearity assumption of $A$ on $Z$. Based on this insight, we propose a flexible method that enforces the linear invariance constraint and can be combined with any type of autoencoder. We validate our theoretical findings through a series of synthetic experiments and show that our approach can indeed succeed in predicting the effects of unseen interventions.'}",https://openreview.net{'value': '/pdf/8a74f7b89a0cd7b490dcfefbdc766bd93dab115c.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=2tVHNRZuCs,{'value': 'Enabling Lanuguage Models to Implicitly Learn Self-Improvement'},Ziqi Wang; Le Hou; Tianjian Lu; Yuexin Wu; Yunxuan Li; Hongkun Yu; Heng Ji,~Ziqi_Wang2; ~Le_Hou1; ~Tianjian_Lu1; ~Yuexin_Wu1; ~Yunxuan_Li2; ~Hongkun_Yu2; ~Heng_Ji3,{'value': ['large language models; self-improvement; alignment']},"{'value': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpfulness and less harmful). To this end, we propose an imPlicit self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models with no extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.'}",https://openreview.net{'value': '/pdf/d405f177b3c6d2fea10dfb6a0968d4431d17c75a.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=2iGiSHmeAN,{'value': 'BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics'},Suresh Bishnoi; Jayadeva Jayadeva; Sayan Ranu; N M Anoop Krishnan,~Suresh_Bishnoi1; ~Jayadeva_Jayadeva1; ~Sayan_Ranu2; ~N_M_Anoop_Krishnan1,"{'value': ['Brownian dynamics', 'stochastic differential equation', 'graph neural network', 'scientific machine learning']}","{'value': 'Neural networks (NNs) that exploit strong inductive biases based on physical laws and symmetries have shown remarkable success in learning the dynamics of physical systems directly from their trajectory. However, these works focus only on the systems that follow deterministic dynamics, such as Newtonian or Hamiltonian. Here, we propose a framework, namely Brownian graph neural networks (BroGNet), combining stochastic differential equations (SDEs) and GNNs to learn Brownian dynamics directly from the trajectory. We modify the architecture of BroGNet to enforce linear momentum conservation of the system, which, in turn, provides superior performance on learning dynamics as revealed empirically. We demonstrate this approach on several systems, namely, linear spring, linear spring with binary particle types, and non-linear spring systems, all following Brownian dynamics at finite temperatures. We show that BroGNet significantly outperforms proposed baselines across all the benchmarked Brownian systems. In addition, we demonstrate zero-shot generalizability of BroGNet to simulate unseen system sizes that are two orders of magnitude larger and to different temperatures than those used during training. Finally, we show that BroGNet conserves the momentum of the system resulting in superior performance and data efficiency. Altogether, our study contributes to advancing the understanding of the intricate dynamics of Brownian motion and demonstrates the effectiveness of graph neural networks in modeling such complex systems.'}",https://openreview.net{'value': '/pdf/7068bcb7f8feb34ebb6a585e0060e3135e592aac.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=2Q8TZWAHv4,{'value': 'GOAt: Explaining Graph Neural Networks via Graph Output Attribution'},Shengyao Lu; Keith G. Mills; Jiao He; Bang Liu; Di Niu,~Shengyao_Lu1; ~Keith_G._Mills1; ~Jiao_He1; ~Bang_Liu1; ~Di_Niu1,"{'value': ['Graph Neural Networks', 'Explainability', 'Interpretability', 'Local-level explanation', 'Instance-level explanation']}","{'value': 'Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-of-the-art GNN explainers in terms of the commonly used fidelity metric, but also exhibits stronger discriminability, and stability by a remarkable margin.'}",https://openreview.net{'value': '/pdf/0eaefa07ba231c9530e4732367c9afffe0597cba.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=22pyNMuIoa,{'value': 'PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization'},Xinyuan Wang; Chenxi Li; Zhen Wang; Fan Bai; Haotian Luo; Jiayou Zhang; Nebojsa Jojic; Eric Xing; Zhiting Hu,xiw136@ucsd.edu; chl078@ucsd.edu; ~Zhen_Wang6; ~Fan_Bai5; 1203616626@sjtu.edu.cn; ~Jiayou_Zhang1; ~Nebojsa_Jojic1; ~Eric_Xing1; ~Zhiting_Hu3,"{'value': ['Large Language Models', 'Expert-level Prompt Optimization', 'Strategic Planning']}","{'value': ""Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts.""}",https://openreview.net{'value': '/pdf/d4dbdee0d105cd3020bffdc2f56d99c33429d49c.pdf'},{'title_filter': 'Agent'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=1mNFsbvo2P,{'value': 'Domain constraints improve risk prediction when outcome data is missing'},Sidhika Balachandar; Nikhil Garg; Emma Pierson,~Sidhika_Balachandar1; ~Nikhil_Garg2; ~Emma_Pierson1,"{'value': ['Bayesian model', 'health', 'selective labels', 'distribution shift', 'domain constraint', 'biomedicine']}","{'value': ""Machine learning models are often trained to predict the outcome resulting from a human decision. For example, if a doctor decides to test a patient for disease, will the patient test positive? A challenge is that historical decision-making determines whether the outcome is observed: we only observe test outcomes for patients doctors historically tested. Untested patients, for whom outcomes are unobserved, may differ from tested patients along observed and unobserved dimensions. We propose a Bayesian model class which captures this setting. The purpose of the model is to accurately estimate risk for both tested and untested patients. Estimating this model is challenging due to the wide range of possibilities for untested patients. To address this, we propose two domain constraints which are plausible in health settings: a prevalence constraint, where the overall disease prevalence is known, and an expertise constraint, where the human decision-maker deviates from purely risk-based decision-making only along a constrained feature set. We show theoretically and on synthetic data that domain constraints improve parameter inference. We apply our model to a case study of cancer risk prediction, showing that the model's inferred risk predicts cancer diagnoses, its inferred testing policy captures known public health policies, and it can identify suboptimalities in test allocation. Though our case study is in healthcare, our analysis reveals a general class of domain constraints which can improve model estimation in many settings.""}",https://openreview.net{'value': '/pdf/b86176320b4bfa73e8eee4928ee9ff15cc4181f7.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=1hsVvgW0rU,{'value': 'Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight'},Jiacheng Guo; Minshuo Chen; Huan Wang; Caiming Xiong; Mengdi Wang; Yu Bai,~Jiacheng_Guo1; ~Minshuo_Chen1; ~Huan_Wang1; ~Caiming_Xiong1; ~Mengdi_Wang1; ~Yu_Bai1,"{'value': ['reinforcement learning theory', 'POMDPs', 'partially observable reinforcement learning']}","{'value': ""This paper studies the sample-efficiency of learning in Partially Observable Markov Decision Processes (POMDPs), a challenging problem in reinforcement learning that is known to be exponentially hard in the worst-case. Motivated by real-world settings such as loading in game playing, we propose an enhanced feedback model called ``multiple observations in hindsight'', where after each episode of interaction with the POMDP, the learner may collect multiple additional observations emitted from the encountered latent states, but may not observe the latent states themselves. We show that sample-efficient learning under this feedback model is possible for two new subclasses of POMDPs: \\emph{multi-observation revealing POMDPs} and \\emph{distinguishable POMDPs}. Both subclasses generalize and substantially relax \\emph{revealing POMDPs}---a widely studied subclass for which sample-efficient learning is possible under standard trajectory feedback. Notably, distinguishable POMDPs only require the emission distributions from different latent states to be \\emph{different} instead of \\emph{linearly independent} as required in revealing POMDPs.""}",https://openreview.net{'value': '/pdf/5f2fca010d39884681afbf62727150266e47e06f.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=1NHgmKqOzZ,{'value': 'Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality'},Xuxi Chen; Yu Yang; Zhangyang Wang; Baharan Mirzasoleiman,~Xuxi_Chen1; ~Yu_Yang4; ~Zhangyang_Wang1; ~Baharan_Mirzasoleiman1,"{'value': ['dataset distillation', 'dataset condensation']}","{'value': 'Dataset distillation aims to minimize the time and memory needed for training deep networks on large datasets, by creating a small set of synthetic images that has a similar generalization performance to that of the full dataset. However, current dataset distillation techniques fall short, showing a notable performance gap compared to training on the original data. In this work, we are the first to argue that the use of only one synthetic subset for distillation may not yield optimal generalization performance. This is because the training dynamics of deep networks drastically changes during training. Therefore, multiple synthetic subsets are required to capture the dynamics of training in different stages. To address this issue, we propose Progressive Dataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic images, each conditioned on the previous sets, and trains the model on the cumulative union of these subsets without requiring additional training time. Our extensive experiments show that PDD can effectively improve the performance of existing dataset distillation methods by up to 4.3%. In addition, our method for the first time enables generating considerably larger synthetic datasets. Our codes are available at https://github.com/VITA-Group/ProgressiveDD.'}",https://openreview.net{'value': '/pdf/9e441d8d82995f6fd1859104998e6b597e5f6bbb.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=1JbsdayvhO,{'value': 'Denoising Diffusion via Image-Based Rendering'},Titas Anciukevičius; Fabian Manhardt; Federico Tombari; Paul Henderson,~Titas_Anciukevičius1; ~Fabian_Manhardt1; ~Federico_Tombari1; ~Paul_Henderson1,"{'value': ['Neural Scene Representations', 'Generative Models', 'Denoising Diffusion', '3D Reconstruction']}","{'value': 'Generating 3D scenes is a challenging open problem, which requires synthesizing plausible content that is fully consistent in 3D space. While recent methods such as neural radiance fields excel at view synthesis and 3D reconstruction, they cannot synthesize plausible details in unobserved regions since they lack a generative capability. Conversely, existing generative methods are typically not capable of reconstructing detailed, large-scale scenes in the wild, as they use limited-capacity 3D scene representations, require aligned camera poses, or rely on additional regularizers. In this work, we introduce the first diffusion model able to perform fast, detailed reconstruction and generation of real-world 3D scenes. To achieve this, we make three contributions. First, we introduce a new neural scene representation, IB-planes, that can efficiently and accurately represent large 3D scenes, dynamically allocating more capacity as needed to capture details visible in each image. Second, we propose a denoising-diffusion framework to learn a prior over this novel 3D scene representation, using only 2D images without the need for any additional supervision signal such as masks or depths.  This supports 3D reconstruction and generation in a unified architecture. Third, we develop a principled approach to avoid trivial 3D solutions when integrating the image-based rendering with the diffusion model, by dropping out representations of some images. We evaluate the model on several challenging datasets of real and synthetic images, and demonstrate superior results on generation, novel view synthesis and 3D reconstruction.'}",https://openreview.net{'value': '/pdf/cefa1b3e50f7db4d2660331b3bcddef98f91af65.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=0t1O8ziRZp,{'value': 'Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization'},Animesh Basak Chowdhury; Marco Romanelli; Benjamin Tan; Ramesh Karri; Siddharth Garg,~Animesh_Basak_Chowdhury1; ~Marco_Romanelli1; ~Benjamin_Tan1; ~Ramesh_Karri1; ~Siddharth_Garg1,"{'value': ['Electronics Design Automation (EDA)', 'Logic Synthesis', 'Reinforcement Learning', 'Hardware design', 'Circuits']}","{'value': 'Logic synthesis, a pivotal stage in chip design, entails optimizing chip specifications encoded in hardware description languages like Verilog into highly efficient implementations using Boolean logic gates. The process involves a sequential application of logic minimization heuristics (``synthesis recipe""), with their arrangement significantly impacting crucial metrics such as area and delay. Addressing the challenge posed by the broad spectrum of hardware design complexities — from variations of past designs (e.g., adders and multipliers) to entirely novel configurations (e.g., innovative processor instructions) — requires a nuanced \'synthesis recipe\' guided by human expertise and intuition. This study conducts a thorough examination of learning and search techniques for logic synthesis, unearthing a surprising revelation: pre-trained agents, when confronted with entirely novel designs, may veer off course, detrimentally affecting the search trajectory. We present ABC-RL, a meticulously tuned $\\alpha$ parameter that adeptly adjusts recommendations from pre-trained agents during the search process. Computed based on similarity scores through nearest neighbor retrieval from the training dataset, ABC-RL yields superior synthesis recipes tailored for a wide array of hardware designs. Our findings showcase substantial enhancements in the Quality of Result (QoR) of synthesized circuits, boasting improvements of up to 24.8\\% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an impressive up to 9x reduction in runtime (iso-QoR) when compared to current state-of-the-art methodologies.'}",https://openreview.net{'value': '/pdf/45352e5a0436249e936152d04e6550dff1475ac5.pdf'},{'abstract_filter': 'Trajectory'},ICLR.cc,2024,Conference
https://openreview.net/forum?id=0jHkUDyEO9,{'value': 'Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors'},Guocheng Qian; Jinjie Mai; Abdullah Hamdi; Jian Ren; Aliaksandr Siarohin; Bing Li; Hsin-Ying Lee; Ivan Skorokhodov; Peter Wonka; Sergey Tulyakov; Bernard Ghanem,~Guocheng_Qian1; ~Jinjie_Mai1; ~Abdullah_Hamdi1; ~Jian_Ren2; ~Aliaksandr_Siarohin1; ~Bing_Li7; ~Hsin-Ying_Lee2; ~Ivan_Skorokhodov1; ~Peter_Wonka1; ~Sergey_Tulyakov1; ~Bernard_Ghanem1,"{'value': ['Neural Radiance Fields', 'Shape from Image', 'Generative 3D models']}","{'value': ""We present ``Magic123'', a two-stage coarse-to-fine approach for high-quality, textured 3D mesh generation from a single image in the wild using *both 2D and 3D priors*. In the first stage, we optimize a neural radiance field to produce a coarse geometry. In the second stage, we adopt a memory-efficient differentiable mesh representation to yield a high-resolution mesh with a visually appealing texture. In both stages, the 3D content is learned through reference-view supervision and novel-view guidance by a joint 2D and 3D diffusion prior. We introduce a trade-off parameter between the 2D and 3D priors to control the details and 3D consistencies of the generation. Magic123 demonstrates a significant improvement over previous image-to-3D techniques, as validated through extensive experiments on diverse synthetic and real-world images.""}",https://openreview.net{'value': '/pdf/053391ae7565c3fe76e1f7f83c629094b3e3262e.pdf'},{'abstract_filter': 'Synthetic'},ICLR.cc,2024,Conference
