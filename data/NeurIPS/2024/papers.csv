forum,title,authors,authorids,keywords,abstract,pdf,match,venue,year,type
https://openreview.net/forum?id=zv4UISZzp5,{'value': 'IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation'},Fan Lin; Shuyi Xie; Yong Dai; Wenlin Yao; TianJiao Lang; Yu Zhang,~Fan_Lin2; ~Shuyi_Xie1; ~Yong_Dai1; ~Wenlin_Yao1; ~TianJiao_Lang1; ~Yu_Zhang21,"{'value': ['LLM', 'Data Generalization', 'Discrimination Indexes']}","{'value': 'As Large Language Models (LLMs) become more capable of handling increasingly complex tasks, the evaluation set must keep pace with these advancements to ensure it remains sufficiently discriminative. Item Discrimination (ID) theory, which is widely used in educational assessment, measures the ability of individual test items to differentiate between high and low performers. Inspired by this theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs so that the evaluation set continually updates and refines according to model abilities. \nOur data synthesis framework prioritizes both breadth and specificity. It can generate prompts that comprehensively evaluate the capabilities of LLMs while revealing meaningful performance differences between models, allowing for effective discrimination of their relative strengths and weaknesses across various tasks and domains.\nTo produce high-quality data, we incorporate a self-correct mechanism into our generalization framework and develop two models to predict prompt discrimination and difficulty score to facilitate our data synthesis framework, contributing valuable tools to evaluation data synthesis research. We apply our generated data to evaluate five SOTA models. Our data achieves an average score of 51.92, accompanied by a variance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and WizardLM) obtain an average score exceeding 67, with a variance below 3.2.\nThe results demonstrate that the data generated by our framework is more challenging and discriminative compared to previous works.\nWe will release a dataset of over 3,000 carefully crafted prompts to facilitate evaluation research of LLMs.'}",https://openreview.net{'value': '/pdf/74ed0078ffe00fb63ba32cc447f4540054349fbb.pdf'},{'abstract_filter': 'Data Synthesis'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ziYC4FHRNr,{'value': 'Entrywise error bounds for low-rank approximations of kernel matrices'},Alexander Modell,~Alexander_Modell1,"{'value': ['low-rank approximation', 'kernel methods', 'SVD', 'theory', 'error bounds']}","{'value': 'In this paper, we derive *entrywise* error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition). While this approximation is well-known to be optimal with respect to the spectral and Frobenius norm error, little is known about the statistical behaviour of individual entries. Our error bounds fill this gap. A key technical innovation is a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues, which takes inspiration from the field of Random Matrix Theory. Finally, we validate our theory with an empirical study of a collection of synthetic and real-world datasets.'}",https://openreview.net{'value': '/pdf/b7d1213d226d72a537e0292965c1e860cf9010e7.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=zcEPOB9rCR,{'value': 'Bridging Geometric States via Geometric Diffusion Bridge'},Shengjie Luo; Yixian Xu; Di He; Shuxin Zheng; Tie-Yan Liu; Liwei Wang,~Shengjie_Luo1; ~Yixian_Xu1; ~Di_He1; ~Shuxin_Zheng1; ~Tie-Yan_Liu1; ~Liwei_Wang1,"{'value': ['Bridging Geometric States', 'Generative Modeling', 'Geometric Deep Learning', 'Diffusion Bridge', ""Doob's h-transform"", 'Equivariance']}","{'value': ""The accurate prediction of geometric state evolution in complex systems is critical for advancing scientific domains such as quantum chemistry and material modeling. Traditional experimental and computational methods face challenges in terms of environmental constraints and computational demands, while current deep learning approaches still fall short in terms of precision and generality. In this work, we introduce the Geometric Diffusion Bridge (GDB), a novel generative modeling framework that accurately bridges initial and target geometric states. GDB leverages a probabilistic approach to evolve geometric state distributions, employing an equivariant diffusion bridge derived by a modified version of Doob's $h$-transform for connecting geometric states. This tailored diffusion process is anchored by initial and target geometric states as fixed endpoints and governed by equivariant transition kernels. Moreover, trajectory data can be seamlessly leveraged in our GDB framework by using a chain of equivariant diffusion bridges, providing a more detailed and accurate characterization of evolution dynamics. Theoretically, we conduct a thorough examination to confirm our framework's ability to preserve joint distributions of geometric states and capability to completely model the underlying dynamics inducing trajectory distributions with negligible error. Experimental evaluations across various real-world scenarios show that GDB surpasses existing state-of-the-art approaches, opening up a new pathway for accurately bridging geometric states and tackling crucial scientific challenges with improved accuracy and applicability.""}",https://openreview.net{'value': '/pdf/f266817fbebd23596b1820451d7726da8a9cd128.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=zV2GDsZb5a,{'value': 'Neural Gaffer: Relighting Any Object via Diffusion'},Haian Jin; Yuan Li; Fujun Luan; Yuanbo Xiangli; Sai Bi; Kai Zhang; Zexiang Xu; Jin Sun; Noah Snavely,~Haian_Jin1; ~Yuan_Li13; ~Fujun_Luan2; ~Yuanbo_Xiangli1; ~Sai_Bi1; ~Kai_Zhang7; ~Zexiang_Xu1; ~Jin_Sun2; ~Noah_Snavely1,"{'value': ['relighting', 'single-image relighting', 'diffusion models', '3D relighting.']}","{'value': 'Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting. Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight. Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive. In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition. Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model. We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy. Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion. Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field.'}",https://openreview.net{'value': '/pdf/71a526cee2abe808ec8027770fd2ee1ce6e3a7fc.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=zLU21oQjD5,{'value': 'DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving'},Yuxuan Tong; Xiwen Zhang; Rui Wang; Ruidong Wu; Junxian He,~Yuxuan_Tong2; ~Xiwen_Zhang2; ~Rui_Wang1; ~Ruidong_Wu1; ~Junxian_He1,"{'value': ['Large Language Models', 'Mathematical Reasoning', 'Synthetic Data']}","{'value': 'Solving mathematical problems requires advanced reasoning abilities and presents notable challenges for large language models. Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results. However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries.\nHypothesizing that difficult queries are crucial to learning complex reasoning, we propose *Difficulty-Aware Rejection Tuning* (`DART`), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples.\nUtilizing `DART`, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones. Remarkably, our synthesis process solely relies on a 7B-sized open-weight model, without reliance on the commonly used proprietary GPT-4.\nWe fine-tune various base models on our datasets ranging from 7B to 70B in size, resulting in a series of strong models called `DART-Math`.\nIn comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, `DART-Math` outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models. Furthermore, our results position our synthetic datasets as the most effective and cost-efficient publicly available resources for advancing mathematical problem-solving. Our datasets, models and code are publicly available at https://github.com/hkust-nlp/dart-math.'}",https://openreview.net{'value': '/pdf/26d6bf8a231686aaa5faf9277e38c2b2d934ff28.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=zJremsKVyh,{'value': 'Marginal Causal Flows for Validation and Inference'},Daniel de Vassimon Manela; Laura Battaglia; Robin J. Evans,~Daniel_de_Vassimon_Manela1; ~Laura_Battaglia1; ~Robin_J._Evans2,"{'value': ['Causal Inference', 'Normalising Flows', 'Synthetic Data', 'Marginal Structural Models']}","{'value': 'Investigating the marginal causal effect of an intervention on an outcome from complex data remains challenging due to the inflexibility of employed models and the lack of complexity in causal benchmark datasets, which often fail to reproduce intricate real-world data patterns. In this paper we introduce Frugal Flows, a likelihood-based machine learning model that uses normalising flows to flexibly learn the data-generating process, while also directly targeting the marginal causal quantities inferred from observational data. We provide a novel algorithm for fitting a model to observational data with a parametrically specified causal distribution, and propose that these models are exceptionally well suited for synthetic data generation to validate causal methods. Unlike existing data generation methods, Frugal Flows generate synthetic data that closely resembles the empirical dataset, while also automatically and exactly satisfying a user-defined average treatment effect. To our knowledge, Frugal Flows are the first generative model to both learn flexible data representations and also \\textit{exactly} parameterise quantities such as the average treatment effect and the degree of unobserved confounding. We demonstrate the above with experiments on  both simulated and real-world datasets.'}",https://openreview.net{'value': '/pdf/5ca85bc9b90e258067e112db30bfa5eae96a4a2a.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=zBMKodNgKX,{'value': 'FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction'},Ziwei Li; Xiaoqi Wang; Hong-You Chen; Han Wei Shen; Wei-Lun Chao,~Ziwei_Li3; ~Xiaoqi_Wang2; ~Hong-You_Chen1; ~Han_Wei_Shen1; ~Wei-Lun_Chao1,"{'value': ['Federated Learning', 'Dimensionality Reduction', 'Unsupervised Learning', 'Representation Learning']}","{'value': 'Federated learning (FL) has rapidly evolved as a promising paradigm that enables collaborative model training across distributed participants without exchanging their local data. Despite its broad applications in fields such as computer vision, graph learning, and natural language processing, the development of a data projection model that can be effectively used to visualize data in the context of FL is crucial yet remains heavily under-explored. Neighbor embedding (NE) is an essential technique for visualizing complex high-dimensional data, but collaboratively learning a joint NE model is difficult. The key challenge lies in the objective function, as effective visualization algorithms like NE require computing loss functions among pairs of data. \nIn this paper, we introduce \\textsc{FedNE}, a novel approach that integrates the \\textsc{FedAvg} framework with the contrastive NE technique, without any requirements of shareable data. To address the lack of inter-client repulsion which is crucial for the alignment in the global embedding space, we develop a surrogate loss function that each client learns and shares with each other. Additionally, we propose a data-mixing strategy to augment the local data, aiming to relax the problems of invisible neighbors and false neighbors constructed by the local $k$NN graphs. We conduct comprehensive experiments on both synthetic and real-world datasets. The results demonstrate that our \\textsc{FedNE} can effectively preserve the neighborhood data structures and enhance the alignment in the global embedding space compared to several baseline methods.'}",https://openreview.net{'value': '/pdf/713ead3a7d3c84218a49bae4d46cdf7a3a34d042.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=z6reLFqv6w,{'value': 'Learning diverse causally emergent representations from time series data'},David McSharry; Christos Kaplanis; Fernando E Rosas; Pedro A. M. Mediano,~David_McSharry1; ~Christos_Kaplanis2; ~Fernando_E_Rosas1; ~Pedro_A._M._Mediano1,"{'value': ['emergence', 'representation learning']}","{'value': 'Cognitive processes usually take place at a macroscopic scale in systems characterised by emergent properties, which make the whole ‘more than the sum of its parts.’ While recent proposals have provided quantitative, information-theoretic metrics to detect emergence in time series data, it is often highly non-trivial to identify the relevant macroscopic variables a priori. In this paper we leverage recent advances in representation learning and differentiable information estimators to put forward a data-driven method to find emergent variables. The proposed method successfully detects emergent variables and recovers the ground-truth emergence values in a synthetic dataset. Furthermore, we show the method can be extended to learn multiple independent features, extracting a diverse set of emergent quantities. We finally show that a modified method scales to real experimental data from primate brain activity, paving the ground for future analyses uncovering the emergent structure of cognitive representations in biological and artificial intelligence systems.'}",https://openreview.net{'value': '/pdf/8fbf255282581472847dd90c5114aca7f4d35e2d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=z4FaPUslma,{'value': 'Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame'},Evan Markou; Thalaiyasingam Ajanthan; Stephen Gould,~Evan_Markou1; ~Thalaiyasingam_Ajanthan1; ~Stephen_Gould1,"{'value': ['neural collapse', 'equiangular tight frames', 'Riemannian optimisation', 'deep learning']}","{'value': 'Neural Collapse (NC) is a recently observed phenomenon in neural networks that characterises the solution space of the final classifier layer when trained until zero training loss. Specifically, NC suggests that the final classifier layer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weights corresponding to each class. By duality, the penultimate layer feature means also converge to the same simplex ETF. Since this simple symmetric structure is optimal, our idea is to utilise this property to improve convergence speed. Specifically, we introduce the notion of \\textit{nearest simplex ETF geometry} for the penultimate layer features at any given training iteration, by formulating it as a Riemannian optimisation. Then, at each iteration, the classifier weights are implicitly set to the nearest simplex ETF by solving this inner-optimisation, which is encapsulated within a declarative node to allow backpropagation. Our experiments on synthetic and real-world architectures on classification tasks demonstrate that our approach accelerates convergence and enhances training stability.'}",https://openreview.net{'value': '/pdf/ac02c11fa162633bf19fadb27beddf13e3c58e97.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ylceJ2xIw5,{'value': 'Fair Wasserstein Coresets'},Zikai Xiong; Niccolo Dalmasso; Shubham Sharma; Freddy Lecue; Daniele Magazzeni; Vamsi K. Potluru; Tucker Balch; Manuela Veloso,~Zikai_Xiong1; ~Niccolo_Dalmasso1; ~Shubham_Sharma4; ~Freddy_Lecue1; ~Daniele_Magazzeni1; ~Vamsi_K._Potluru1; ~Tucker_Balch2; ~Manuela_Veloso1,"{'value': ['Algorithmic Fairness', 'Nonconvex Optimization', 'Coresets']}","{'value': ""Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored.  In this work, we present fair Wasserstein coresets ($\\texttt{FWC}$), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. $\\texttt{FWC}$ uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of $\\texttt{FWC}$ is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that $\\texttt{FWC}$:  (i) achieves a competitive fairness-performance tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4).""}",https://openreview.net{'value': '/pdf/93985e308e0356a2b95c8e021f79d007aeda2429.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ykQnxko1cJ,{'value': 'CemiFace: Center-based Semi-hard Synthetic Face Generation for Face Recognition'},Zhonglin Sun; Siyang Song; Ioannis Patras; Georgios Tzimiropoulos,~Zhonglin_Sun1; ~Siyang_Song1; ~Ioannis_Patras2; ~Georgios_Tzimiropoulos1,"{'value': ['synthetic face recognition', 'diffusion models', 'center-based semi-hard']}","{'value': 'Privacy issue is a main concern in developing face recognition techniques. Although synthetic face images can partially mitigate potential legal risks while maintaining effective face recognition (FR) performance, FR models trained by face images synthesized by existing generative approaches frequently suffer from performance degradation problems due to the insufficient discriminative quality of these synthesized samples. In this paper, we systematically investigate what contributes to solid face recognition model training, and reveal that face images with certain degree of similarities to their identity centers show great effectiveness in the performance of trained FR models. Inspired by this, we propose a novel diffusion-based approach (namely **Ce**nter-based Se**mi**-hard Synthetic Face\nGeneration (**CemiFace**) which produces facial samples with various levels of similarity to the subject center, thus allowing to generate face datasets containing effective discriminative samples for training face recognition. Experimental results show that with a modest degree of similarity, training on the generated dataset can produce competitive performance compared to previous generation methods. The code will be available at:https://github.com/szlbiubiubiu/CemiFace'}",https://openreview.net{'value': '/pdf/30f78a219d61e45796c28fce873caf8b9bd87ab7.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=yfQwyxiSJ7,{'value': 'Color-Oriented Redundancy Reduction in Dataset Distillation'},Bowen Yuan; Zijian Wang; Mahsa Baktashmotlagh; Yadan Luo; Zi Huang,~Bowen_Yuan3; ~Zijian_Wang2; ~Mahsa_Baktashmotlagh1; ~Yadan_Luo1; ~Zi_Huang1,"{'value': ['Computer Vision', 'Data Distillation', 'Parameterization']}","{'value': 'Dataset Distillation (DD) is designed to generate condensed representations of extensive image datasets, enhancing training efficiency. Despite recent advances, there remains considerable potential for improvement, particularly in addressing the notable redundancy within the color space of distilled images. In this paper, we propose a two-fold optimization strategy to minimize color redundancy at the individual image and overall dataset levels, respectively. At the image level, we employ a palette network, a specialized neural network, to dynamically allocate colors from a reduced color space to each pixel. The palette network identifies essential areas in synthetic images for model training, and consequently assigns more unique colors to them. At the dataset level, we develop a color-guided initialization strategy to minimize redundancy among images. Representative images with the least replicated color patterns are selected based on the information gain. A comprehensive performance study involving various datasets and evaluation scenarios is conducted, demonstrating the superior performance of our proposed color-aware DD compared to existing DD methods.'}",https://openreview.net{'value': '/pdf/20b534cf5fff43e4e9a8229eb66f4841e6dba9df.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=yeFx5NQmr7,{'value': 'Learning 3D Garment Animation from Trajectories of A Piece of Cloth'},Yidi Shao; Chen Change Loy; Bo Dai,~Yidi_Shao1; ~Chen_Change_Loy2; ~Bo_Dai2,"{'value': ['Garment', 'Cloth', 'Simulation', 'Constitutive Model', '3D']}","{'value': 'Garment animation is ubiquitous in various applications, such as virtual reality, gaming, and film producing. Recently, learning-based approaches obtain compelling performance in animating diverse garments under versatile scenarios. Nevertheless, to mimic the deformations of the observed garments, data-driven methods require large scale of garment data, which are both resource-wise expensive and time-consuming. In addition, forcing models to match the dynamics of observed garment animation may hinder the potentials to generalize to unseen cases. In this paper, instead of using garment-wise supervised-learning we adopt a disentangled scheme to learn how to animate observed garments: 1). learning constitutive behaviors from the observed cloth; 2). dynamically animate various garments constrained by the learned constitutive laws. Specifically, we propose Energy Unit network (EUNet) to model the constitutive relations in the format of energy. Without the priors from analytical physics models and differentiable simulation engines, EUNet is able to directly capture the constitutive behaviors from the observed piece of cloth and uniformly describes the change of energy caused by deformations, such as stretching and bending. We further apply the pre-trained EUNet to animate various garments based on energy optimizations. The disentangled scheme alleviates the need of garment data and enables us to utilize the dynamics of a piece of cloth for animating garments. Experiments show that while EUNet effectively delivers the energy gradients due to the deformations, models constrained by EUNet achieve more stable and physically plausible performance comparing with those trained in garment-wise supervised manner.'}",https://openreview.net{'value': '/pdf/d57b0731216ccd13a02117aa1f63730ec58dae56.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=yWSxjlFsmX,{'value': 'Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?'},Yang Dai; Oubo Ma; Longfei Zhang; Xingxing Liang; Shengchao Hu; Mengzhu Wang; Shouling Ji; Jincai Huang; Li Shen,~Yang_Dai2; ~Oubo_Ma1; ~Longfei_Zhang3; ~Xingxing_Liang1; ~Shengchao_Hu1; ~Mengzhu_Wang1; ~Shouling_Ji1; ~Jincai_Huang1; ~Li_Shen1,{'value': ['Offline RL; Trajectory Optimization; Mamba']},"{'value': ""Transformer-based trajectory optimization methods have demonstrated exceptional performance in offline Reinforcement Learning (offline RL). Yet, it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power. Mamba, a promising new linear-time sequence model, offers performance on par with transformers while delivering substantially fewer parameters on long sequences. As it remains unclear whether Mamba is compatible with trajectory optimization, this work aims to conduct comprehensive experiments to explore the potential of Decision Mamba (dubbed DeMa) in offline RL from the aspect of data structures and essential components with the following insights: (1) Long sequences impose a significant computational burden without contributing to performance improvements since DeMa's focus on sequences diminishes approximately exponentially. Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa. (2) For the components of DeMa, we identify the hidden attention mechanism as a critical factor in its success, which can also work well with other residual structures and does not require position embedding. Extensive evaluations demonstrate that our specially designed DeMa is compatible with trajectory optimization and surpasses previous methods, outperforming Decision Transformer (DT) with higher performance while using 30\\% fewer parameters in Atari, and exceeding DT with only a quarter of the parameters in MuJoCo.""}",https://openreview.net{'value': '/pdf/e8f05bc8b78365623dc8f45e047f65b46390a923.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=yUqUBGioBG,{'value': 'Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations'},Yuli Slavutsky; Yuval Benjamini,~Yuli_Slavutsky1; ~Yuval_Benjamini1,"{'value': ['Zero-Shot Learning', 'Distribution Shift', 'Out of Distribution Generalization', 'Robust Representation Learning']}","{'value': 'Zero-shot learning methods typically assume that the new, unseen classes encountered during deployment come from the same distribution as the the classes in the training set. However, real-world scenarios often involve class distribution shifts (e.g., in age or gender for person identification), posing challenges for zero-shot classifiers that rely on learned representations from training classes. In this work, we propose and analyze a model that assumes that the attribute responsible for the shift is unknown in advance. We show that in this setting, standard training may lead to non-robust representations. To mitigate this, we develop an algorithm for learning robust representations in which (a) synthetic data environments are constructed via hierarchical sampling, and (b) environment balancing penalization, inspired by out-of-distribution problems, is applied. We show that our algorithm improves generalization to diverse class distributions in both simulations and experiments on real-world datasets.'}",https://openreview.net{'value': '/pdf/56a6242c839cd0715aaa64931c08b31501e061f9.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=yURca4wi2L,{'value': 'Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations'},Haoming Cai; Jingxi Chen; Brandon Y. Feng; Weiyun Jiang; Mingyang Xie; Kevin Zhang; Cornelia Fermuller; Yiannis Aloimonos; Ashok Veeraraghavan; Christopher Metzler,~Haoming_Cai2; ~Jingxi_Chen1; ~Brandon_Y._Feng1; ~Weiyun_Jiang1; ~Mingyang_Xie1; ~Kevin_Zhang3; ~Cornelia_Fermuller3; ~Yiannis_Aloimonos1; ~Ashok_Veeraraghavan1; ~Christopher_Metzler1,{'value': ['Atmospheric Turbulence Mitigation']},"{'value': ""Atmospheric turbulence, caused by random fluctuations in the atmosphere's refractive index, introduces complex spatio-temporal distortions in imagery captured at long range. Video Atmospheric Turbulence Mitigation (ATM) aims to restore videos affected by these distortions. However, existing video ATM methods, both supervised and self-supervised, struggle to maintain temporally consistent mitigation across frames, leading to visually incoherent results. This limitation arises from the stochastic nature of atmospheric turbulence, which varies across space and time. Inspired by the observation that atmospheric turbulence induces high-frequency temporal variations, we propose ConVRT, a novel framework for consistent video restoration through turbulence. ConVRT introduces a neural video representation that explicitly decouples spatial and temporal information into a spatial content field and a temporal deformation field, enabling targeted regularization of the network's temporal representation capability. By leveraging the low-pass filtering properties of the regularized temporal representations, ConVRT effectively mitigates turbulence-induced temporal frequency variations and promotes temporal consistency. Furthermore, our training framework seamlessly integrates supervised pre-training on synthetic turbulence data with self-supervised learning on real-world videos, significantly improving the temporally consistent mitigation of ATM methods on diverse real-world data. More information can be found on our project page: https://convrt-2024.github.io/""}",https://openreview.net{'value': '/pdf/b24f5f37299924fd9dcef2c90341e7676d541ddb.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=yRRCH1OsGW,{'value': 'Generative Modeling of Molecular Dynamics Trajectories'},Bowen Jing; Hannes Stark; Tommi Jaakkola; Bonnie Berger,~Bowen_Jing1; ~Hannes_Stark1; ~Tommi_S._Jaakkola1; ~Bonnie_Berger1,"{'value': ['molecular dynamics', 'molecular simulation', 'transition paths', 'Boltzmann distribution', 'proteins']}","{'value': 'Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show preliminary results on scaling to protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen.'}",https://openreview.net{'value': '/pdf/da6d5f4f8604d64cfdf93e0c217c30eb2526a5cd.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=yKvHJJE9le,{'value': 'Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel'},Jialin Li; Marta Zagorowska; Giulia De Pasquale; Alisa Rupenyan; John Lygeros,~Jialin_Li6; ~Marta_Zagorowska1; ~Giulia_De_Pasquale1; ~Alisa_Rupenyan2; ~John_Lygeros1,"{'value': ['Safe learning', 'Bayesian optimization', 'Time-varying optimization']}","{'value': 'Ensuring safety is a key aspect in sequential decision making problems, such as robotics or process control. The complexity of the underlying systems often makes finding the optimal decision challenging, especially when the safety-critical system is time-varying. Overcoming the problem of optimizing an unknown time-varying reward subject to unknown time-varying safety constraints, we propose TVSAFEOPT, a new algorithm built on Bayesian optimization with a spatio-temporal kernel. The algorithm is capable of safely tracking a time-varying safe region without the need for explicit change detection. Optimality guarantees are also provided for the algorithm when the optimization problem becomes stationary. We show that TVSAFEOPT compares favorably against SAFEOPT on synthetic data, both regarding safety and optimality. Evaluation on a realistic case study with gas compressors confirms that TVSAFEOPT ensures safety when solving time-varying optimization problems with unknown reward and safety functions.'}",https://openreview.net{'value': '/pdf/8ccc16f75fc2b8ad413d7dc3ee80d673efeeab6c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=yAAQWBMGiT,{'value': 'Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning'},Yijun Dong; Hoang Phan; Xiang Pan; Qi Lei,~Yijun_Dong1; ~Hoang_Phan1; ~Xiang_Pan3; ~Qi_Lei1,"{'value': ['Data selection', 'Finetuning', 'Sketching', 'Johnson-Lindenstrauss transform']}","{'value': 'We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\\mathcal{S}$; (ii) then the variance is reduced over $\\mathcal{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\\mathcal{S}$ preserves the fast-rate generalization $O(\\dim(\\mathcal{S})/n)$, independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks.'}",https://openreview.net{'value': '/pdf/342630203974cf3966cb02c9c856602a6fdba381.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=y8HUXkwAOg,{'value': 'ChronoEpilogi: Scalable Time Series Selection with Multiple Solutions'},Etienne Vareille; Michele Linardi; Ioannis Tsamardinos; Vassilis Christophides,~Etienne_Vareille1; ~Michele_Linardi1; ~Ioannis_Tsamardinos1; ~Vassilis_Christophides1,"{'value': ['Multivariate Time Series Causal Discovery', 'Forecasting', 'Explanations', 'Multiple Markov Boundaries']}","{'value': 'We consider the problem of selecting all the minimal-size subsets of multivariate time-series (TS) variables whose past leads to an optimal predictive model for the future (forecasting) of a given target variable (multiple feature selection problem for times-series). Identifying these subsets leads to gaining insights, domain intuition,and a better understanding of the data-generating mechanism; it is often the first step in causal modeling. While identifying a single solution to the feature selection problem suffices for forecasting purposes, identifying all such minimal-size, optimally predictive subsets is necessary for knowledge discovery and important to avoid misleading a practitioner. We develop the theory of multiple feature selection for time-series data, propose the ChronoEpilogi algorithm, and prove its soundness and completeness under two mild, broad, non-parametric distributional assumptions, namely Compositionality of the distribution and Interchangeability of time-series variables in solutions. Experiments on synthetic and real datasets demonstrate the scalability of ChronoEpilogi to hundreds of TS variables and its efficacy in identifying multiple solutions. In the real datasets, ChronoEpilogi is shown to reduce the number of TS variables by 96% (on average) by conserving or even improving forecasting performance. Furthermore, it is on par with GroupLasso performance, with the added benefit of providing multiple solutions.'}",https://openreview.net{'value': '/pdf/f177a5a2fd2abbbc575b39ef2994a06b9da31513.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=xvYI7TCiU6,{'value': 'Measuring Mutual Policy Divergence for Multi-Agent Sequential Exploration'},Haowen Dou; Lujuan Dang; Zhirong Luan; Badong Chen,~Haowen_Dou1; ~Lujuan_Dang1; ~Zhirong_Luan1; ~Badong_Chen1,"{'value': ['multi-agent reinforcement learning', 'sequential updating', 'exploration', 'Cauchy-Schwarz divergence']}","{'value': 'Despite the success of Multi-Agent Reinforcement Learning (MARL) algorithms in cooperative tasks, previous works, unfortunately, face challenges in heterogeneous scenarios since they simply disable parameter sharing for agent specialization. Sequential updating scheme was thus proposed, naturally diversifying agents by encouraging agents to learn from preceding ones. However, the exploration strategy in sequential scheme has not been investigated. Benefiting from updating one-by-one, agents have the access to the information from preceding agents. Thus, in this work, we propose to exploit the preceding information to enhance exploration and heterogeneity sequentially. We present Multi-Agent Divergence Policy Optimization (MADPO), equipped with mutual policy divergence maximization framework. We quantify the policy discrepancies between episodes to enhance exploration and between agents to heterogenize agents, termed intra-agent and inter-agent policy divergence. To address the issue that traditional divergence measurements lack stability and directionality, we propose to employ the conditional Cauchy-Schwarz divergence to provide entropy-guided exploration incentives. Extensive experiments show that the proposed method outperforms state-of-the-art sequential updating approaches in two challenging multi-agent tasks with various heterogeneous scenarios. Source code is available at \\url{https://github.com/hwdou6677/MADPO}.'}",https://openreview.net{'value': '/pdf/b7229947dbdbbf04ca5c8c83d49e4cd55a3a0c39.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=xtK3gZjQDC,{'value': 'Towards Human-AI Complementarity with Prediction Sets'},Giovanni De Toni; Nastaran Okati; Suhas Thejaswi; Eleni Straitouri; Manuel Gomez Rodriguez,~Giovanni_De_Toni1; ~Nastaran_Okati2; ~Suhas_Thejaswi1; ~Eleni_Straitouri1; ~Manuel_Gomez_Rodriguez1,"{'value': ['conformal prediction', 'decision support systems', 'human-ai complementarity']}","{'value': 'Decision support systems based on prediction sets have proven to be effective at helping human experts solve classification tasks. Rather than providing single-label predictions, these systems provide sets of label predictions constructed using conformal prediction, namely prediction sets, and ask human experts to predict label values from these sets. In this paper, we first show that the prediction sets constructed using conformal prediction are, in general, suboptimal in terms of average accuracy. Then, we show that the problem of finding the optimal prediction sets under which the human experts achieve the highest average accuracy is NP-hard. More strongly, unless P = NP, we show that the problem is hard to approximate to any factor less than the size of the label set. However, we introduce a simple and efficient greedy algorithm that, for a large class of expert models and non-conformity scores, is guaranteed to find prediction sets that provably offer equal or greater performance than those constructed using conformal prediction. Further, using a simulation study with both synthetic and real expert predictions, we demonstrate that, in practice, our greedy algorithm finds near-optimal prediction sets offering greater performance than conformal prediction.'}",https://openreview.net{'value': '/pdf/2b252bb64aa670e885a730dbaf8f392c032ec70b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=xnmm1jThkv,{'value': 'Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models'},Sujai Hiremath; Jacqueline R. M. A. Maasch; Mengxiao Gao; Promit Ghosal; Kyra Gan,~Sujai_Hiremath1; ~Jacqueline_R._M._A._Maasch1; ~Mengxiao_Gao1; ~Promit_Ghosal2; ~Kyra_Gan1,"{'value': ['global causal discovery', 'additive noise model', 'local structure']}","{'value': 'Learning the unique directed acyclic graph corresponding to an unknown causal model is a challenging task. Methods based on functional causal models can identify a unique graph, but either suffer from the curse of dimensionality or impose strong parametric assumptions. To address these challenges, we propose a novel hybrid approach for global causal discovery in observational data that leverages local causal substructures. We first present a topological sorting algorithm that leverages ancestral relationships in linear structural causal models to establish a compact top-down hierarchical ordering, encoding more causal information than linear orderings produced by existing methods. We demonstrate that this approach generalizes to nonlinear settings with arbitrary noise. We then introduce a nonparametric constraint-based algorithm that prunes spurious edges by searching for local conditioning sets, achieving greater accuracy than current methods. We provide theoretical guarantees for correctness and worst-case polynomial time complexities, with empirical validation on synthetic data.'}",https://openreview.net{'value': '/pdf/0bca5f701021ef49d152b7de7f75842cb0a63c54.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=xgP5ynlZWf,{'value': 'RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large Language Models'},Haoyu Chen; Wenbo Li; Jinjin Gu; Jingjing Ren; Sixiang Chen; Tian Ye; Renjing Pei; Kaiwen Zhou; Fenglong Song; Lei Zhu,~Haoyu_Chen2; ~Wenbo_Li6; ~Jinjin_Gu1; ~Jingjing_Ren1; ~Sixiang_Chen2; ~Tian_Ye3; ~Renjing_Pei1; ~Kaiwen_Zhou2; ~Fenglong_Song1; ~Lei_Zhu1,"{'value': ['Image Restoration', 'Low level vision', 'Agent', 'Multimodal Large Language Model']}","{'value': 'Natural images captured by mobile devices often suffer from multiple types of degradation, such as noise, blur, and low light. Traditional image restoration methods require manual selection of specific tasks, algorithms, and execution sequences, which is time-consuming and may yield suboptimal results. All-in-one models, though capable of handling multiple tasks, typically support only a limited range and often produce overly smooth, low-fidelity outcomes due to their broad data distribution fitting. To address these challenges, we first define a new pipeline for restoring images with multiple degradations, and then introduce RestoreAgent, an intelligent image restoration system leveraging multimodal large language models. RestoreAgent autonomously assesses the type and extent of degradation in input images and performs restoration through (1) determining the appropriate restoration tasks, (2) optimizing the task sequence, (3) selecting the most suitable models, and (4) executing the restoration. Experimental results demonstrate the superior performance of RestoreAgent in handling complex degradation, surpassing human experts. Furthermore, the system’s modular design facilitates the fast integration of new tasks and models.'}",https://openreview.net{'value': '/pdf/a7abb52bb68d236e32bce92953c8abf4bfa5f495.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=xabStWAUtr,{'value': 'Co-occurrence is not Factual Association in Language Models'},Xiao Zhang; Miao Li; Ji Wu,~Xiao_Zhang9; ~Miao_Li10; ~Ji_Wu3,"{'value': ['language model', 'knowledge learning', 'reasoning']}","{'value': 'Pretrained language models can encode a large amount of knowledge and utilize it for various reasoning tasks, yet they can still struggle to learn novel factual knowledge effectively from finetuning on limited textual demonstrations. In this work, we show that the reason for this deficiency is that language models are biased to learn word co-occurrence statistics instead of true factual associations. We identify the differences between two forms of knowledge representation in language models: knowledge in the form of co-occurrence statistics is encoded in the middle layers of the transformer model and does not generalize well to reasoning scenarios beyond simple question answering, while true factual associations are encoded in the lower layers and can be freely utilized in various reasoning tasks. Based on these observations, we propose two strategies to improve the learning of factual associations in language models. We show that training on text with implicit rather than explicit factual associations can force the model to learn factual associations instead of co-occurrence statistics, significantly improving the generalization of newly learned knowledge. We also propose a simple training method to actively forget the learned co-occurrence statistics, which unblocks and enhances the learning of factual associations when training on plain narrative text. On both synthetic and real-world corpora, the two proposed strategies improve the generalization of the knowledge learned during finetuning to reasoning scenarios such as indirect and multi-hop question answering.'}",https://openreview.net{'value': '/pdf/a05ca368d45ceff595e9950cf21de3cd1baf43fe.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=xUjBZR6b1T,{'value': 'ReVideo: Remake a Video with Motion and Content Control'},Chong Mou; Mingdeng Cao; Xintao Wang; Zhaoyang Zhang; Ying Shan; Jian Zhang,~Chong_Mou1; ~Mingdeng_Cao1; ~Xintao_Wang1; ~Zhaoyang_Zhang1; ~Ying_Shan2; ~Jian_Zhang22,"{'value': ['Diffusion model', 'Video editing']}","{'value': 'Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge. Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing. In this paper, we present a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness.'}",https://openreview.net{'value': '/pdf/bb0cf0788a982c6b491da99b791d82fa60d2e219.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=xM5m7J6Lbl,{'value': 'Can an AI Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies'},Frédéric Berdoz; Roger Wattenhofer,~Frédéric_Berdoz1; ~Roger_Wattenhofer1,"{'value': ['Alignment', 'Planning', 'Social Choice', 'AI Safety']}","{'value': 'While autonomous agents often surpass humans in their ability to handle vast and complex data, their potential misalignment (i.e., lack of transparency regarding their true objective) has thus far hindered their use in critical applications such as social decision processes. More importantly, existing alignment methods provide no formal guarantees on the safety of such models. Drawing from utility and social choice theory, we provide a novel quantitative definition of alignment in the context of social decision-making. Building on this definition, we introduce probably approximately aligned (i.e., near-optimal) policies, and we derive a sufficient condition for their existence. Lastly, recognizing the practical difficulty of satisfying this condition, we introduce the relaxed concept of safe (i.e., nondestructive) policies, and we propose a simple yet robust method to safeguard the black-box policy of any autonomous agent, ensuring all its actions are verifiably safe for the society.'}",https://openreview.net{'value': '/pdf/bf68ea6396e873fe823317adb57a897d04b26805.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=x4HMnqs6IE,{'value': '$\\text{ID}^3$: Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition'},Jianqing Xu; Shen Li; Jiaying Wu; Miao Xiong; Ailin Deng; Jiazhen Ji; Yuge Huang; Guodong Mu; Wenjie Feng; Shouhong Ding; Bryan Hooi,~Jianqing_Xu1; ~Shen_Li2; ~Jiaying_Wu2; ~Miao_Xiong2; ~Ailin_Deng1; ~Jiazhen_Ji1; ~Yuge_Huang1; ~Guodong_Mu1; ~Wenjie_Feng1; ~Shouhong_Ding3; ~Bryan_Hooi1,{'value': ['synthetic face recognition']},"{'value': 'Synthetic face recognition (SFR) aims to generate synthetic face datasets that mimic the distribution of real face data, which allows for training face recognition models in a privacy-preserving manner. Despite the remarkable potential of diffusion models in image generation, current diffusion-based SFR models struggle with generalization to real-world faces. To address this limitation, we outline three key objectives for SFR: (1) promoting diversity across identities (inter-class diversity), (2) ensuring diversity within each identity by injecting various facial attributes (intra-class diversity), and (3) maintaining identity consistency within each identity group (intra-class identity preservation). Inspired by these goals, we introduce a diffusion-fueled SFR model termed $\\text{ID}^3$. $\\text{ID}^3$ employs an ID-preserving loss to generate diverse yet identity-consistent facial appearances. Theoretically, we show that minimizing this loss is equivalent to maximizing the lower bound of an adjusted conditional log-likelihood over ID-preserving data. This equivalence motivates an ID-preserving sampling algorithm, which operates over an adjusted gradient vector field, enabling the generation of fake face recognition datasets that approximate the distribution of real-world faces. Extensive experiments across five challenging benchmarks validate the advantages of $\\text{ID}^3$.'}",https://openreview.net{'value': '/pdf/773ad46986854b909ee32c632457744c279a0962.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=x33oWJQyH0,{'value': 'Unsupervised Object Detection with Theoretical Guarantees'},Marian Longa; Joao F. Henriques,~Marian_Longa1; ~Joao_F._Henriques1,"{'value': ['unsupervised object detection', 'object detection', 'unsupervised learning', 'representation learning']}","{'value': ""Unsupervised object detection using deep neural networks is typically a difficult problem with few to no guarantees about the learned representation. In this work we present the first unsupervised object detection method that is theoretically guaranteed to recover the true object positions up to quantifiable small shifts. We develop an unsupervised object detection architecture and prove that the learned variables correspond to the true object positions up to small shifts related to the encoder and decoder receptive field sizes, the object sizes, and the widths of the Gaussians used in the rendering process. We perform detailed analysis of how the error depends on each of these variables and perform synthetic experiments validating our theoretical predictions up to a precision of individual pixels. We also perform experiments on CLEVR-based data and show that, unlike current SOTA object detection methods (SAM, CutLER), our method's prediction errors always lie within our theoretical bounds. We hope that this work helps open up an avenue of research into object detection methods with theoretical guarantees.""}",https://openreview.net{'value': '/pdf/7878a3bbb19a093b6e8f4e67ce9a0e0e1dfa65b6.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=wlLjYl0Gi6,{'value': 'Efficient LLM Scheduling by Learning to Rank'},Yichao Fu; Siqi Zhu; Runlong Su; Aurick Qiao; Ion Stoica; Hao Zhang,~Yichao_Fu1; ~Siqi_Zhu1; ~Runlong_Su1; ~Aurick_Qiao1; ~Ion_Stoica1; ~Hao_Zhang2,{'value': ['Large Language Models']},"{'value': 'In Large Language Model (LLM) inference, the output length of an LLM request is typically regarded as not known a priori. Consequently, most LLM serving systems employ a simple First-come-first-serve (FCFS) scheduling strategy, leading to Head-Of-Line (HOL) blocking and reduced throughput and service quality. \nIn this paper, we reexamine this assumption -- we show that, although predicting the exact generation length of each request is infeasible, it is possible to predict the relative ranks of output lengths in a batch of requests, using learning to rank. The ranking information offers valuable guidance for scheduling requests. Building on this insight, we develop a novel scheduler for LLM inference and serving that can approximate the shortest-job-first (SJF) schedule better than existing approaches. We integrate this scheduler with the state-of-the-art LLM serving system and show significant performance improvement in several important applications: 2.8x lower latency in chatbot serving and 6.5x higher throughput in synthetic data generation. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git'}",https://openreview.net{'value': '/pdf/ef9ade264c14ae815c219f762df83610938eb101.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=wgpmDyJgsg,{'value': 'Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis'},Qitao Zhao; Shubham Tulsiani,~Qitao_Zhao1; ~Shubham_Tulsiani1,"{'value': ['computer vision', '3D vision', '3D reconstruction', 'camera pose estimation', 'analysis-by-synthesis', 'generative model']}","{'value': ""Inferring the 3D structure underlying a set of multi-view images typically requires solving two co-dependent tasks -- accurate 3D reconstruction requires precise camera poses, and predicting camera poses relies on (implicitly or explicitly) modeling the underlying 3D. The classical framework of analysis by synthesis casts this inference as a joint optimization seeking to explain the observed pixels, and recent instantiations learn expressive 3D representations (e.g., Neural Fields) with gradient-descent-based pose refinement of initial pose estimates. However, given a sparse set of observed views, the observations may not provide sufficient direct evidence to obtain complete and accurate 3D. Moreover, large errors in pose estimation may not be easily corrected and can further degrade the inferred 3D. To allow robust 3D reconstruction and pose estimation in this challenging setup, we propose SparseAGS, a method that adapts this analysis-by-synthesis approach by: a) including novel-view-synthesis-based generative priors in conjunction with photometric objectives to improve the quality of the inferred 3D, and b) explicitly reasoning about outliers and using a discrete search with a continuous optimization-based strategy to correct them. We validate our framework across real-world and synthetic datasets in combination with several off-the-shelf pose estimation systems as initialization. We find that it significantly improves the base systems' pose accuracy while yielding high-quality 3D reconstructions that outperform the results from current multi-view reconstruction baselines.""}",https://openreview.net{'value': '/pdf/9deb0cefa84b633dd45b98a2c28dfa4cb9a5847d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=wWiAR5mqXq,{'value': 'Reflective Multi-Agent Collaboration based on Large Language Models'},Xiaohe Bo; Zeyu Zhang; Quanyu Dai; Xueyang Feng; Lei Wang; Rui Li; Xu Chen; Ji-Rong Wen,~Xiaohe_Bo1; ~Zeyu_Zhang6; ~Quanyu_Dai1; ~Xueyang_Feng1; ~Lei_Wang46; ~Rui_Li16; ~Xu_Chen13; ~Ji-Rong_Wen1,"{'value': ['Large Language Models', 'Multi-Agent Systems', 'Reflection Mechanism']}","{'value': 'Benefiting from the powerful language expression and planning capabilities of Large Language Models (LLMs), LLM-based autonomous agents have achieved promising performance in various downstream tasks. Recently, based on the development of single-agent systems, researchers propose to construct LLM-based multi-agent systems to tackle more complicated tasks. In this paper, we propose a novel framework, named COPPER, to enhance the collaborative capabilities of LLM-based agents with the self-reflection mechanism. To improve the quality of reflections, we propose to fine-tune a shared reflector, which automatically tunes the prompts of actor models using our counterfactual PPO mechanism. On the one hand, we propose counterfactual rewards to assess the contribution of a single agent’s reflection within the system, alleviating the credit assignment problem. On the other hand, we propose to train a shared reflector, which enables the reflector to generate personalized reflections according to agent roles, while reducing the computational resource requirements and improving training stability. We conduct experiments on three datasets to evaluate the performance of our model in multi-hop question answering, mathematics, and chess scenarios. Experimental results show that COPPER possesses stronger reflection capabilities and exhibits excellent generalization performance across different actor models.'}",https://openreview.net{'value': '/pdf/3b17b8aba5d866085a47c8258c92406af2fc2e10.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=wT2TIfHKp8,{'value': 'Taming the Long Tail in Human Mobility Prediction'},Xiaohang Xu; Renhe Jiang; Chuang Yang; zipei fan; Kaoru Sezaki,~Xiaohang_Xu1; ~Renhe_Jiang1; ~Chuang_Yang3; ~zipei_fan1; ~Kaoru_Sezaki1,"{'value': ['Human Mobility', 'Next POI Prediction', 'Long-Tail Learning']}","{'value': ""With the popularity of location-based services, human mobility prediction plays a key role in enhancing personalized navigation, optimizing recommendation systems, and facilitating urban mobility and planning. This involves predicting a user's next POI (point-of-interest) visit using their past visit history. However, the uneven distribution of visitations over time and space, namely the long-tail problem in spatial distribution, makes it difficult for AI models to predict those POIs that are less visited by humans. In light of this issue, we propose the $\\underline{\\bf{Lo}}$ng-$\\underline{\\bf{T}}$ail Adjusted $\\underline{\\bf{Next}}$ POI Prediction (LoTNext) framework for mobility prediction, combining a Long-Tailed Graph Adjustment module to reduce the impact of the long-tailed nodes in the user-POI interaction graph and a novel Long-Tailed Loss Adjustment module to adjust loss by logit score and sample weight adjustment strategy. Also, we employ the auxiliary prediction task to enhance generalization and accuracy. Our experiments with two real-world trajectory datasets demonstrate that LoTNext significantly surpasses existing state-of-the-art works.""}",https://openreview.net{'value': '/pdf/948e70fa886488a4436315922b773598b84b073d.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=wHFaAH3E8z,{'value': 'FasMe: Fast and Sample-efficient Meta Estimator for Precision Matrix Learning in Small Sample Settings'},Xiao Tan; Yiqin Wang; Yangyang Shen; Dian Shen; Meng Wang; Peibo Duan; Beilun Wang,~Xiao_Tan5; ~Yiqin_Wang2; ~Yangyang_Shen1; ~Dian_Shen1; ~Meng_Wang11; ~Peibo_Duan1; ~Beilun_Wang1,"{'value': ['Precision Matrix Estimation', 'Meta Learning', 'Graphical Model', 'Small Sample']}","{'value': 'Precision matrix estimation is a ubiquitous task featuring numerous applications such as rare disease diagnosis and neural connectivity exploration. However, this task becomes challenging in small sample settings, where the number of samples is significantly less than the number of dimensions, leading to unreliable estimates. Previous approaches either fail to perform well in small sample settings or suffer from inefficient estimation processes, even when incorporating meta-learning techniques.\nTo this end, we propose a novel approach FasMe for Fast and Sample-efficient Meta Precision Matrix Learning, which first extracts meta-knowledge through a multi-task learning diagram. Then, meta-knowledge constraints are applied using a maximum determinant matrix completion algorithm for the novel task. As a result, we reduce the sample size requirements to $O(\\log p/K)$ per meta-training task and $O(\\log\\vert \\mathcal{G}\\vert)$ for the meta-testing task. Moreover, the hereby proposed model only needs $O(p \\log\\epsilon^{-1})$ time and $O(p)$ memory for converging to an $\\epsilon$-accurate solution. On multiple synthetic and biomedical datasets, FasMe is at least ten times faster than the four baselines while promoting prediction accuracy in small sample settings.'}",https://openreview.net{'value': '/pdf/7fb400b0af5f12f29a6d981142457903acaa8378.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=wAqdvcK1Fv,{'value': 'Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces'},Tobias Schröder; Zijing Ou; Yingzhen Li; Andrew B. Duncan,~Tobias_Schröder2; ~Zijing_Ou1; ~Yingzhen_Li1; ~Andrew_B._Duncan1,"{'value': ['Energy-based models', 'discrete probabilistic modelling', 'tabular data']}","{'value': 'Energy-based models (EBMs) offer a flexible framework for probabilistic modelling across various data domains. However, training EBMs on data in discrete or mixed state spaces poses significant challenges due to the lack of robust and fast sampling methods. In this work, we propose to train discrete EBMs with Energy Discrepancy, a loss function which only requires the evaluation of the energy function at data points and their perturbed counterparts, thus eliminating the need for Markov chain Monte Carlo. We introduce perturbations of the data distribution by simulating a diffusion process on the discrete state space endowed with a graph structure. This allows us to inform the choice of perturbation from the structure of the modelled discrete variable, while the continuous time parameter enables fine-grained control of the perturbation. Empirically, we demonstrate the efficacy of the proposed approaches in a wide range of applications, including the estimation of discrete densities with non-binary vocabulary and binary image modelling. We also introduce the first application of EBMs to tabular data sets with applications in synthetic data generation and calibrated classification.'}",https://openreview.net{'value': '/pdf/b940a6b863bcbcdb355543eae11b90bc3e6fd5e2.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=w50ICQC6QJ,{'value': 'Discovery of the Hidden World with Large Language Models'},Chenxi Liu; Yongqiang Chen; Tongliang Liu; Mingming Gong; James Cheng; Bo Han; Kun Zhang,~Chenxi_Liu3; ~Yongqiang_Chen1; ~Tongliang_Liu1; ~Mingming_Gong1; ~James_Cheng2; ~Bo_Han1; ~Kun_Zhang1,"{'value': ['Causal Discovery', 'Large Language Models', 'Causal Representation Learning']}","{'value': 'Revealing the underlying causal mechanisms in the real world is the key to the development of science. Despite the progress in the past decades, traditional causal discovery approaches (CDs) mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. The lack of well-defined high-level variables in many real-world applications has already been a longstanding roadblock to a broader application of CDs. To this end, this paper presents Causal representatiOn AssistanT (COAT) that introduces large language models (LLMs) to bridge the gap. LLMs are trained on massive observations of the world and have demonstrated great capability in extracting key information from unstructured data. Therefore, it is natural to employ LLMs to assist with proposing useful high-level factors and crafting their measurements. Meanwhile, COAT also adopts CDs to find causal relations among the identified variables as well as to provide feedback to LLMs to iteratively refine the proposed factors. We show that LLMs and CDs are mutually beneficial and the constructed feedback provably also helps with the factor proposal. We construct and curate several synthetic and real-world benchmarks including analysis of human reviews and diagnosis of neuropathic and brain tumors, to comprehensively evaluate COAT. Extensive empirical results confirm the effectiveness and reliability of COAT with significant improvements.'}",https://openreview.net{'value': '/pdf/d7bdc070a6044df2e284ee1476561ea96fa74dae.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=w28i9oe9Xr,{'value': 'High Rank Path Development: an approach to learning the filtration of stochastic processes'},Jiajie Tao; Hao Ni; Chong Liu,~Jiajie_Tao1; ~Hao_Ni2; ~Chong_Liu8,{'value': ['adapted weak topology; stochastic process; synthetic time series generation; path development']},"{'value': 'Since the weak convergence for stochastic processes does not account for the growth of information over time which is represented by the underlying filtration, a slightly erroneous stochastic model in weak topology may cause huge loss in multi-periods decision making problems. To address such discontinuities, Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the filtration, of stochastic processes; however, it was considered to be hard to find efficient numerical implementations. In this paper, we introduce a novel metric called High Rank PCF Distance (HRPCFD) for extended weak convergence based on the high rank path development method from rough path theory, which also defines the characteristic function for measure-valued processes. We then show that such HRPCFD admits many favourable analytic properties which allows us to design an efficient algorithm for training HRPCFD from data and construct the HRPCF-GAN by using HRPCFD as the discriminator for conditional time series generation. Our numerical experiments on both hypothesis testing and generative modelling validate the out-performance of our approach compared with several state-of-the-art methods, highlighting its potential in broad applications of synthetic time series generation and in addressing classic financial and economic challenges, such as optimal stopping or utility maximisation problems. Code is available at https://github.com/DeepIntoStreams/High-Rank-PCF-GAN.git.'}",https://openreview.net{'value': '/pdf/eb7aecacaf5d45f5ac40f8a3fe78d6f3122cb6e7.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=vt2qkE1Oax,{'value': 'Learning Segmentation from Point Trajectories'},Laurynas Karazija; Iro Laina; Christian Rupprecht; Andrea Vedaldi,~Laurynas_Karazija1; ~Iro_Laina1; ~Christian_Rupprecht1; ~Andrea_Vedaldi1,"{'value': ['unsupervised segmentation', 'motion segmentation', 'point tracking']}","{'value': 'We consider the problem of segmenting objects in videos based on their motion and no other forms of supervision. Prior work has often approached this problem by using the principle of common fate, namely the fact that the motion of points that belong to the same object is strongly correlated. However, most authors have only considered instantaneous motion from optical flow. In this work, we present a way to train a segmentation network using long-term point trajectories as a supervisory signal to complement optical flow. The key difficulty is that long-term motion, unlike instantaneous motion, is difficult to model -- any parametric approximation is unlikely to capture complex motion patterns over long periods of time. We instead draw inspiration from subspace clustering approaches, proposing a loss function that seeks to group the trajectories into low-rank matrices where the motion of object points can be approximately explained as a linear combination of other point tracks. Our method outperforms the prior art on motion-based segmentation, which shows the utility of long-term motion and the effectiveness of our formulation.'}",https://openreview.net{'value': '/pdf/59c057325ef06b75796a650f052e761c2c377f1e.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=vjsd8Bcipv,{'value': '$\\epsilon$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise'},Jialiang Wang; Xiong Zhou; Deming Zhai; Junjun Jiang; Xiangyang Ji; Xianming Liu,~Jialiang_Wang3; ~Xiong_Zhou3; ~Deming_Zhai2; ~Junjun_Jiang2; ~Xiangyang_Ji1; ~Xianming_Liu5,"{'value': ['Learning with Noisy Labels', 'Robust Loss Function', 'Excess Risk Bound']}","{'value': 'Noisy labels pose a common challenge for training accurate deep neural networks. To mitigate label noise, prior studies have proposed various robust loss functions to achieve noise tolerance in the presence of label noise, particularly symmetric losses. However, they usually suffer from the underfitting issue due to the overly strict symmetric condition. In this work, we propose a simple yet effective approach for relaxing the symmetric condition, namely **$\\epsilon$-softmax**, which simply modifies the outputs of the softmax layer to approximate one-hot vectors with a controllable error $\\epsilon$. Essentially, ***$\\epsilon$-softmax** not only acts as an alternative for the softmax layer, but also implicitly plays the crucial role in modifying the loss function.* We  prove theoretically that **$\\epsilon$-softmax** can achieve noise-tolerant learning with controllable excess risk bound for almost any loss function. Recognizing that **$\\epsilon$-softmax**-enhanced losses may slightly reduce fitting ability on clean datasets, we further incorporate them with one symmetric loss, thereby achieving a better trade-off between robustness and effective learning. Extensive experiments demonstrate the superiority of our method in mitigating synthetic and real-world label noise.'}",https://openreview.net{'value': '/pdf/f2d333e0e79783e10cbe29dab26d04b300ab7d1c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=v4dXL3LsGX,{'value': 'Learning to Cooperate with Humans using Generative Agents'},Yancheng Liang; Daphne Chen; Abhishek Gupta; Simon Shaolei Du; Natasha Jaques,~Yancheng_Liang1; ~Daphne_Chen1; ~Abhishek_Gupta1; ~Simon_Shaolei_Du1; ~Natasha_Jaques1,"{'value': ['multi-agent reinforcement learning', 'human-AI cooperation']}","{'value': ""Training agents that can coordinate zero-shot with humans is a key mission in multi-agent reinforcement learning (MARL). Current algorithms focus on training simulated human partner policies which are then used to train a Cooperator agent. The simulated human is produced either through behavior cloning over a dataset of human cooperation behavior, or by using MARL to create a population of simulated agents. However, these approaches often struggle to produce a Cooperator that can coordinate well with real humans, since the simulated humans fail to cover the diverse strategies and styles employed by people in the real world.  We show \\emph{learning a generative model of human partners} can effectively address this issue. Our model learns a latent variable representation of the human that can be regarded as encoding the human's unique strategy, intention, experience, or style. This generative model can be flexibly trained from any (human or neural policy) agent interaction data. By sampling from the latent space, we can use the generative model to produce different partners to train Cooperator agents. We evaluate our method---Generative Agent Modeling for Multi-agent Adaptation (GAMMA)---on Overcooked, a challenging cooperative cooking game that has become a standard benchmark for zero-shot coordination. We conduct an evaluation with real human teammates, and the results show that GAMMA consistently improves performance, whether the generative model is trained on simulated populations or human datasets. Further, we propose a method for posterior sampling from the generative model that is biased towards the human data, enabling us to efficiently improve performance with only a small amount of expensive human interaction data.""}",https://openreview.net{'value': '/pdf/298dd46c5abad00c26df7b356fb1f3812baaeb74.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=uwSaDHLlYc,{'value': 'Diversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment'},Jiawei Du; Xin Zhang; Juncheng Hu; Wenxin Huang; Joey Tianyi Zhou,~Jiawei_Du1; ~Xin_Zhang29; ~Juncheng_Hu1; ~Wenxin_Huang1; ~Joey_Tianyi_Zhou1,"{'value': ['Dataset Distillation', 'Synthetic Data', 'Diversity', 'Generatlization']}","{'value': 'The sharp increase in data-related expenses has motivated research into condensing datasets while retaining the most informative features. Dataset distillation has thus recently come to the fore. This paradigm generates synthetic datasets that are representative enough to replace the original dataset in training a neural network. To avoid redundancy in these synthetic datasets, it is crucial that each element contains unique features and remains diverse from others during the synthesis stage. In this paper, we provide a thorough theoretical and empirical analysis of diversity within synthesized datasets. We argue that enhancing diversity can improve the parallelizable yet isolated synthesizing approach. Specifically, we introduce a novel method that employs dynamic and directed weight adjustment techniques to modulate the synthesis process, thereby maximizing the representativeness and diversity of each synthetic instance. Our method ensures that each batch of synthetic data mirrors the characteristics of a large, varying subset of the original dataset. Extensive experiments across multiple datasets, including CIFAR, Tiny-ImageNet, and ImageNet-1K, demonstrate the superior performance of our method, highlighting its effectiveness in producing diverse and representative synthetic datasets with minimal computational expense. Our code is available at https://github.com/AngusDujw/Diversity-Driven-Synthesis.'}",https://openreview.net{'value': '/pdf/15929c1e779391b46cb1e2a2780b2c8c2975171b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=uuQQwrjMzb,{'value': 'Adaptive Labeling for Efficient Out-of-distribution Model Evaluation'},Daksh Mittal; Yuanzhe Ma; Shalmali Joshi; Hongseok Namkoong,~Daksh_Mittal1; ~Yuanzhe_Ma1; ~Shalmali_Joshi1; ~Hongseok_Namkoong2,"{'value': ['Model Evaluation', 'Uncertainty Quantification', 'Markov Decision Process', 'Policy Gradient', 'Auto-differentiation']}","{'value': 'Datasets often suffer severe selection bias; clinical labels are only available on patients for whom doctors ordered medical exams. To assess model performance outside the support of available data, we present a computational framework for adaptive labeling, providing cost-efficient model evaluations under severe distribution shifts. We formulate the problem as a Markov Decision Process over states defined by posterior beliefs on model performance. Each batch of new labels incurs a “state transition” to sharper beliefs, and we choose batches to minimize uncertainty on model performance at the end of the label collection process. Instead of relying on high-variance REINFORCE policy gradient estimators that do not scale, our adaptive labeling policy is optimized using path-wise policy gradients computed by auto-differentiating through simulated roll-outs. Our framework is agnostic to different uncertainty quantification approaches and highlights the virtue of planning in adaptive labeling. On synthetic and real datasets, we empirically demonstrate even a one-step lookahead policy substantially outperforms active learning-inspired heuristics.'}",https://openreview.net{'value': '/pdf/9e66a48e6d866d69d2cb84d0ffc85fca6dc59ac5.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ujDKXWTbJX,{'value': 'JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models'},Kun Zhou; Beichen Zhang; jiapeng wang; Zhipeng Chen; Xin Zhao; Jing Sha; Zhichao Sheng; Shijin Wang; Ji-Rong Wen,~Kun_Zhou2; ~Beichen_Zhang1; ~jiapeng_wang4; ~Zhipeng_Chen2; ~Xin_Zhao10; ~Jing_Sha1; ~Zhichao_Sheng1; ~Shijin_Wang1; ~Ji-Rong_Wen1,"{'value': ['Large Language Models', 'Mathematical Reasoning', 'Data Synthesis']}","{'value': 'Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications.\nTo enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis.\nTo reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data.\nTo achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM.\nConcretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.\nBesides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts.\nThe both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM.\nWe leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model. The whole process only needs to invoke GPT-4 API 9.3k times and use 4.6B data for training.\nExperimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings.\nOur code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}.'}",https://openreview.net{'value': '/pdf/b61328368f8231b0bef39a7ed803d8db25907822.pdf'},{'title_filter': 'Data Synthesis'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=u7okTt4ZyE,{'value': 'Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs'},Qinpeng Cui; Yi'xuan Liu; Xinyi Zhang; Qiqi Bao; Qingmin Liao; liwang Amd; Lu Tian; Zicheng Liu; Zhongdao Wang; Emad Barsoum,~Qinpeng_Cui1; ~Yi'xuan_Liu1; ~Xinyi_Zhang10; ~Qiqi_Bao1; ~Qingmin_Liao1; ~liwang_Amd1; ~Lu_Tian3; ~Zicheng_Liu1; ~Zhongdao_Wang2; ~Emad_Barsoum1,{'value': ['Diffusion Models; Super-Resolution']},"{'value': 'Diffusion-based image super-resolution (SR) models have attracted substantial interest due to their powerful image restoration capabilities. However, prevailing diffusion models often struggle to strike an optimal balance between efficiency and performance. Typically, they either neglect to exploit the potential of existing extensive pretrained models, limiting their generative capacity, or they necessitate a dozens of forward passes starting from random noises, compromising inference efficiency. In this paper, we present DoSSR, a $\\textbf{Do}$main $\\textbf{S}$hift diffusion-based SR model that capitalizes on the generative powers of pretrained diffusion models while significantly enhancing efficiency by initiating the diffusion process with low-resolution (LR) images. At the core of our approach is a domain shift equation that integrates seamlessly with existing diffusion models. This integration not only improves the use of diffusion prior but also boosts inference efficiency. Moreover, we advance our method by transitioning the discrete shift process to a continuous formulation, termed as DoS-SDEs. This advancement leads to the fast and customized solvers that further enhance sampling efficiency. Empirical results demonstrate that our proposed method achieves state-of-the-art performance on synthetic and real-world datasets, while notably requiring $\\textbf{\\emph{only 5 sampling steps}}$. Compared to previous diffusion prior based methods, our approach achieves a remarkable speedup of 5-7 times, demonstrating its superior efficiency.'}",https://openreview.net{'value': '/pdf/fe140549f7d235b4d8486b6f7227fafebbdde760.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=tnQbciDjVf,{'value': 'TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration'},Yiwei Guo; Shaobin Zhuang; Kunchang Li; Yu Qiao; Yali Wang,~Yiwei_Guo2; ~Shaobin_Zhuang1; ~Kunchang_Li1; ~Yu_Qiao1; ~Yali_Wang1,"{'value': ['Vision-Language Models', 'Few-shot Transfer Learning', 'Heterogeneous Agent Collaboration']}","{'value': 'Vision-language foundation models (such as CLIP) have recently shown their power in transfer learning, owing to large-scale image-text pre-training. However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such a single model to generalize well. Alternatively, there exists a wide range of expert models that contain diversified vision and/or language knowledge pre-trained on different modalities, tasks, networks, and datasets. Unfortunately, these models are ""isolated agents"" with heterogeneous structures, and how to integrate their knowledge for generalizing CLIP-like models has not been fully explored. To bridge this gap, we propose a general and concise TransAgent framework, which transports the knowledge of the isolated agents in a unified manner, and effectively guides CLIP to generalize with multi-source knowledge distillation. With such a distinct framework, we flexibly collaborate with 11 heterogeneous agents to empower vision-language foundation models, without further cost in the inference phase. Finally, our TransAgent achieves state-of-the-art performance on 11 visual recognition datasets. Under the same low-shot setting, it outperforms the popular CoOp with around 10\\% on average, and 20\\% on EuroSAT which contains large domain shifts.'}",https://openreview.net{'value': '/pdf/49216ae30f248ec7be10e4a2d12eb5a0235dee9b.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=tj8nsfxi5r,{'value': 'From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection'},Xinlei Wang; Maike Feng; Jing Qiu; Jinjin Gu; Junhua Zhao,~Xinlei_Wang3; ~Maike_Feng1; ~Jing_Qiu3; ~Jinjin_Gu1; ~Junhua_Zhao1,"{'value': ['Large Language Model', 'Time Series Forecasting', 'AI Agent']}","{'value': ""This paper introduces a novel approach that leverages Large Language Models (LLMs) and Generative Agents to enhance time series forecasting by reasoning across both text and time series data. With language as a medium, our method adaptively integrates social events into forecasting models, aligning news content with time series fluctuations to provide richer insights. Specifically, we utilize LLM-based agents to iteratively filter out irrelevant news and employ human-like reasoning to evaluate predictions. This enables the model to analyze complex events, such as unexpected incidents and shifts in social behavior, and continuously refine the selection logic of news and the robustness of the agent's output. By integrating selected news events with time series data, we fine-tune a pre-trained LLM to predict sequences of digits in time series. The results demonstrate significant improvements in forecasting accuracy, suggesting a potential paradigm shift in time series forecasting through the effective utilization of unstructured news data.""}",https://openreview.net{'value': '/pdf/ee49cee9662dd2a8cae6583a9b82aff4cb4b5298.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=tb1MlJCY5g,{'value': 'KALM: Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts'},Jing-Cheng Pang; Si-Hang Yang; Kaiyuan Li; Jiaji Zhang; Xiong-Hui Chen; Nan Tang; Yang Yu,~Jing-Cheng_Pang1; ~Si-Hang_Yang1; ~Kaiyuan_Li2; ~Jiaji_Zhang1; ~Xiong-Hui_Chen1; ~Nan_Tang4; ~Yang_Yu5,"{'value': ['reinforcement learning', 'large language models', 'knowledgeable agents']}","{'value': 'Reinforcement learning (RL) traditionally trains agents using interaction data, which limits their capabilities to the scope of the training data. To create more knowledgeable agents, leveraging knowledge from large language models (LLMs) has shown a promising way. Despite various attempts to combine LLMs with RL, there is commonly a semantic gap between action signals and LLM tokens, which hinders their integration. This paper introduces a novel approach, KALM (Knowledgeable Agents from Language Model Rollouts), to learn knowledgeable agents by bridging this gap. KALM extracts knowledge from LLMs in the form of imaginary rollouts, which agents can learn through offline RL. To overcome the limitation that LLMs are inherently text-based and may be incompatible with numerical environmental data, KALM fine-tunes the LLM to perform bidirectional translation between textual goals and rollouts. This process enables the LLM to understand the environment better, facilitating the generation of meaningful rollouts. Experiments on robotic manipulation tasks demonstrate that KALM allows agents to rephrase complex goals and tackle novel tasks requiring new optimal behaviors. KALM achieves a 46% success rate in completing 1400 various novel goals, significantly outperforming the 26% success rate of baseline methods. Project homepage: https://kalmneurips2024.github.io.'}",https://openreview.net{'value': '/pdf/82a85efda24bbab8b3b9acb9f056b2105ab5be11.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=taI8M5DiXj,{'value': 'When to Act and When to Ask: Policy Learning With Deferral Under Hidden Confounding'},Marah Ghoummaid; Uri Shalit,~Marah_Ghoummaid1; ~Uri_Shalit1,"{'value': ['policy learning', 'causal inference', 'sensitivity analysis', 'human-algorithm collaboration']}","{'value': 'We consider the task of learning how to act in collaboration with a human expert based on observational data. The task is motivated by high-stake scenarios such as healthcare and welfare where algorithmic action recommendations are made to a human expert, opening the option of deferring making a recommendation in cases where the human might act better on their own.\n    This task is especially challenging when dealing with observational data, as using such data runs the risk of hidden confounders whose existence can lead to biased and harmful policies. However, unlike standard policy learning, the presence of a human expert can mitigate some of these risks. We build on the work of Mozannar and Sontag (2020) on consistent surrogate loss for learning with the option of deferral to an expert, where they solve a cost-sensitive supervised classification problem. Since we are solving a causal problem, where labels don’t exist, we use a causal model to learn costs which are robust to a bounded degree of hidden confounding.\n    We prove that our approach can take advantage of the strengths of both the model and the expert to obtain a better policy than either. We demonstrate our results by conducting experiments on synthetic and semi-synthetic data and show the advantages of our method compared to baselines.'}",https://openreview.net{'value': '/pdf/6fae7b33c6be48b9db0784a050cc4bc4d4f3c1b0.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=tPgagXpvcV,{'value': 'Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal Transport Loss'},Paul KRZAKALA; Junjie Yang; Rémi Flamary; Florence d'Alché-Buc; Charlotte Laclau; Matthieu Labeau,~Paul_KRZAKALA1; ~Junjie_Yang3; ~Rémi_Flamary1; ~Florence_d'Alché-Buc2; ~Charlotte_Laclau2; ~Matthieu_Labeau2,"{'value': ['Optimal Transport', 'Graph Prediction', 'Structured Prediction', 'Graph', 'Deep Learning']}","{'value': 'We propose Any2graph, a generic framework for end-to-end Supervised Graph Prediction (SGP) i.e. a deep learning model that predicts an entire graph for any kind of input. The framework is built on a novel Optimal Transport loss, the Partially-Masked Fused Gromov-Wasserstein, that exhibits all necessary properties (permutation invariance, differentiability and scalability) and is designed to handle any-sized graphs. Numerical experiments showcase the versatility of the approach that outperform existing competitors on a novel challenging synthetic dataset and a variety of real-world tasks such as map construction from satellite image (Sat2Graph) or molecule prediction from fingerprint (Fingerprint2Graph).'}",https://openreview.net{'value': '/pdf/2da8ac1a2d0e07ab727eb467d8d3907addafb2ca.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=tDMTwto6jv,{'value': 'SEL-BALD: Deep Bayesian Active Learning with Selective Labels'},Ruijiang Gao; Mingzhang Yin; Maytal Saar-Tsechansky,~Ruijiang_Gao2; ~Mingzhang_Yin1; ~Maytal_Saar-Tsechansky1,{'value': ['Bayesian Active Learning with Disagreement; Selective Labels;']},"{'value': 'Machine learning systems are widely used in many high-stakes contexts in which experimental designs for assigning treatments are infeasible. When evaluating decisions is costly, such as investigating fraud cases, or evaluating biopsy decisions, a sample-efficient strategy is needed. However, while existing active learning methods assume humans will always label the instances selected by the machine learning model, in many critical applications, humans may decline to label  instances selected by the machine learning model due to reasons such as regulation constraint, domain knowledge, or algorithmic aversion, thus not sample efficient. \nIn this paper, we study the Active Learning with Instance Rejection (ALIR) problem, which considers the human discretion behavior for high-stakes decision making problems. We propose new active learning algorithms under deep bayesian active learning for selective labeling (SEL-BALD) to address the ALIR problem. Our algorithms consider how to acquire information for both the machine learning model and the human discretion model. We conduct experiments on both synthetic and real-world datasets to demonstrate the effectiveness of our proposed algorithms.'}",https://openreview.net{'value': '/pdf/ea44cb24d4d45cbc9e4fd0819527e6c39cb4b1d4.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=tCf7S75xFa,{'value': 'Physics-Informed Variational State-Space Gaussian Processes'},Oliver Hamelijnck; Arno Solin; Theodoros Damoulas,~Oliver_Hamelijnck1; ~Arno_Solin1; ~Theodoros_Damoulas1,"{'value': ['gaussian processes', 'variational approximations', 'state space gaussian processes', 'physics informed gaussian processes']}","{'value': 'Differential equations are important mechanistic models that are integral to many scientific and engineering applications. With the abundance of available data there has been a growing interest in data-driven physics-informed models. Gaussian processes (GPs) are particularly suited to this task as they can model complex, non-linear phenomena whilst incorporating prior knowledge and quantifying uncertainty. Current approaches have found some success but are limited as they either achieve poor computational scalings or focus only on the temporal setting. This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs. We demonstrate our methods in a range of synthetic and real-world settings and outperform the current state-of-the-art in both predictive and computational performance.'}",https://openreview.net{'value': '/pdf/4424cd43913ea9035300e356e962178e273d1e61.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=sp8wHIsnu9,"{'value': 'Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models'}",Yeming Wen; Swarat Chaudhuri,~Yeming_Wen1; ~Swarat_Chaudhuri1,"{'value': ['diverse generation', 'instruction fine-tuning', 'synthetic dataset']}","{'value': 'Presenting users with diverse responses from foundation models is crucial for enhancing user experience and accommodating varying preferences. \nHowever, generating multiple high-quality and diverse responses without sacrificing accuracy remains a challenge, especially when using greedy sampling. \nIn this work, we propose a novel framework, Synthesize-Partition-Adapt (SPA), that leverages the abundant synthetic data available in many domains to elicit diverse responses from foundation models.\nBy leveraging signal provided by data attribution methods such as influence functions, SPA partitions data into subsets, each targeting unique aspects of the data, and trains multiple model adaptations optimized for these subsets.\nExperimental results demonstrate the effectiveness of our approach in diversifying foundation model responses while maintaining high quality, showcased through the HumanEval and MBPP tasks in the code generation domain and several tasks in the natural language understanding domain, highlighting its potential to enrich user experience across various applications.'}",https://openreview.net{'value': '/pdf/54f326367efea82bf16bab0b436f0f91089cac82.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=skeopn3q5Y,{'value': 'SfPUEL: Shape from Polarization under Unknown Environment Light'},Youwei Lyu; Heng Guo; Kailong Zhang; Si Li; Boxin Shi,~Youwei_Lyu1; ~Heng_Guo3; ~Kailong_Zhang1; ~Si_Li5; ~Boxin_Shi3,"{'value': ['shape-from-polarization', 'photometric 3D reconstruction', 'physics-based vision']}","{'value': 'Shape from polarization (SfP) benefits from advancements like polarization cameras for single-shot normal estimation, but its performance heavily relies on light conditions. This paper proposes SfPUEL, an end-to-end SfP method to jointly estimate surface normal and material under unknown environment light. To handle this challenging light condition, we design a transformer-based framework for enhancing the perception of global context features. We further propose to integrate photometric stereo (PS) priors from pretrained models to enrich extracted features for high-quality normal predictions. As metallic and dielectric materials exhibit different BRDFs, SfPUEL additionally predicts dielectric and metallic material segmentation to further boost performance. Experimental results on synthetic and our collected real-world dataset demonstrate that SfPUEL significantly outperforms existing SfP and single-shot normal estimation methods. The code and dataset is available at https://github.com/YouweiLyu/SfPUEL.'}",https://openreview.net{'value': '/pdf/c45f78bb98e028e3ae80abb25f11f945f5cb1994.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=sOhFyFFnxT,{'value': 'Exploring the Precise Dynamics of Single-Layer GAN Models: Leveraging Multi-Feature Discriminators for High-Dimensional Subspace Learning'},Andrew Bond; Zafer Dogan,~Andrew_Bond1; ~Zafer_Dogan1,"{'value': ['Training Dynamics', 'High-Dimensional Analysis', 'Scaling Limit Analysis', 'Subspace Learning']}","{'value': 'Subspace learning is a critical endeavor in contemporary machine learning, particularly given the vast dimensions of modern datasets. In this study, we delve into the training dynamics of a single-layer GAN model from the perspective of subspace learning, framing these GANs as a novel approach to this fundamental task. Through a rigorous scaling limit analysis, we offer insights into the behavior of this model. Extending beyond prior research that primarily focused on sequential feature learning, we investigate the non-sequential scenario, emphasizing the pivotal role of inter-feature interactions in expediting training and enhancing performance, particularly with an uninformed initialization strategy. Our investigation encompasses both synthetic and real-world datasets, such as MNIST and Olivetti Faces, demonstrating the robustness and applicability of our findings to practical scenarios. By bridging our analysis to the realm of subspace learning, we systematically compare the efficacy of GAN-based methods against conventional approaches, both theoretically and empirically. Notably, our results unveil that while all methodologies successfully capture the underlying subspace, GANs exhibit a remarkable capability to acquire a more informative basis, owing to their intrinsic ability to generate new data samples. This elucidates the unique advantage of GAN-based approaches in subspace learning tasks.'}",https://openreview.net{'value': '/pdf/c1614b551656053cedd678a3f2a3c41e24876e83.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=s2hA6Bz3LE,{'value': 'Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of CKA'},David Smerkous; Qinxun Bai; Li Fuxin,~David_Smerkous1; ~Qinxun_Bai4; ~Li_Fuxin1,"{'value': ['bayesian inference', 'variational inference', 'uncertainty quantification', 'deep learning', 'hypernetworks']}","{'value': 'Particle-based Bayesian deep learning often requires a similarity metric to compare two networks. However, naive similarity metrics lack permutation invariance and are inappropriate for comparing networks. Centered Kernel Alignment (CKA) on feature kernels has been proposed to compare deep networks but has not been used as an optimization objective in Bayesian deep learning. In this paper, we explore the use of CKA in Bayesian deep learning to generate diverse ensembles and hypernetworks that output a network posterior. Noting that CKA projects kernels onto a unit hypersphere and that directly optimizing the CKA objective leads to diminishing gradients when two networks are very similar. We propose adopting the approach of hyperspherical energy (HE) on top of CKA kernels to address this drawback and improve training stability. Additionally, by leveraging CKA-based feature kernels, we derive feature repulsive terms applied to synthetically generated outlier examples. Experiments on both diverse ensembles and hypernetworks show that our approach significantly outperforms baselines in terms of uncertainty quantification in both synthetic and realistic outlier detection tasks.'}",https://openreview.net{'value': '/pdf/e8fd6b257ea14297e3fcc15e027f5b978526a38b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=rgwhJ7INtZ,{'value': 'Super Consistency of Neural Network Landscapes and Learning Rate Transfer'},Lorenzo Noci; Alexandru Meterez; Thomas Hofmann; Antonio Orvieto,~Lorenzo_Noci1; ~Alexandru_Meterez1; ~Thomas_Hofmann1; ~Antonio_Orvieto3,"{'value': ['mup', 'deep learning theory', 'optimization theory', 'edge of stability', 'NTK']}","{'value': ""Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\\mu$P and its depth extension), then some hyperparameters --- such as the learning rate --- exhibit transfer from small to very large models. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is consistently similar across very different model sizes. In this work, we study the landscape through the lens of the Hessian, with a focus on its largest eigenvalue (i.e. the sharpness), and find that certain spectral properties under $\\mu$P are largely independent of the width and depth of the network along the training trajectory. We name this property *super consistency* of the landscape. On the other hand, we show that in the Neural Tangent Kernel (NTK) and other scaling regimes, the sharpness exhibits very different dynamics at different scales. But what causes these differences in the sharpness dynamics? Through a connection between the Hessian's and the NTK's spectrum, we argue that the cause lies in the presence (for $\\mu$P) or progressive absence (for the NTK scaling) of feature learning.\nWe corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and Vision Transformers trained on benchmark vision datasets to Transformers-based language models trained on WikiText.""}",https://openreview.net{'value': '/pdf/4554cac55256e63fef98c0bf476ad4b5b0572873.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=rbtnRsiXSN,{'value': 'DeMo: Decoupling Motion Forecasting into  Directional Intentions and Dynamic States'},Bozhou Zhang; Nan Song; Li Zhang,~Bozhou_Zhang1; ~Nan_Song4; ~Li_Zhang5,"{'value': ['Motion Forecasting', 'Autonomous Driving', 'Mamba', 'Attention']}","{'value': ""Accurate motion forecasting for traffic agents is crucial for ensuring the safety and efficiency of autonomous driving systems in dynamically changing environments. Mainstream methods adopt a one-query-one-trajectory paradigm, where each query corresponds to a unique trajectory for predicting multi-modal trajectories. While straightforward and effective, the absence of detailed representation of future trajectories may yield suboptimal outcomes, given that the agent states dynamically evolve over time. To address this problem, we introduce DeMo, a framework that decouples multi-modal trajectory queries into two types: mode queries capturing distinct directional intentions and state queries tracking the agent's dynamic states over time. By leveraging this format, we separately optimize the multi-modality and dynamic evolutionary properties of trajectories. Subsequently, the mode and state queries are integrated to obtain a comprehensive and detailed representation of the trajectories. To achieve these operations, we additionally introduce combined Attention and Mamba techniques for global information aggregation and state sequence modeling, leveraging their respective strengths. Extensive experiments on both the Argoverse 2 and nuScenes benchmarks demonstrate that our DeMo achieves state-of-the-art performance in motion forecasting. In addition, we will make our code and models publicly available.""}",https://openreview.net{'value': '/pdf/3d4994978d6ca281a3b560f62d73cecc4d2310f7.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=rXGxbDJadh,{'value': 'Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor'},Keji He; Kehan Chen; Jiawang Bai; Yan Huang; Qi Wu; Shu-Tao Xia; Liang Wang,~Keji_He1; ~Kehan_Chen3; ~Jiawang_Bai2; ~Yan_Huang2; ~Qi_Wu3; ~Shu-Tao_Xia1; ~Liang_Wang3,"{'value': ['Vision-and-Language Navigation', 'Multimodal', 'Continous Decision-Making', 'Backdoor Attack']}","{'value': 'Vision-and-Language Navigation (VLN) requires an agent to dynamically explore environments following natural language.\nThe VLN agent, closely integrated into daily lives, poses a substantial threat to the security of privacy and property upon the occurrence of malicious behavior.\nHowever, this serious issue has long been overlooked.\nIn this paper, we pioneer the exploration of an object-aware backdoored VLN, achieved by implanting object-aware backdoors during the training phase. \nTailored to the unique VLN nature of cross-modality and continuous decision-making, we propose a novel backdoored VLN paradigm: IPR Backdoor. \nThis enables the agent to act in abnormal behavior once encountering the object triggers during language-guided navigation in unseen environments, thereby executing an attack on the target scene.\nExperiments demonstrate the effectiveness of our method in both physical and digital spaces across different VLN agents, as well as its robustness to various visual and textual variations. Additionally, our method also well ensures navigation performance in normal scenarios with remarkable stealthiness.'}",https://openreview.net{'value': '/pdf/947b9c499117e1094c174a0a32f3ea8987265e42.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=rIOTceoNc8,{'value': 'Graph Coarsening with Message-Passing Guarantees'},Antonin Joly; Nicolas Keriven,~Antonin_Joly1; ~Nicolas_Keriven1,"{'value': ['graph coarsening', 'message passing', 'graph neural network']}","{'value': 'Graph coarsening aims to reduce the size of a large graph while preserving some of its key properties, which has been used in many applications to reduce computational load and memory footprint. For instance, in graph machine learning, training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic savings in time and memory. However, GNNs rely on the Message-Passing (MP) paradigm, and classical spectral preservation guarantees for graph coarsening do not directly lead to theoretical guarantees when performing naive message-passing on the coarsened graph.\n\nIn this work, we propose a new message-passing operation specific to coarsened graphs, which exhibit theoretical guarantees on the preservation of the propagated signal. Interestingly, and in a sharp departure from previous proposals, this operation on coarsened graphs is oriented, even when the original graph is undirected. We conduct node classification tasks on synthetic and real data and observe improved results compared to performing naive message-passing on the coarsened graph.'}",https://openreview.net{'value': '/pdf/18c2962b68647987ac2041fa833e47449b2d7b51.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=r6V7EjANUK,{'value': 'GSDF: 3DGS Meets SDF for Improved Neural Rendering and Reconstruction'},Mulin Yu; Tao Lu; Linning Xu; Lihan Jiang; Yuanbo Xiangli; Bo Dai,~Mulin_Yu2; ~Tao_Lu4; ~Linning_Xu2; ~Lihan_Jiang2; ~Yuanbo_Xiangli1; ~Bo_Dai2,{'value': ['Neural Rendering; 3D Reconstruction;3D Gaussian Splatting; Signed Distance Field']},"{'value': 'Representing 3D scenes from multiview images remains a core challenge in computer vision and graphics, requiring both reliable rendering and reconstruction, which often conflicts due to the mismatched prioritization of image quality over precise underlying scene geometry. Although both neural implicit surfaces and explicit Gaussian primitives have advanced with neural rendering techniques, current methods impose strict constraints on density fields or primitive shapes, which enhances the affinity for geometric reconstruction at the sacrifice of rendering quality. To address this dilemma, we introduce GSDF, a dual-branch architecture combining 3D Gaussian Splatting (3DGS) and neural Signed Distance Fields (SDF). Our approach leverages mutual guidance and joint supervision during the training process to mutually enhance reconstruction and rendering. Specifically, our method guides the Gaussian primitives to locate near potential surfaces and accelerates the SDF convergence. This implicit mutual guidance ensures robustness and accuracy in both synthetic and real-world scenarios. Experimental results demonstrate that our method boosts the SDF optimization process to reconstruct more detailed geometry, while reducing floaters and blurry edge artifacts in rendering by aligning Gaussian primitives with the underlying geometry.'}",https://openreview.net{'value': '/pdf/6249ed4cb62d4456ad444614a177383b1b92faf9.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=r5nev2SHtJ,{'value': 'From Causal to Concept-Based Representation Learning'},Goutham Rajendran; Simon Buchholz; Bryon Aragam; Bernhard Schölkopf; Pradeep Kumar Ravikumar,~Goutham_Rajendran1; ~Simon_Buchholz1; ~Bryon_Aragam1; ~Bernhard_Schölkopf1; ~Pradeep_Kumar_Ravikumar1,"{'value': ['concept learning', 'causal representation learning', 'interpretable representation learning']}","{'value': 'To build intelligent machine learning systems, modern representation learning attempts to recover latent generative factors from data, such as in causal representation learning. A key question in this growing field is to provide rigorous conditions under which latent factors can be identified and thus, potentially learned. Motivated by extensive empirical literature on linear representations and concept learning, we propose to relax causal notions with a geometric notion of concepts. We formally define a notion of concepts and show rigorously that they can be provably recovered from diverse data. Instead of imposing assumptions on the ""true"" generative latent space, we assume that concepts can be represented linearly in this latent space. The tradeoff is that instead of identifying the ""true"" generative factors, we identify a subset of desired human-interpretable concepts that are relevant for a given application. Experiments on synthetic data, multimodal CLIP models and large language models supplement our results and show the utility of our approach. In this way, we provide a foundation for moving from causal representations to interpretable, concept-based representations by bringing together ideas from these two neighboring disciplines.'}",https://openreview.net{'value': '/pdf/abb49fa44fa001a4ca6a32e9b2c5040ac4ed23be.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=qevq3FZ63J,{'value': 'MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution'},Wei Tao; Yucheng Zhou; Yanlin Wang; Wenqiang Zhang; Hongyu Zhang; Yu Cheng,~Wei_Tao4; ~Yucheng_Zhou1; ~Yanlin_Wang1; ~Wenqiang_Zhang1; ~Hongyu_Zhang1; ~Yu_Cheng1,"{'value': ['Code Change', 'LLM Agent', 'Software Evolution']}","{'value': 'In software development, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code.\nLarge Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level. \nTo overcome this challenge, we empirically study the reason why LLMs fail to resolve GitHub issues and analyze the major factors. \nMotivated by the empirical findings, we propose a novel LLM-based **M**ulti-**A**gent framework for **G**itHub **I**ssue re**S**olution, **MAGIS**, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. \nThis framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. \nIn experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. \nMAGIS can resolve **13.94%** GitHub issues, significantly outperforming the baselines.\nSpecifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM.'}",https://openreview.net{'value': '/pdf/160f5e4c2c7ce5f4555901cb61fa6bd97dbfbd5c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=qGiZQb1Khm,{'value': 'Watermarking Makes Language Models Radioactive'},Tom Sander; Pierre Fernandez; Alain Oliviero Durmus; Matthijs Douze; Teddy Furon,~Tom_Sander1; ~Pierre_Fernandez1; ~Alain_Oliviero_Durmus1; ~Matthijs_Douze1; ~Teddy_Furon1,"{'value': ['Watermarking', 'Large Language Models', 'Membership Inference']}","{'value': 'We investigate the radioactivity of text generated by large language models (LLM), \\ie whether it is possible to detect that such synthetic input was used to train a subsequent LLM.\nCurrent methods like membership inference or active IP protection either work only in settings where the suspected text is known or do not provide reliable statistical guarantees.\nWe discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM.\nOur new methods, specialized for radioactivity, detects with a provable confidence weak residuals of the watermark signal in the fine-tuned LLM.\nWe link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process.\nFor instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $< 10^{-5}$) even when as little as $5\\%$ of training text is watermarked.'}",https://openreview.net{'value': '/pdf/c64b470e2896272a99180a7d3ad4df270ed3e516.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=q7TxGUWlhD,{'value': 'N-agent Ad Hoc Teamwork'},Caroline Wang; Arrasy Rahman; Ishan Durugkar; Elad Liebman; Peter Stone,~Caroline_Wang1; ~Arrasy_Rahman1; ~Ishan_Durugkar1; ~Elad_Liebman1; ~Peter_Stone1,"{'value': ['ad hoc teamwork', 'reinforcement learning', 'multi-agent systems', 'multi-agent reinforcement learning']}","{'value': 'Current approaches to learning cooperative multi-agent behaviors assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls *all* agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a *single* agent in the scenario. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous driving scenario,  a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company. Towards expanding the class of scenarios that cooperative learning methods may optimally address, we introduce $N$*-agent ad hoc teamwork* (NAHT), where a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates. This paper formalizes the problem, and proposes the *Policy Optimization with Agent Modelling* (POAM) algorithm. POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem, that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors. Empirical evaluation on tasks from the multi-agent particle environment and StarCraft II shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates.'}",https://openreview.net{'value': '/pdf/b2a493d4f38a4116108b0ba02a974d3b686c5421.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=paYwtPBpyZ,{'value': 'Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Generation'},Guillaume Huguet; James Vuckovic; Kilian FATRAS; Eric Thibodeau-Laufer; Pablo Lemos; Riashat Islam; Cheng-Hao Liu; Jarrid Rector-Brooks; Tara Akhound-Sadegh; Michael M. Bronstein; Alexander Tong; Joey Bose,~Guillaume_Huguet1; ~James_Vuckovic1; ~Kilian_FATRAS1; ~Eric_Thibodeau-Laufer1; ~Pablo_Lemos1; ~Riashat_Islam1; ~Cheng-Hao_Liu1; ~Jarrid_Rector-Brooks2; ~Tara_Akhound-Sadegh1; ~Michael_M._Bronstein1; ~Alexander_Tong1; ~Joey_Bose1,"{'value': ['Proteins', 'Flow Matching', 'Generative Models']}","{'value': 'Proteins are essential for almost all biological processes and derive their diverse functions from complex $3 \\rm D$ structures, which are in turn determined by their amino acid sequences. \nIn this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow++, a novel sequence-conditioned $\\text{SE}(3)$-equivariant flow matching model for protein structure generation. FoldFlow++ presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase \ndiversity and novelty of generated samples -- crucial for de-novo drug design -- we\ntrain FoldFlow++ at scale on a new dataset \nthat is an order of magnitude \nlarger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FoldFlow++ to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow++ outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow++ makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.'}",https://openreview.net{'value': '/pdf/503e86547852b43509aa82eecef8210d45232c5b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=pWowK7jqok,{'value': 'E-Motion: Future Motion Simulation via Event Sequence Diffusion'},Song Wu; Zhiyu Zhu; Junhui Hou; Guangming Shi; Jinjian Wu,~Song_Wu7; ~Zhiyu_Zhu1; ~Junhui_Hou2; ~Guangming_Shi1; ~Jinjian_Wu1,"{'value': ['Event-based vision', 'video diffusion model']}","{'value': ""Forecasting a typical object's future motion is a critical task for interpreting and interacting with dynamic environments in computer vision. Event-based sensors, which could capture changes in the scene with exceptional temporal granularity, may potentially offer a unique opportunity to predict future motion with a level of detail and precision previously unachievable. Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework. Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset. This process facilitates the transfer of extensive knowledge from RGB videos to an event-centric domain. Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy. Through extensive testing and validation, we demonstrate the effectiveness of our method in various complex scenarios, showcasing its potential to revolutionize motion flow prediction in computer vision applications such as autonomous vehicle guidance, robotic navigation, and interactive media. Our findings suggest a promising direction for future research in enhancing the interpretative power and predictive accuracy of computer vision systems. The source code is\npublicly available at https://github.com/p4r4mount/E-Motion.""}",https://openreview.net{'value': '/pdf/4468e8e5ba691cc274470c891f728ce7aa135bb1.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=pHiTmEsAfZ,{'value': 'Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion'},Yongyuan Liang; Tingqiang Xu; Kaizhe Hu; Guangqi Jiang; Furong Huang; Huazhe Xu,~Yongyuan_Liang1; ~Tingqiang_Xu1; ~Kaizhe_Hu1; ~Guangqi_Jiang1; ~Furong_Huang1; ~Huazhe_Xu1,"{'value': ['Diffusion Model', 'Policy Learning', 'Parameter Generation', 'Reinforcement Learning']}","{'value': 'Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description?\nIn this paper, we present **Make-An-Agent**, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. \nTrained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by **Make-An-Agent** onto real-world robots on locomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/.'}",https://openreview.net{'value': '/pdf/30950d8eec8a2456a781dff694642e9b4c2d048c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=p3tSEFMwpG,{'value': 'Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data'},Kai Helli; David Schnurr; Noah Hollmann; Samuel Müller; Frank Hutter,~Kai_Helli1; ~David_Schnurr1; ~Noah_Hollmann1; ~Samuel_Müller1; ~Frank_Hutter1,"{'value': ['Temporal Distribution Shifts', 'In-Context Learning', 'Bayesian Inference', 'Prior-Data Fitted Networks', 'Temporal Domain Generalization', 'Structural Causal Models', 'TabPFN', 'Tabular Data Modeling', 'Out-Of-Distribution Generalization', 'Domain Generalization', 'Meta-Learning', 'Concept Drift']}","{'value': ""While most ML models expect independent and identically distributed data, this assumption is often violated in real-world scenarios due to distribution shifts, resulting in the degradation of machine learning model performance. Until now, no tabular method has consistently outperformed classical supervised learning, which ignores these shifts. To address temporal distribution shifts, we present Drift-Resilient TabPFN, a fresh approach based on In-Context Learning with a Prior-Data Fitted Network that learns the learning algorithm itself: it accepts the entire training dataset as input and makes predictions on the test set in a single forward pass. Specifically, it learns to approximate Bayesian inference on synthetic datasets drawn from a prior that specifies the model's inductive bias. This prior is based on structural causal models (SCM), which gradually shift over time. To model shifts of these causal models, we use a secondary SCM, that specifies changes in the primary model parameters. The resulting Drift-Resilient TabPFN can be applied to unseen data, runs in seconds on small to moderately sized datasets and needs no hyperparameter tuning. Comprehensive evaluations across 18 synthetic and real-world datasets demonstrate large performance improvements over a wide range of baselines, such as XGB, CatBoost, TabPFN, and applicable methods featured in the Wild-Time benchmark. Compared to the strongest baselines, it improves accuracy from 0.688 to 0.744 and ROC AUC from 0.786 to 0.832 while maintaining stronger calibration. This approach could serve as significant groundwork for further research on out-of-distribution prediction.""}",https://openreview.net{'value': '/pdf/b3a5a1ac99a58bf02a2ed186dff50e980bba1c89.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=owuEcT6BTl,{'value': 'Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space'},Core Francisco Park; Maya Okawa; Andrew Lee; Ekdeep Singh Lubana; Hidenori Tanaka,~Core_Francisco_Park1; ~Maya_Okawa1; ~Andrew_Lee2; ~Ekdeep_Singh_Lubana1; ~Hidenori_Tanaka1,"{'value': ['Learning Dynamics', 'Compositional Generalization', 'Emergent Abilities', 'Diffusion Models', 'Mechanistic Interpretability']}","{'value': 'Modern generative models demonstrate impressive capabilities, likely stemming from an ability to identify and manipulate abstract concepts underlying their training data. However, fundamental questions remain: what determines the concepts a model learns, the order in which it learns them, and its ability to manipulate those concepts? To address these questions, we propose analyzing a model’s learning dynamics via a framework we call the concept space, where each axis represents an independent concept underlying the data generating process. By characterizing learning dynamics in this space, we identify how the speed at which a concept is learned, and hence the order of concept learning, is controlled by properties of the data we term concept signal. Further, we observe moments of sudden turns in the direction of a model’s learning dynamics in concept space. Surprisingly, these points precisely correspond to the emergence of hidden capabilities, i.e., where latent interventions show the model possesses the capability to manipulate a concept, but these capabilities cannot yet be elicited via naive input prompting. While our results focus on synthetically defined toy datasets, we hypothesize a general claim on emergence of hidden capabilities may hold: generative models possess latent capabilities that emerge suddenly and consistently during training, though a model might not exhibit these capabilities under naive input prompting.'}",https://openreview.net{'value': '/pdf/cd344c6f2d8a5970e4d4267156a7cfb867521da6.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=owHj0G15cd,{'value': 'Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandits'},Tian Huang; Shengbo Wang; Ke Li,~Tian_Huang3; ~Shengbo_Wang2; ~Ke_Li5,"{'value': ['multi-objective optimization', 'dueling bandit', 'interactive multi-objective optimization', 'preference learning']}","{'value': 'The ultimate goal of multi-objective optimization (MO) is to assist human decision-makers (DMs) in identifying solutions of interest (SOI) that optimally reconcile multiple objectives according to their preferences. Preference-based evolutionary MO (PBEMO) has emerged as a promising framework that progressively approximates SOI by involving human in the optimization-cum-decision-making process. Yet, current PBEMO approaches are prone to be inefficient and misaligned with the DM’s true aspirations, especially when inadvertently exploiting mis-calibrated reward models. This is further exacerbated when considering the stochastic nature of human feedback. This paper proposes a novel framework that navigates MO to SOI by directly leveraging human feedback without being restricted by a predefined reward model nor cumbersome model selection. Specifically, we developed a clustering-based stochastic dueling bandits algorithm that strategically scales well to high-dimensional dueling bandits, and achieves a regret of $\\mathcal{O}(K^2\\log T)$, where $K$ is the number of clusters and $T$ is the number of rounds. The learned preferences are then transformed into a unified probabilistic format that can be readily adapted to prevalent EMO algorithms. This also leads to a principled termination criterion that strategically manages human cognitive loads and computational budget. Experiments on $48$ benchmark test problems, including synthetic problems, RNA inverse design and protein structure prediction, fully demonstrate the effectiveness of our proposed approach.'}",https://openreview.net{'value': '/pdf/93d75c0607de645189e6114e7cc0f9816082393a.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=otxOtsWCMb,{'value': 'From an Image to a Scene: Learning to Imagine the World from a Million 360° Videos'},Matthew Wallingford; Anand Bhattad; Aditya Kusupati; Vivek Ramanujan; Matt Deitke; Aniruddha Kembhavi; Roozbeh Mottaghi; Wei-Chiu Ma; Ali Farhadi,~Matthew_Wallingford1; ~Anand_Bhattad1; ~Aditya_Kusupati1; ~Vivek_Ramanujan1; ~Matt_Deitke1; ~Aniruddha_Kembhavi1; ~Roozbeh_Mottaghi1; ~Wei-Chiu_Ma1; ~Ali_Farhadi3,"{'value': ['Novel View Synthesis', '3D', 'Video', '360 Video', 'Large-Scale', 'Data', 'Scene Generation']}","{'value': ""Three-dimensional (3D) understanding of objects and scenes play a key role in humans' ability to interact with the world and has been an active area of research in computer vision, graphics, and robotics. Large scale synthetic and object-centric 3D datasets have shown to be effective in training models that have 3D understanding of objects. However, applying a similar approach to real-world objects and scenes is difficult due to a lack of large-scale data. Videos are a potential source for real-world 3D data, but finding diverse yet corresponding views of the same content have shown to be difficult at scale. Furthermore, standard videos come with fixed viewpoints, determined at the time of capture. This restricts the ability to access scenes from a variety of more diverse and potentially useful perspectives. We argue that large scale ODIN videos can address these limitations to provide scalable corresponding frames from diverse views.  In this paper we introduce 360-1M, a 360° video dataset consisting of 1 million videos, and a process for efficiently finding corresponding frames from diverse viewpoints at scale. We train our diffusion-based model, ODIN, on 360-1M. Empowered by the largest real-world, multi-view dataset to date, ODIN is able to freely generate novel views of real-world scenes. Unlike previous methods, ODIN can move the camera through the environment, enabling the model to infer the geometry and layout of the scene. Additionally, we show improved performance on standard novel view synthesis and 3D reconstruction benchmarks.""}",https://openreview.net{'value': '/pdf/e92b581e47ffbaa019c305f5181802de017fd66e.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=otZPBS0un6,{'value': 'FreqBlender: Enhancing DeepFake Detection by Blending Frequency Knowledge'},Hanzhe LI; Jiaran Zhou; Yuezun Li; Baoyuan Wu; Bin Li; Junyu Dong,~Hanzhe_LI2; ~Jiaran_Zhou1; ~Yuezun_Li1; ~Baoyuan_Wu1; ~Bin_Li16; ~Junyu_Dong1,"{'value': ['DeepFake Detection', 'Security and Privacy', 'Multimedia Forensics']}","{'value': 'Generating synthetic fake faces, known as pseudo-fake faces, is an effective way to improve the generalization of DeepFake detection. Existing methods typically generate these faces by blending real or fake faces in spatial domain. While these methods have shown promise, they overlook the simulation of frequency distribution in pseudo-fake faces, limiting the learning of generic forgery traces in-depth. To address this, this paper introduces {\\em FreqBlender}, a new method that can generate pseudo-fake faces by blending frequency knowledge. Concretely, we investigate the major frequency components and propose a Frequency Parsing Network to adaptively partition frequency components related to forgery traces. Then we blend this frequency knowledge from fake faces into real faces to generate pseudo-fake faces. Since there is no ground truth for frequency components, we describe a dedicated training strategy by leveraging the inner correlations among different frequency knowledge to instruct the learning process. Experimental results demonstrate the effectiveness of our method in enhancing DeepFake detection, making it a potential plug-and-play strategy for other methods.'}",https://openreview.net{'value': '/pdf/069e8992a7029f0458ddc78b200f7894c856f29b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=oqdy2EFrja,{'value': 'Compositional 3D-aware Video Generation with LLM Director'},Hanxin Zhu; Tianyu He; Anni Tang; Junliang Guo; Zhibo Chen; Jiang Bian,~Hanxin_Zhu1; ~Tianyu_He1; ~Anni_Tang1; ~Junliang_Guo1; ~Zhibo_Chen1; ~Jiang_Bian1,{'value': ['3D-aware Video Generation; LLM; Compositional']},"{'value': 'Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data. However, substantial challenges remain in precisely controlling individual elements within the generated video, such as the movement and appearance of specific characters and the manipulation of viewpoints. In this work, we propose a novel paradigm that generates each element in 3D representation separately and then composites them with priors from Large Language Models (LLMs) and 2D diffusion models. Specifically, given an input textual query, our scheme consists of four stages: 1) we leverage the LLMs as the director to first decompose the complex query into several sub-queries, where each sub-query describes each element of the generated video; 2) to generate each element, pre-trained models are invoked by the LLMs to obtain the corresponding 3D representation; 3) to composite the generated 3D representations, we prompt multi-modal LLMs to produce coarse guidance on the scale, location, and trajectory of different objects; 4) to make the results adhere to natural distribution, we further leverage 2D diffusion priors and use score distillation sampling to refine the composition. Extensive experiments demonstrate that our method can generate high-fidelity videos from text with flexible control over each element.'}",https://openreview.net{'value': '/pdf/208a96eeea481035e495ffccce283af8e1625a86.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=oNMnR0NJ2e,{'value': 'A Label is Worth A Thousand Images in Dataset Distillation'},Tian Qin; Zhiwei Deng; David Alvarez-Melis,~Tian_Qin3; ~Zhiwei_Deng3; ~David_Alvarez-Melis1,"{'value': ['Dataset distillation', 'Data condensation', 'Synthetic data generation', 'Data-efficient learning']}","{'value': 'Data *quality* is a crucial factor in the performance of machine learning models, a principle that dataset distillation methods exploit by compressing training datasets into much smaller counterparts that maintain similar downstream performance. Understanding how and why data distillation methods work is vital not only for improving these methods but also for revealing fundamental characteristics of ""good” training data. However, a major challenge in achieving this goal is the observation that distillation approaches, which rely on sophisticated but mostly disparate methods to generate synthetic data, have little in common with each other. In this work, we highlight a largely overlooked aspect common to most of these methods: the use of soft (probabilistic) labels. Through a series of ablation experiments, we study the role of soft labels in depth. Our results reveal that the main factor explaining the performance of state-of-the-art distillation methods is not the specific techniques used to generate synthetic data but rather the use of soft labels. Furthermore, we demonstrate that not all soft labels are created equal; they must contain *structured information* to be beneficial. We also provide empirical scaling laws that characterize the effectiveness of soft labels as a function of images-per-class in the distilled dataset and establish an empirical Pareto frontier for data-efficient learning. Combined, our findings challenge conventional wisdom in dataset distillation, underscore the importance of soft labels in learning, and suggest new directions for improving distillation methods. Code for all experiments is available at https://github.com/sunnytqin/no-distillation.'}",https://openreview.net{'value': '/pdf/e453b8d02edbaab707276479e85ca31daeade255.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ntF7D8tAlQ,{'value': 'Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression'},Kai Tan; Pierre C Bellec,~Kai_Tan1; ~Pierre_C_Bellec1,"{'value': ['Robust regression', 'generalization error', 'stochastic gradient descent', 'early stopping', ""Stein's formula""]}","{'value': 'This paper studies the generalization performance of iterates obtained by Gradient Descent (GD), Stochastic Gradient Descent (SGD) and their proximal variants in high-dimensional robust regression problems. The number of features is comparable to the sample size and errors may be heavy-tailed. We introduce estimators that precisely track the generalization error of the iterates along the trajectory of the iterative algorithm. These estimators are provably consistent under suitable conditions. The results are illustrated through several examples, including Huber regression, pseudo-Huber regression, and their penalized variants with non-smooth regularizer. We provide explicit generalization error estimates for iterates generated from GD and SGD, or from proximal SGD in the presence of a non-smooth regularizer. The proposed risk estimates serve as effective proxies for the actual generalization error, allowing us to determine the optimal stopping iteration that minimizes the generalization error. Extensive simulations confirm the effectiveness of the proposed generalization error estimates.'}",https://openreview.net{'value': '/pdf/34445a6dbcca7d59b437582e07c2c2b3e4cd90cd.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=njvPjG0BfK,{'value': 'HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation'},Joseph Cotnareanu; Zhanguang Zhang; Hui-Ling Zhen; Yingxue Zhang; Mark Coates,~Joseph_Cotnareanu1; ~Zhanguang_Zhang1; ~Hui-Ling_Zhen1; ~Yingxue_Zhang1; ~Mark_Coates1,"{'value': ['Graph Learning', 'Boolean Satisfiability', 'Circuit Design']}","{'value': ""Efficiently determining the satisfiability of a boolean equation --- known as the SAT problem for brevity --- is crucial in various industrial problems.  Recently, the advent of deep learning methods has introduced significant potential for enhancing SAT solving. However, a major barrier to the advancement of this field has been the scarcity of large, realistic datasets.  The majority of current public datasets are either randomly generated or extremely limited, containing only a few examples from unrelated problem families. These datasets are inadequate for meaningful training of deep learning methods.  In light of this, researchers have started exploring generative techniques to create data that more accurately reflect SAT problems encountered in practical situations. These methods have so far suffered from either the inability to produce challenging SAT problems or time-scalability obstacles.  In this paper we address both by identifying and manipulating the key contributors to a problem's ``hardness'', known as cores. Although some previous work has addressed cores, the time costs are unacceptably high due to the expense of traditional heuristic core detection techniques. We introduce a fast core detection procedure that uses a graph neural network. Our empirical results demonstrate that we can efficiently generate problems that remain hard to solve and retain key attributes of the original example problems. We show via experiment that the generated synthetic SAT problems can be used in a data augmentation setting to provide improved prediction of solver runtimes.""}",https://openreview.net{'value': '/pdf/11d85eae348bd6102c120bbcd9e9f0b24e68bfd5.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=nd8Q4a8aWl,{'value': 'A Geometric View of Data Complexity: Efficient Local Intrinsic Dimension Estimation with Diffusion Models'},Hamidreza Kamkari; Brendan Leigh Ross; Rasa Hosseinzadeh; Jesse C. Cresswell; Gabriel Loaiza-Ganem,~Hamidreza_Kamkari1; ~Brendan_Leigh_Ross1; ~Rasa_Hosseinzadeh2; ~Jesse_C._Cresswell1; ~Gabriel_Loaiza-Ganem1,"{'value': ['diffusion models', 'deep generative modelling', 'manifold hypothesis', 'intrinsic dimension']}","{'value': 'High-dimensional data commonly lies on low-dimensional submanifolds, and estimating the local intrinsic dimension (LID) of a datum -- i.e. the dimension of the submanifold it belongs to -- is a longstanding problem. LID can be understood as the number of local factors of variation: the more factors of variation a datum has, the more complex it tends to be. Estimating this quantity has proven useful in contexts ranging from generalization in neural networks to detection of out-of-distribution data, adversarial examples, and AI-generated text. The recent successes of deep generative models present an opportunity to leverage them for LID estimation, but current methods based on generative models produce inaccurate estimates, require more than a single pre-trained model, are computationally intensive, or do not exploit the best available deep generative models: diffusion models (DMs). In this work, we show that the Fokker-Planck equation associated with a DM can provide an LID estimator which addresses the aforementioned deficiencies. Our estimator, called FLIPD, is easy to implement and compatible with all popular DMs. Applying FLIPD to synthetic LID estimation benchmarks, we find that DMs implemented as fully-connected networks are highly effective LID estimators that outperform existing baselines. We also apply FLIPD to natural images where the true LID is unknown. Despite being sensitive to the choice of network architecture, FLIPD estimates remain a useful measure of relative complexity; compared to competing estimators, FLIPD exhibits a consistently higher correlation with image PNG compression rate and better aligns with qualitative assessments of complexity. Notably, FLIPD is orders of magnitude faster than other LID estimators, and the first to be tractable at the scale of Stable Diffusion.'}",https://openreview.net{'value': '/pdf/2923abb88a8b518f873227827f652a291f04f8ac.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=nLQeE8QGGe,{'value': 'Active learning of neural population dynamics using two-photon holographic optogenetics'},Andrew Wagenmaker; Lu Mi; Marton Rozsa; Matthew Storm Bull; Karel Svoboda; Kayvon Daie; Matthew D. Golub; Kevin Jamieson,~Andrew_Wagenmaker1; ~Lu_Mi1; ~Marton_Rozsa1; ~Matthew_Storm_Bull1; ~Karel_Svoboda1; ~Kayvon_Daie1; ~Matthew_D._Golub2; ~Kevin_Jamieson1,"{'value': ['active learning', 'experiment design', 'neural system identification', 'neural behavior']}","{'value': 'Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain.  In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.'}",https://openreview.net{'value': '/pdf/f65e32e082781fef08ab450af2a506bb55487173.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=nBjmMF2IZU,{'value': 'Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning'},Yuexiang Zhai; Hao Bai; Zipeng Lin; Jiayi Pan; Shengbang Tong; Yifei Zhou; Alane Suhr; Saining Xie; Yann LeCun; Yi Ma; Sergey Levine,~Yuexiang_Zhai1; ~Hao_Bai1; ~Zipeng_Lin1; ~Jiayi_Pan1; ~Shengbang_Tong1; ~Yifei_Zhou1; ~Alane_Suhr1; ~Saining_Xie2; ~Yann_LeCun1; ~Yi_Ma4; ~Sergey_Levine1,"{'value': ['large vision language model', 'reinforcement learning']}","{'value': 'Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.'}",https://openreview.net{'value': '/pdf/84c596658e84793b911234c08d43b5038aa98129.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=n5lLSskwtu,{'value': 'Evidential Mixture Machines: Deciphering Multi-Label Correlations for Active Learning Sensitivity'},Dayou Yu; Minghao Li; Weishi Shi; Qi Yu,~Dayou_Yu1; ~Minghao_Li12; ~Weishi_Shi2; ~Qi_Yu1,"{'value': ['Active learning', 'multi-label classification']}","{'value': ""Multi-label active learning is a crucial yet challenging area in contemporary machine learning, often complicated by a large and sparse label space. This challenge is further exacerbated in active learning scenarios where labeling resources are constrained. Drawing inspiration from existing mixture of Bernoulli models, which efficiently compress the label space into a more manageable weight coefficient space by learning correlated Bernoulli components, we propose a novel model called Evidential Mixture Machines (EMM). Our model leverages mixture components derived from unsupervised learning in the label space and improves prediction accuracy by predicting weight coefficients following the evidential learning paradigm. These coefficients are aggregated as proxy pseudo counts to enhance component offset predictions. The evidential learning approach provides an uncertainty-aware connection between input features and the predicted coefficients and components. Additionally, our method combines evidential uncertainty with predicted label embedding covariances for active sample selection, creating a richer, multi-source uncertainty metric beyond traditional uncertainty scores. Experiments on synthetic datasets show the effectiveness of evidential uncertainty prediction and EMM's capability to capture label correlations through predicted components. Further testing on real-world datasets demonstrates improved performance compared to existing multi-label active learning methods.""}",https://openreview.net{'value': '/pdf/951c4ba4cdcd1d3b044b89eca3c505a7af66aabd.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=n5R6TvBVcX,{'value': 'WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models'},Liwei Jiang; Kavel Rao; Seungju Han; Allyson Ettinger; Faeze Brahman; Sachin Kumar; Niloofar Mireshghallah; Ximing Lu; Maarten Sap; Yejin Choi; Nouha Dziri,~Liwei_Jiang2; ~Kavel_Rao1; ~Seungju_Han2; ~Allyson_Ettinger1; ~Faeze_Brahman1; ~Sachin_Kumar1; ~Niloofar_Mireshghallah1; ~Ximing_Lu1; ~Maarten_Sap1; ~Yejin_Choi1; ~Nouha_Dziri2,"{'value': ['Red-teaming', 'AI Safety', 'Safety Training', 'LLM Defense', 'Safety Training Data', 'Adversarial Training', 'Adversarial Attacks', 'Jailbreak']}","{'value': 'We introduce WildTeaming, an automatic red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes selections of multiple mined tactics for systematic exploration of novel and even more challenging jailbreaks.\nCompared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with large language models (LLMs), our work investigates jailbreaks from chatbot users in-the-wild who were not specifically instructed to break the system.  WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in more diverse and successful adversarial attacks compared to state-of-the-art jailbreaking methods. \n\nWhile there exist many datasets for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed among all frontier models even when their weights are open. Therefore, with WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. In order to mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (both vanilla and adversarial) and 2) benign queries that resemble harmful queries in form but contain no harmful intent. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive model training and evaluations, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of both vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All the components of WildJailbreak contribute to achieving balanced safety behaviors of models'}",https://openreview.net{'value': '/pdf/5c0e189c5b92a109f691a752108334b171f24840.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=muYhNDlxWc,{'value': 'MGF: Mixed Gaussian Flow for Diverse Trajectory Prediction'},Jiahe Chen; Jinkun Cao; Dahua Lin; Kris M. Kitani; Jiangmiao Pang,~Jiahe_Chen1; ~Jinkun_Cao1; ~Dahua_Lin1; ~Kris_M._Kitani1; ~Jiangmiao_Pang1,"{'value': ['trajectory prediction', 'trajectory forecasting']}","{'value': 'To predict future trajectories, the normalizing flow with a standard Gaussian prior suffers from weak diversity. \nThe ineffectiveness comes from the conflict between the fact of asymmetric and multi-modal distribution of likely outcomes and symmetric and single-modal original distribution and supervision losses.\nInstead, we propose constructing a mixed Gaussian prior for a normalizing flow model for trajectory prediction.\nThe prior is constructed by analyzing the trajectory patterns in the training samples without requiring extra annotations while showing better expressiveness and being multi-modal and asymmetric.\nBesides diversity, it also provides better controllability for probabilistic trajectory generation.\nWe name our method Mixed Gaussian Flow (MGF). It achieves state-of-the-art performance in the evaluation of both trajectory alignment and diversity on the popular UCY/ETH and SDD datasets. Code is available at https://github.com/mulplue/MGF.'}",https://openreview.net{'value': '/pdf/a11044897deff6568a01c74aefb77e5928555379.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=mp6OWpDIJC,{'value': 'Autonomous Agents for Collaborative Task under Information Asymmetry'},Wei Liu; Chenxi Wang; YiFei Wang; Zihao Xie; Rennai Qiu; Yufan Dang; Zhuoyun Du; Weize Chen; Cheng Yang; Chen Qian,~Wei_Liu40; ~Chenxi_Wang6; ~YiFei_Wang13; ~Zihao_Xie1; ~Rennai_Qiu1; ~Yufan_Dang1; ~Zhuoyun_Du1; ~Weize_Chen1; ~Cheng_Yang6; ~Chen_Qian8,"{'value': ['autonomous agent', 'social network', 'large language model']}","{'value': ""Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks. It communicates among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes.""}",https://openreview.net{'value': '/pdf/cbc02f9426edb1ceb1bfcc4517984b4916550c94.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=mljDUaQpln,{'value': 'Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus'},Terufumi Morishita; Gaku Morio; Atsuki Yamaguchi; Yasuhiro Sogawa,~Terufumi_Morishita1; ~Gaku_Morio1; ~Atsuki_Yamaguchi1; ~Yasuhiro_Sogawa1,"{'value': ['large language model', 'artificial intelligence', 'reasoning', 'logical reasoning', 'math', 'coding', 'synthetic corpus']}","{'value': ""Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning.\nTo address this, we propose $\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs' reasoning capabilities by program-generated logical reasoning samples.\nWe first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights.\nThen, based on these principles, we construct a synthetic corpus named $\\textbf{Formal} \\ \\textbf{Logic} \\ \\textbf{\\textit{D}eduction} \\ \\textbf{\\textit{D}iverse}$ (FLD$ _{\\times2}$), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors.\nFinally, we empirically show that ALT on FLD$ _{\\times2}$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B.\nImprovements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH.""}",https://openreview.net{'value': '/pdf/246a03e416453fbc32408c9fdfd22b45f6bbfff0.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=miO8odRzto,{'value': 'Online Relational Inference for Evolving Multi-agent Interacting Systems'},Beomseok Kang; Priyabrata Saha; Sudarshan Sharma; Biswadeep Chakraborty; Saibal Mukhopadhyay,~Beomseok_Kang1; ~Priyabrata_Saha1; ~Sudarshan_Sharma1; ~Biswadeep_Chakraborty1; ~Saibal_Mukhopadhyay2,"{'value': ['Neural Relational Inference', 'Online Learning', 'Multi-agent System']}","{'value': 'We introduce a novel framework, Online Relational Inference (ORI), designed to efficiently identify hidden interaction graphs in evolving multi-agent interacting systems using streaming data. Unlike traditional offline methods that rely on a fixed training set, ORI employs online backpropagation, updating the model with each new data point, thereby allowing it to adapt to changing environments in real-time. A key innovation is the use of an adjacency matrix as a trainable parameter, optimized through a new adaptive learning rate technique called AdaRelation, which adjusts based on the historical sensitivity of the decoder to changes in the interaction graph. Additionally, a data augmentation method named Trajectory Mirror (TM) is introduced to improve generalization by exposing the model to varied trajectory patterns. Experimental results on both synthetic datasets and real-world data (CMU MoCap for human motion) demonstrate that ORI significantly improves the accuracy and adaptability of relational inference in dynamic settings compared to existing methods. This approach is model-agnostic, enabling seamless integration with various neural relational inference (NRI) architectures, and offers a robust solution for real-time applications in complex, evolving systems.'}",https://openreview.net{'value': '/pdf/4fb239ecfecd14053ab49d36e096b20fb212be52.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=mfTvNzhsht,"{'value': 'Dueling over Dessert, Mastering the Art of Repeated Cake Cutting'}",Simina Branzei; MohammadTaghi Hajiaghayi; Reed Phillips; Suho Shin; Kun Wang,~Simina_Branzei1; ~MohammadTaghi_Hajiaghayi1; ~Reed_Phillips1; ~Suho_Shin1; ~Kun_Wang9,"{'value': ['fair division', 'online learning', 'fictitious play', 'repeated games']}","{'value': ""We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice  cuts the cake at a point of her choice, while  Bob  chooses the left piece or the right piece, leaving the remainder for Alice. \nWe consider two versions: sequential, where Bob observes Alice's cut point  before choosing left/right, and simultaneous, where he only observes her cut point after making his choice. The simultaneous version was first considered by Aumann and Maschler.\n \nWe observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.\n\nWe analyze the limits of how much a player can exploit the other one and show that fair utility profiles are in fact achievable. Specifically, the players can enforce  the equitable utility profile  of $(1/2, 1/2)$  in the limit  on every trajectory of play, by keeping the other player's utility to approximately $1/2$ on average while guaranteeing they themselves get at least approximately $1/2$ on average. We show this theorem using a connection with Blackwell approachability.\n\nFinally, we analyze a natural dynamic known as fictitious play, where players best respond to the empirical distribution of the other player. We show that\nfictitious play converges to the equitable utility profile of $(1/2, 1/2)$ at a rate of $O(1/\\sqrt{T})$.""}",https://openreview.net{'value': '/pdf/7f68a3d996ea18de7f300e14ce2c1a0e9687e069.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=mcY221BgKi,{'value': 'Learning Cooperative Trajectory Representations for Motion Forecasting'},Hongzhi Ruan; Haibao Yu; Wenxian Yang; Siqi Fan; Zaiqing Nie,~Hongzhi_Ruan1; ~Haibao_Yu2; ~Wenxian_Yang1; ~Siqi_Fan2; ~Zaiqing_Nie2,"{'value': ['Cooperative Autonomous Driving', 'Motion Forecasting']}","{'value': 'Motion forecasting is an essential task for autonomous driving, and utilizing information from infrastructure and other vehicles can enhance forecasting capabilities.\nExisting research mainly focuses on leveraging single-frame cooperative information to enhance the limited perception capability of the ego vehicle, while underutilizing the motion and interaction context of traffic participants observed from cooperative devices. \nIn this paper, we propose a forecasting-oriented representation paradigm to utilize motion and interaction features from cooperative information. \nSpecifically, we present V2X-Graph, a representative framework to achieve interpretable and end-to-end trajectory feature fusion for cooperative motion forecasting. \nV2X-Graph is evaluated on V2X-Seq in vehicle-to-infrastructure (V2I) scenarios.\nTo further evaluate on vehicle-to-everything (V2X) scenario, we construct the first real-world V2X motion forecasting dataset V2X-Traj, which contains multiple autonomous vehicles and infrastructure in every scenario.\nExperimental results on both V2X-Seq and V2X-Traj show the advantage of our method. \nWe hope both V2X-Graph and V2X-Traj will benefit the further development of cooperative motion forecasting.\nFind the project at https://github.com/AIR-THU/V2X-Graph.'}",https://openreview.net{'value': '/pdf/262f0f4a1a5579270c8d2e1921cd2c8b943c1a59.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=mXpq6ut8J3,{'value': 'SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering'},John Yang; Carlos E Jimenez; Alexander Wettig; Kilian Lieret; Shunyu Yao; Karthik R Narasimhan; Ofir Press,~John_Yang3; ~Carlos_E_Jimenez1; ~Alexander_Wettig1; ~Kilian_Lieret1; ~Shunyu_Yao1; ~Karthik_R_Narasimhan1; ~Ofir_Press1,"{'value': ['Language models', 'Natural language processing', 'Software engineering']}","{'value': ""Language model agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that language model agents represent a new category of end users with their own needs and abilities, and would benefit from specially built interfaces to the software they use. We investigate how the role of interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates language model agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive language models. Finally, we provide insight on how the design of the agent-computer interface can impact agents' behavior and performance.""}",https://openreview.net{'value': '/pdf/7b9425730150fb166d4e6c77995f67ea38638fca.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=mSaqxZVZW8,{'value': 'SeeA*: Efficient Exploration-Enhanced A* Search by Selective Sampling'},Dengwei Zhao; Shikui Tu; Lei Xu,~Dengwei_Zhao1; ~Shikui_Tu1; ~Lei_Xu7,"{'value': ['search algorithm', 'reinforcement learning', 'exploration']}","{'value': 'Monte-Carlo tree search (MCTS) and reinforcement learning contributed crucially to the success of AlphaGo and AlphaZero, and A$^*$ is a tree search algorithm among the most well-known ones in the classical AI literature. MCTS and  A$^*$ both perform heuristic search and are mutually beneficial. Efforts have been made to the renaissance of A$^*$ from three possible aspects, two of which have been confirmed by studies in recent years, while the third is about the OPEN list that consists of open nodes of A$^*$ search, but still lacks deep investigation. This paper aims at the third, i.e., developing the Sampling-exploration enhanced A$^*$ (SeeA$^*$) search by constructing a dynamic subset of OPEN through a selective sampling process, such that the node with the best heuristic value in this subset instead of in the OPEN is expanded. Nodes with the best heuristic values in OPEN are most probably picked into this subset, but sometimes may not be included, which enables SeeA$^*$ to explore other promising branches. Three sampling techniques are presented for comparative investigations. Moreover, under the assumption about the distribution of prediction errors, we have theoretically shown the superior efficiency of SeeA$^*$ over A$^*$ search, particularly when the accuracy of the guiding heuristic function is insufficient. Experimental results on retrosynthetic planning in organic chemistry, logic synthesis in integrated circuit design, and the classical Sokoban game empirically demonstrate the efficiency of SeeA$^*$, in comparison with the state-of-the-art heuristic search algorithms.'}",https://openreview.net{'value': '/pdf/fa5dedfe169ea46edcf332d8d7d9b5256b506793.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=mRIQz8Zd6O,{'value': 'AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents'},Yao Fu; Dong-Ki Kim; Jaekyeom Kim; Sungryull Sohn; Lajanugen Logeswaran; Kyunghoon Bae; Honglak Lee,~Yao_Fu5; ~Dong-Ki_Kim1; ~Jaekyeom_Kim1; ~Sungryull_Sohn1; ~Lajanugen_Logeswaran1; ~Kyunghoon_Bae2; ~Honglak_Lee2,"{'value': ['large language model agents', 'sequential decision-making']}","{'value': ""Recent advances in large language models (LLMs) have empowered AI agents capable of performing various sequential decision-making tasks. However, effectively guiding LLMs to perform well in unfamiliar domains like web navigation, where they lack sufficient knowledge, has proven to be difficult with the demonstration-based in-context learning paradigm. In this paper, we introduce a novel framework, called AutoGuide, which addresses this limitation by automatically generating context-aware guidelines from offline experiences. Importantly, each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context where it is applicable. As a result, our guidelines facilitate the provision of relevant knowledge for the agent's current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm. Our evaluation demonstrates that AutoGuide significantly outperforms competitive baselines in complex benchmark domains, including real-world web navigation.""}",https://openreview.net{'value': '/pdf/72f0fa8a1ec129e7e890cda96edbcc8d2d98f171.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=mP084aMFsd,{'value': 'A Simple yet Scalable Granger Causal Structural Learning Approach for Topological Event Sequences'},Mingjia Li; Shuo Liu; Hong Qian; Aimin Zhou,~Mingjia_Li3; ~Shuo_Liu7; ~Hong_Qian1; ~Aimin_Zhou1,"{'value': ['Telecommunication Network Fault Diagnosis', 'Topological Hawkes Processes', 'Causal structure learning', 'Event Sequences', 'Scalability']}","{'value': 'In modern telecommunication networks, faults manifest as alarms, generating thousands of events daily. Network operators need an efficient method to identify the root causes of these alarms to mitigate potential losses. This task is challenging due to the increasing scale of telecommunication networks and the interconnected nature of devices, where one fault can trigger a cascade of alarms across multiple devices within a topological network. Recent years have seen a growing focus on causal approaches to addressing this problem, emphasizing the importance of learning a Granger causal graph from topological event sequences. Such causal graphs delineate the relations among alarms and can significantly aid engineers in identifying and rectifying faults. However, existing methods either ignore the topological relationships among devices or suffer from relatively low scalability and efficiency, failing to deliver high-quality responses in a timely manner. To this end, this paper proposes $S^2GCSL$, a simple yet scalable Granger causal structural learning approach for topological event sequences. $S^2GCSL$ utilizes a linear kernel to model activation interactions among various event types within a topological network, and employs gradient descent to efficiently optimize the likelihood function. Notably, it can seamlessly incorporate expert knowledge as constraints within the optimization process, which enhances the interpretability of the outcomes. Extensive experimental results on both large-scale synthetic and real-world problems verify the scalability and efficacy of $S^2GCSL$.'}",https://openreview.net{'value': '/pdf/5ffde646c6f78d98a34098818f079bb9ae62df7b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=mJZH9w8qgu,{'value': 'In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before an Ongoing Trajectory Terminates'},Shicheng Liu; Minghui Zhu,~Shicheng_Liu1; ~Minghui_Zhu1,{'value': ['inverse reinforcement learning']},"{'value': 'Inverse reinforcement learning (IRL) aims to learn a reward function and a corresponding policy that best fit the demonstrated trajectories of an expert. However, current IRL works cannot learn incrementally from an ongoing trajectory because they have to wait to collect at least one complete trajectory to learn. To bridge the gap, this paper considers the problem of learning a reward function and a corresponding policy while observing the initial state-action pair of an ongoing trajectory and keeping updating the learned reward and policy when new state-action pairs of the ongoing trajectory are observed. We formulate this problem as an online bi-level optimization problem where the upper level dynamically adjusts the learned reward according to the newly observed state-action pairs with the help of a meta-regularization term, and the lower level learns the corresponding policy. We propose a novel algorithm to solve this problem and guarantee that the algorithm achieves sub-linear local regret $O(\\sqrt{T}+\\log T+\\sqrt{T}\\log T)$. If the reward function is linear, we prove that the proposed algorithm achieves sub-linear regret $O(\\log T)$. Experiments are used to validate the proposed algorithm.'}",https://openreview.net{'value': '/pdf/ea746bec3dcd6ab777a0d7beef69a24679f99785.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=mH1xtt2bJE,{'value': 'MaNo: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts'},RENCHUNZI XIE; Ambroise Odonnat; Vasilii Feofanov; Weijian Deng; Jianfeng Zhang; Bo An,~RENCHUNZI_XIE1; ~Ambroise_Odonnat1; ~Vasilii_Feofanov1; ~Weijian_Deng1; ~Jianfeng_Zhang2; ~Bo_An2,"{'value': ['Unsupervised Learning', 'Distribution Shifts', 'Unsupervised Accuracy Estimation', 'Generalization', 'Deep Learning']}","{'value': ""Leveraging the model’s outputs, specifically the logits, is a common approach to estimating the test accuracy of a pre-trained neural network on out-of-distribution (OOD) samples without requiring access to the corresponding ground-truth labels.\nDespite their ease of implementation and computational efficiency, current logit-based methods are vulnerable to overconfidence issues, leading to prediction bias, especially under the natural shift. In this work, we first study the relationship between logits and generalization performance from the view of low-density separation assumption. Our findings motivate our proposed method \\method{} that \\textbf{(1)}~applies a data-dependent normalization on the logits to reduce prediction bias, and \\textbf{(2)} takes the $L_p$ norm of the matrix of normalized logits as the estimation score. Our theoretical analysis highlights the connection between the provided score and the model's uncertainty. \nWe conduct an extensive empirical study on common unsupervised accuracy estimation benchmarks and demonstrate that \\method{} achieves state-of-the-art performance across various architectures in the presence of synthetic, natural, or subpopulation shifts. The code is available at https://github.com/Renchunzi-Xie/MaNo.""}",https://openreview.net{'value': '/pdf/9631c6bbc3edf891cd13ddecc4321745d5758956.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=m906PS5G9x,{'value': 'Bayesian Adaptive Calibration and Optimal Design'},Rafael Oliveira; Dino Sejdinovic; David Howard; Edwin V. Bonilla,~Rafael_Oliveira1; ~Dino_Sejdinovic1; ~David_Howard1; ~Edwin_V._Bonilla1,"{'value': ['Gaussian processes', 'Bayesian inference', 'variational inference', 'experimental design', 'active learning', 'calibration']}","{'value': 'The process of calibrating computer models of natural phenomena is essential for applications in the physical sciences, where plenty of domain knowledge can be embedded into simulations and then calibrated against real observations. Current machine learning approaches, however, mostly rely on rerunning simulations over a fixed set of designs available in the observed data, potentially neglecting informative correlations across the design space and requiring a large amount of simulations. Instead, we consider the calibration process from the perspective of Bayesian adaptive experimental design and propose a data-efficient algorithm to run maximally informative simulations within a batch-sequential process. At each round, the algorithm jointly estimates the parameters posterior distribution and optimal designs by maximising a variational lower bound of the expected information gain. The simulator is modelled as a sample from a Gaussian process, which allows us to correlate simulations and real data with the unknown calibration parameters. We show the benefits of our method when compared to related approaches across synthetic and real-data problems.'}",https://openreview.net{'value': '/pdf/d3d7dd5ce31a82a73aa12cf2f27678e6ecfb51ae.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=m8MElyzuwp,{'value': 'Fetch and Forge: Efficient Dataset Condensation for Object Detection'},Ding Qi; Jian Li; Jinlong Peng; Bo Zhao; Shuguang Dou; Jialin Li; Jiangning Zhang; Yabiao Wang; Chengjie Wang; Cairong Zhao,~Ding_Qi1; ~Jian_Li12; ~Jinlong_Peng1; ~Bo_Zhao4; ~Shuguang_Dou1; ~Jialin_Li3; ~Jiangning_Zhang1; ~Yabiao_Wang1; ~Chengjie_Wang1; ~Cairong_Zhao2,"{'value': ['Dataset Condensation', 'Object Detection']}","{'value': 'Dataset condensation (DC) is an emerging technique capable of creating compact synthetic datasets from large originals while maintaining considerable performance. It is crucial for accelerating network training and reducing data storage requirements. \nHowever, current research on DC mainly focuses on image classification, with less exploration of object detection.\nThis is primarily due to two challenges: (i) the multitasking nature of object detection complicates the condensation process, and (ii) Object detection datasets are characterized by large-scale and high-resolution data, which are difficult for existing DC methods to handle.\nAs a remedy, we propose DCOD, the first dataset condensation framework for object detection. It operates in two stages: Fetch and Forge, initially storing key localization and classification information into model parameters, and then reconstructing synthetic images via model inversion. \nFor the complex of multiple objects in an image, we propose Foreground Background Decoupling to centrally update the foreground of multiple instances and Incremental PatchExpand to further enhance the diversity of foregrounds.\nExtensive experiments on various detection datasets demonstrate the superiority of DCOD. Even at an extremely low compression rate of 1\\%, we achieve 46.4\\% and 24.7\\% $\\text{AP}_{50}$ on the VOC and COCO, respectively, significantly reducing detector training duration.'}",https://openreview.net{'value': '/pdf/c12d4adfe1c74582a5747a9602fb1d1e0f9cf70c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=liHe9iumIi,{'value': 'FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training'},Ruihong Yin; Vladimir Yugay; Yue Li; Sezer Karaoglu; Theo Gevers,~Ruihong_Yin1; ~Vladimir_Yugay1; ~Yue_Li12; ~Sezer_Karaoglu1; ~Theo_Gevers1,"{'value': ['Few-shot view synthesis', 'gaussian splatting']}","{'value': 'The field of novel view synthesis from images has seen rapid advancements with the introduction of Neural Radiance Fields (NeRF) and more recently with 3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its efficiency and ability to render novel views accurately. While Gaussian Splatting performs well when a sufficient amount of training images are available, its unstructured explicit representation tends to overfit in scenarios with sparse input images, resulting in poor rendering performance. To address this, we present a 3D Gaussian-based novel view synthesis method using sparse input images that can accurately render the scene from the viewpoints not covered by the training images. We propose a multi-stage training scheme with matching-based consistency constraints imposed on the novel views without relying on pre-trained depth estimation or diffusion models. This is achieved by using the matches of the available training images to supervise the generation of the novel views sampled between the training frames with color, geometry, and semantic losses. In addition, we introduce a locality preserving regularization for 3D Gaussians which removes rendering artifacts by preserving the local color structure of the scene. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance of our method in few-shot novel view synthesis compared to existing state-of-the-art methods.'}",https://openreview.net{'value': '/pdf/ed5cc740cd652472f5409c5773b77bab5a4ff3c2.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=lV4kTHTgpJ,{'value': 'Model Fusion through Bayesian Optimization in Language Model Fine-Tuning'},Chaeyun Jang; Hyungi Lee; Jungtaek Kim; Juho Lee,~Chaeyun_Jang1; ~Hyungi_Lee1; ~Jungtaek_Kim1; ~Juho_Lee2,"{'value': ['fine-tuning', 'language model', 'bayesian optimization']}","{'value': 'Fine-tuning pre-trained models for downstream tasks is a widely adopted technique known for its adaptability and reliability across various domains. Despite its conceptual simplicity, fine-tuning entails several troublesome engineering choices, such as selecting hyperparameters and determining checkpoints from an optimization trajectory. To tackle the difficulty of choosing the best model, one effective solution is model fusion, which combines multiple models in a parameter space. However, we observe a large discrepancy between loss and metric landscapes during the fine-tuning of pre-trained language models. Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through multi-objective Bayesian optimization. In addition, to effectively select hyperparameters, we establish a two-stage procedure by integrating Bayesian optimization processes into our framework. Experiments across various downstream tasks show considerable performance improvements using our Bayesian optimization-guided method.'}",https://openreview.net{'value': '/pdf/a0c73d1c1962cefda48e9b0e93288b862aad428a.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=lKnl4CLhhS,{'value': 'Efficient and Private Marginal Reconstruction with Local Non-Negativity'},Brett Mullins; Miguel Fuentes; Yingtai Xiao; Daniel Kifer; Cameron N Musco; Daniel Sheldon,~Brett_Mullins1; ~Miguel_Fuentes1; ~Yingtai_Xiao1; ~Daniel_Kifer1; ~Cameron_N_Musco1; ~Daniel_Sheldon1,"{'value': ['differential privacy', 'query release', 'synthetic data']}","{'value': 'Differential privacy is the dominant standard for formal and quantifiable privacy and has been used in major deployments that impact millions of people. Many differentially private algorithms for query release and synthetic data contain steps that reconstruct answers to queries from answers to other queries that have been measured privately.  Reconstruction is an important subproblem for such mechanisms to economize the privacy budget, minimize error on reconstructed answers, and allow for scalability to high-dimensional datasets. In this paper, we introduce a principled and efficient postprocessing method ReM (Residuals-to-Marginals) for reconstructing answers to marginal queries. Our method builds on recent work on efficient mechanisms for marginal query release, based on making measurements using a residual query basis that admits efficient pseudoinversion, which is an important primitive used in reconstruction. An extension GReM-LNN (Gaussian Residuals-to-Marginals with Local Non-negativity) reconstructs marginals under Gaussian noise satisfying consistency and non-negativity, which often reduces error on reconstructed answers.  We demonstrate the utility of ReM and GReM-LNN by applying them to improve existing private query answering mechanisms.'}",https://openreview.net{'value': '/pdf/74ef2a254d1aef2663edcdb2e0ac71b90a95897e.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=lA48H7pW3q,{'value': 'QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization'},Qi Song; Tianxiang Gong; Shiqi Gao; Haoyi Zhou; Jianxin Li,~Qi_Song9; ~Tianxiang_Gong1; ~Shiqi_Gao2; ~Haoyi_Zhou1; ~Jianxin_Li3,"{'value': ['Contrastive Learning', 'Multi-View Learning', 'Multimodal Learning', 'Vision-Language Representation Degeneration']}","{'value': ""Multimodal contrastive learning (MCL) has recently demonstrated significant success across various tasks. However, the existing MCL treats all negative samples equally and ignores the potential semantic association with positive samples, which limits the model's ability to achieve fine-grained alignment. In multi-view scenarios, MCL tends to prioritize shared information while neglecting modality-specific unique information across different views, leading to feature suppression and suboptimal performance in downstream tasks. To address these limitations, we propose a novel contrastive framework name *QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization*. In the QUEST framework, we propose quaternion contrastive objectives and orthogonal constraints to extract sufficient unique information. Meanwhile, a shared information-guided penalization is introduced to ensure that shared information does not excessively influence the optimization of unique information. Our method leverages quaternion vector spaces to simultaneously optimize shared and unique information. Experiments on multiple datasets show that our method achieves superior performance in multimodal contrastive learning benchmarks. On public benchmark, our approach achieves state-of-the-art performance, and on synthetic shortcut datasets, we outperform existing baseline methods by an average of 97.95\\% on the CLIP model.""}",https://openreview.net{'value': '/pdf/84107019c16a3341189d0a7d6a78e026b2f05c9c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=kqmucDKVcU,{'value': 'Optimal Flow Matching: Learning Straight Trajectories in Just One Step'},Nikita Maksimovich Kornilov; Petr Mokrov; Alexander Gasnikov; Alexander Korotin,~Nikita_Maksimovich_Kornilov1; ~Petr_Mokrov1; ~Alexander_Gasnikov1; ~Alexander_Korotin2,"{'value': ['Flow Matching', 'Optimal Transport', 'Rectified Flow', 'straight trajectories']}","{'value': ""Over the several recent years, there has been a boom in development of Flow Matching (FM) methods for generative modeling. One intriguing property pursued by the community is the ability to learn flows with straight trajectories which realize the Optimal Transport (OT) displacements. Straightness is crucial for the fast integration (inference) of the learned flow's paths. Unfortunately, most existing flow straightening methods are based on non-trivial iterative FM procedures which accumulate the error during training or exploit heuristics based on minibatch OT. To address these issues, we develop and theoretically justify the novel Optimal Flow Matching approach which allows recovering the straight OT displacement for the quadratic transport in just one FM step. The main idea of our approach is the employment of vector field for FM which are parameterized by convex functions. The code of our OFM implementation and the conducted experiments is available at https://github.com/Jhomanik/Optimal-Flow-Matching""}",https://openreview.net{'value': '/pdf/05aa3a9c9c8809ebd4d4d2eedbec48e8773f322c.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=kq166jACVP,{'value': 'Aligner: Efficient Alignment by Learning to Correct'},Jiaming Ji; Boyuan Chen; Hantao Lou; Donghai Hong; Borong Zhang; Xuehai Pan; Tianyi Qiu; Juntao Dai; Yaodong Yang,~Jiaming_Ji2; ~Boyuan_Chen4; ~Hantao_Lou1; ~Donghai_Hong1; ~Borong_Zhang1; ~Xuehai_Pan1; ~Tianyi_Qiu1; ~Juntao_Dai1; ~Yaodong_Yang1,"{'value': ['Large Language Models', 'Alignment', 'Reinforcement Learning from Human Feedback']}","{'value': ""With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9% in helpfulness and 22.8% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%, surpassing GPT-4 Omni's 57.5% Win Rate (community report).""}",https://openreview.net{'value': '/pdf/8413cbc690f0263fff27f69ca2e9ae16dcdb584d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=kkmPe0rzY1,{'value': 'Robust Conformal Prediction Using Privileged Information'},Shai Feldman; Yaniv Romano,~Shai_Feldman1; ~Yaniv_Romano1,"{'value': ['Conformal Prediction', 'Uncertainty Quantification', 'Distribution Shift', 'Corrupted Data', 'Privileged Information']}","{'value': 'We develop a method to generate prediction sets with a guaranteed coverage rate that is robust to corruptions in the training data, such as missing or noisy variables. \nOur approach builds on conformal prediction, a powerful framework to construct prediction sets that are valid under the i.i.d assumption. Importantly, naively applying conformal prediction does not provide reliable predictions in this setting, due to the distribution shift induced by the corruptions. \nTo account for the distribution shift, we assume access to privileged information (PI). The PI is formulated as additional features that explain the distribution shift, however, they are only available during training and absent at test time.\nWe approach this problem by introducing a novel generalization of weighted conformal prediction and support our method with theoretical coverage guarantees. \nEmpirical experiments on both real and synthetic datasets indicate that our approach achieves a valid coverage rate and constructs more informative predictions compared to existing methods, which are not supported by theoretical guarantees.'}",https://openreview.net{'value': '/pdf/b1f57081fd9a0607d9e9969f0b916f0c1795d449.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=kZpNDbZrzy,{'value': 'GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning'},Jaewoo Lee; Sujin Yun; Taeyoung Yun; Jinkyoo Park,~Jaewoo_Lee3; ~Sujin_Yun1; ~Taeyoung_Yun1; ~Jinkyoo_Park1,"{'value': ['Offline Reinforcement Learning', 'Data Augmentation', 'Diffusion Models.']}","{'value': 'Offline Reinforcement Learning (Offline RL) presents challenges of learning effective decision-making policies from static datasets without any online interactions. Data augmentation techniques, such as noise injection and data synthesizing, aim to improve Q-function approximation by smoothing the learned state-action region. However, these methods often fall short of directly improving the quality of offline datasets, leading to suboptimal results. In response, we introduce GTA, Generative Trajectory Augmentation, a novel generative data augmentation approach designed to enrich offline data by augmenting trajectories to be both high-rewarding and dynamically plausible. GTA applies a diffusion model within the data augmentation framework. GTA partially noises original trajectories and then denoises them with classifier-free guidance via conditioning on amplified return value. Our results show that GTA, as a general data augmentation strategy, enhances the performance of widely used offline RL algorithms across various tasks with unique challenges. Furthermore, we conduct a quality analysis of data augmented by GTA and demonstrate that GTA improves the quality of the data. Our code is available at https://github.com/Jaewoopudding/GTA'}",https://openreview.net{'value': '/pdf/0654ab64b73938184f454a5419d4545766a9f5c3.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=kXErlJSZ84,{'value': 'General Detection-based Text Line Recognition'},Raphael Baena; Syrine Kalleli; Mathieu Aubry,~Raphael_Baena1; ~Syrine_Kalleli2; ~Mathieu_Aubry3,"{'value': ['Text Recognition', 'Handwritten Text Recognition', 'Optical Character Recognition', 'Transformer']}","{'value': 'We introduce a general detection-based approach to text line recognition, be it printed (OCR) or handwritten text (HTR), with latin, chinese or ciphered characters. Detection-based approaches have until now largely been discarded for HTR because reading characters separately is often challenging, and character-level annotation is difficult and expensive. We overcome these challenges thanks to three main insights: (i) synthetic pre-training with diverse enough data to learn reasonable character localization in any script; (ii) modern transformer-based detectors can jointly detect a large number of instances and, if trained with an adequate masking strategy, leverage consistency between the different detections; (iii) once a pre-trained detection model with approximate character localization is available, it is possible to fine-tune it with line-level annotation on real data, even with a different alphabet.  Our approach thus builds on a completely different paradigm than most state-of-the-art methods, which rely on autoregressive decoding, predicting character values one by one, while we treat a complete line in parallel. Remarkably, our method demonstrates good performance on range of scripts, usually tackled with specialized approaches: latin script, chinese script, and ciphers, for which we significantly improve state-of-the-art performances. \nOur code and models are available at [https://github.com/raphael-baena/DTLR](https://github.com/raphael-baena/DTLR).'}",https://openreview.net{'value': '/pdf/b8d5175bef8ceea82145cbd46834f32f5fa9df51.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=kVuw8vzsqZ,{'value': 'SkipPredict: When to Invest in Predictions for Scheduling'},Rana Shahout; Michael Mitzenmacher,~Rana_Shahout1; ~Michael_Mitzenmacher1,{'value': ['Algorithms with predictions; scheduling']},"{'value': 'Expanding on recent work on scheduling with predicted job sizes, we consider the effect of the cost of predictions in queueing systems, removing the assumption in prior research that predictions are external to the system’s resources and/or cost-free. Additionally, we introduce a novel approach to utilizing predictions, SkipPredict, designed to address their inherent cost. Rather than uniformly applying predictions to all jobs, we propose a tailored approach that categorizes jobs to improve the effectiveness of prediction on performance. To achieve this, we employ one-bit “cheap predictions” to classify jobs as either short or long. SkipPredict prioritizes predicted short jobs over long jobs, and for the long jobs, SkipPredict applies a second round of more detailed “expensive predictions” to approximate Shortest Remaining Processing Time for these jobs. Importantly, our analyses take into account the cost of prediction. We derive closed-form formulas that calculate the mean response time of jobs with size predictions accounting for the prediction cost. We examine the effect of this cost for two distinct models in real-world and synthetic datasets. In the external cost model, predictions are generated by external method without impacting job service times but incur a cost. In the server time cost model, predictions themselves require server processing time and are scheduled on the same server as the jobs.'}",https://openreview.net{'value': '/pdf/5162380d9c32a0fd2737b61b998c536a3eec2ceb.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=kTtK65vKvD,{'value': 'ODGEN: Domain-specific Object Detection Data Generation with Diffusion Models'},JingYuan Zhu; Shiyu Li; Yuxuan Liu; Jian Yuan; Ping Huang; Jiulong Shan; Huimin Ma,~JingYuan_Zhu1; ~Shiyu_Li2; ~Yuxuan_Liu11; ~Jian_Yuan1; ~Ping_Huang1; ~Jiulong_Shan2; ~Huimin_Ma1,"{'value': ['Object Detection Dataset Generation', 'Complex Scene Synthesis', 'Domain-Specific', 'Diffusion Models']}","{'value': 'Modern diffusion-based image generative models have made significant progress and become promising to enrich training data for the object detection task. However, the generation quality and the controllability for complex scenes containing multi-class objects and dense objects with occlusions remain limited. This paper presents ODGEN, a novel method to generate high-quality images conditioned on bounding boxes, thereby facilitating data synthesis for object detection. Given a domain-specific object detection dataset, we first fine-tune a pre-trained diffusion model on both cropped foreground objects and entire images to fit target distributions. Then we propose to control the diffusion model using synthesized visual prompts with spatial constraints and object-wise textual descriptions. ODGEN exhibits robustness in handling complex scenes and specific domains. Further, we design a dataset synthesis pipeline to evaluate ODGEN on 7 domain-specific benchmarks to demonstrate its effectiveness. Adding training data generated by ODGEN improves up to 25.3% mAP@.50:.95 with object detectors like YOLOv5 and YOLOv7, outperforming prior controllable generative methods. In addition, we design an evaluation protocol based on COCO-2014 to validate ODGEN in general domains and observe an advantage up to 5.6% in mAP@.50:.95 against existing methods.'}",https://openreview.net{'value': '/pdf/43caf5b26317143d806ba6e2ea8d28c2724fe228.pdf'},{'abstract_filter': 'Data Synthesis'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=kQPzFiwVIu,{'value': 'Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource Programming and Formal Languages'},Federico Mora; Justin Wong; Haley Lepe; Sahil Bhatia; Karim Elmaaroufi; George Varghese; Joseph E. Gonzalez; Elizabeth Polgreen; Sanjit A. Seshia,~Federico_Mora1; ~Justin_Wong1; ~Haley_Lepe1; ~Sahil_Bhatia3; ~Karim_Elmaaroufi1; ~George_Varghese1; ~Joseph_E._Gonzalez1; ~Elizabeth_Polgreen2; ~Sanjit_A._Seshia1,"{'value': ['Text-to-Code', 'Low-Resource Programming Languages', 'MAX-SAT', 'Parsing', 'Program Repair']}","{'value': ""Recent advances in large language models (LLMs) for code applications have demonstrated remarkable zero-shot fluency and instruction following on challenging code related tasks ranging from test case generation to self-repair. Unsurprisingly, however, models struggle to compose syntactically valid programs in programming languages unrepresented in pre-training, referred to as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial settings, including domain-specific languages for internal tools, tool-chains for legacy languages, and formal verification frameworks. Inspired by a technique called natural programming elicitation, we propose designing an intermediate language that LLMs ``naturally'' know how to use and which can be automatically compiled to a target VLPL. When LLMs generate code that lies outside of this intermediate language, we use compiler techniques to repair the code into programs in the intermediate language. Overall, we introduce _synthetic programming elicitation and compilation_ (SPEAC), an approach that enables LLMs to generate syntactically valid code even for VLPLs. We empirically evaluate the performance of SPEAC in a case study for the UCLID5 formal verification language and find that, compared to existing retrieval and fine-tuning baselines, SPEAC produces syntactically correct programs more frequently and without sacrificing semantic correctness.""}",https://openreview.net{'value': '/pdf/fb296b40f1d3c4fd79e6e3ea90f07eef6954f0ea.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=kOMrm4ZJ3m,"{'value': 'Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers'}",Alberto Alfarano; Francois Charton; Amaury Hayat,~Alberto_Alfarano1; ~Francois_Charton1; ~Amaury_Hayat1,"{'value': ['mathematics', 'Lyapunov', 'transformers', 'control', 'AI for science', 'AI for maths', 'reasoning']}","{'value': 'Despite their spectacular progress, language models still struggle on complex reasoning tasks, such as advanced mathematics.\nWe consider a long-standing open problem in mathematics: discovering a Lyapunov function that ensures the global stability of a dynamical system. This problem has no known general solution, and algorithmic solvers only exist for some small polynomial systems.\nWe propose a new method for generating synthetic training samples from random solutions, and show that sequence-to-sequence transformers trained on such datasets perform better than algorithmic solvers and humans on polynomial systems, and can discover new Lyapunov functions for non-polynomial systems.'}",https://openreview.net{'value': '/pdf/c03afce8667bfd67eac3dfb7917b2dcdef937e21.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=kEPpD7yETM,{'value': 'Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach'},Weiyu Ma; Qirui Mi; Yongcheng Zeng; Xue Yan; Runji Lin; Yuqiao Wu; Jun Wang; Haifeng Zhang,~Weiyu_Ma1; ~Qirui_Mi1; ~Yongcheng_Zeng1; ~Xue_Yan2; ~Runji_Lin1; ~Yuqiao_Wu1; ~Jun_Wang2; ~Haifeng_Zhang3,"{'value': ['LLM Agent', 'StarCraft2']}","{'value': ""With the continued advancement of Large Language Models (LLMs) Agents in reasoning, planning, and decision-making, benchmarks have become crucial in evaluating these skills. However, there is a notable gap in benchmarks for real-time strategic decision-making. StarCraft II (SC2), with its complex and dynamic nature, serves as an ideal setting for such evaluations. To this end, we have developed TextStarCraft II, a specialized environment for assessing LLMs in real-time strategic scenarios within SC2. Addressing the limitations of traditional Chain of Thought (CoT) methods, we introduce the Chain of Summarization (CoS) method, enhancing LLMs' capabilities in rapid and effective decision-making. Our key experiments included:\n1. LLM Evaluation: Tested 10 LLMs in TextStarCraft II, most of them defeating LV5 build-in AI, showcasing effective strategy skills.\n2. Commercial Model Knowledge: Evaluated four commercial  models on SC2 knowledge; GPT-4 ranked highest by Grandmaster-level experts.\n3. Human-AI Matches: Experimental results showed that fine-tuned LLMs performed on par with Gold-level players in real-time matches, demonstrating comparable strategic abilities.\n\nAll code and data from this\nstudy have been made pulicly available at https://github.com/histmeisah/Large-Language-Models-play-StarCraftII""}",https://openreview.net{'value': '/pdf/f00682bc7e1756fbaf5b9deed1c49567ba4f89a8.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=jZv9A8Tg9p,{'value': 'Data-faithful Feature Attribution: Mitigating Unobservable Confounders via Instrumental Variables'},Qiheng Sun; Haocheng Xia; Jinfei Liu,~Qiheng_Sun1; ~Haocheng_Xia1; ~Jinfei_Liu1,"{'value': ['feature attribution', 'Shapley value', 'integrated gradients']}","{'value': 'The state-of-the-art feature attribution methods often neglect the influence of unobservable confounders, posing a risk of misinterpretation, especially when it is crucial for the interpretation to remain faithful to the data. To counteract this, we propose a new approach, data-faithful feature attribution, which trains a confounder-free model using instrumental variables. The cluttered effects of unobservable confounders in a model trained as such are decoupled from input features, thereby aligning the output of the model with the contribution of input features to the target feature in the data generation. Furthermore, feature attribution results produced by our method are more robust when focusing on attributions from the perspective of data generation. Our experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approaches.'}",https://openreview.net{'value': '/pdf/5914bd018f2ada8fc687520755fcb9ac4ea14a2b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=jXsxGt80sv,{'value': 'Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning'},Hang Zhou; Yehui Tang; Haochen Qin; Yujie Yang; Renren Jin; Deyi Xiong; Kai Han; Yunhe Wang,~Hang_Zhou19; ~Yehui_Tang1; ~Haochen_Qin1; ~Yujie_Yang4; ~Renren_Jin1; ~Deyi_Xiong2; ~Kai_Han2; ~Yunhe_Wang1,"{'value': ['Large language models', 'Data', 'Instruction-Tuning']}","{'value': 'The efficacy of large language models (LLMs) on downstream tasks usually hinges on instruction tuning, which relies critically on the quality of training data. Unfortunately, collecting high-quality and diverse data is both expensive and time-consuming. To mitigate this issue, we propose  a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment. The framework adopts a three-pronged strategy. It  initially generates diverse instruction data with multiple LLM agents through a bespoke sampling method. Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality. Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality. Our empirical studies, including instruction tuning experiments with models such as Pythia and LLaMA, demonstrate the effectiveness of the proposed framework. Optimized datasets have achieved substantial improvements, with an average increase of 12\\% and notable gains in specific metrics, such as a 40\\% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset. Codes will be released soon.'}",https://openreview.net{'value': '/pdf/962b19247201561b296decd84195c9f06e4291be.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=jWaXhCYTV1,{'value': 'Identifying General Mechanism Shifts in Linear Causal Representations'},Tianyu Chen; Kevin Bello; Francesco Locatello; Bryon Aragam; Pradeep Kumar Ravikumar,~Tianyu_Chen3; ~Kevin_Bello1; ~Francesco_Locatello1; ~Bryon_Aragam1; ~Pradeep_Kumar_Ravikumar1,"{'value': ['latent variable modeling', 'distribution shifts', 'causal representation learning', 'heterogeneous data', 'root cause analysis']}","{'value': 'We consider the linear causal representation learning setting where we observe a linear mixing of $d$ unknown latent factors, which follow a linear structural causal model. \nRecent work has shown that it is possible to recover the latent factors as well as the underlying structural causal model over them, up to permutation and scaling, provided that we have at least $d$ environments, each of which corresponds to perfect interventions on a single latent node (factor). \nAfter this powerful result, a key open problem faced by the community has been to relax these conditions: allow for coarser than perfect single-node interventions, and allow for fewer than $d$ of them, since the number of latent factors $d$ could be very large. \nIn this work, we consider precisely such a setting, where we allow a smaller than $d$ number of environments, and also allow for very coarse interventions that can very coarsely \\textit{change the entire causal graph over the latent factors}. \nOn the flip side, we relax what we wish to extract to simply the \\textit{list of nodes that have shifted between one or more environments}. \nWe provide a surprising identifiability result that it is indeed possible, under some very mild standard assumptions, to identify the set of shifted nodes. \nOur identifiability proof moreover is a constructive one: we explicitly provide necessary and sufficient conditions for a node to be a shifted node, and show that we can check these conditions given observed data. \nOur algorithm lends itself very naturally to the sample setting where instead of just interventional distributions, we are provided datasets of samples from each of these distributions. \nWe corroborate our results on both synthetic experiments as well as an interesting psychometric dataset. The code can be found at https://github.com/TianyuCodings/iLCS.'}",https://openreview.net{'value': '/pdf/548aa0f1888d7e062e04466cb9a70a0f79f34ae3.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=jS34QpqdWs,{'value': 'Nonstationary Sparse Spectral Permanental Process'},Zicheng Sun; Yixuan Zhang; Zenan Ling; Xuhui Fan; Feng Zhou,~Zicheng_Sun2; ~Yixuan_Zhang12; ~Zenan_Ling1; ~Xuhui_Fan1; ~Feng_Zhou9,"{'value': ['permanental process', 'nonstationary kernel', 'Bayesian inference']}","{'value': ""Existing permanental processes often impose constraints on kernel types or stationarity, limiting the model's expressiveness. To overcome these limitations, we propose a novel approach utilizing the sparse spectral representation of nonstationary kernels. \nThis technique relaxes the constraints on kernel types and stationarity, allowing for more flexible modeling while reducing computational complexity to the linear level. \nAdditionally, we introduce a deep kernel variant by hierarchically stacking multiple spectral feature mappings, further enhancing the model's expressiveness to capture complex patterns in data. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our approach, particularly in scenarios with pronounced data nonstationarity. Additionally, ablation studies are conducted to provide insights into the impact of various hyperparameters on model performance.""}",https://openreview.net{'value': '/pdf/95d303ea5a03f8c300d97a4825910ab765228036.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=jImXgQEmX3,{'value': 'AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback'},Jian Guan; Wei Wu; zujie wen; Peng Xu; Hongning Wang; Minlie Huang,~Jian_Guan1; ~Wei_Wu1; ~zujie_wen1; ~Peng_Xu12; ~Hongning_Wang1; ~Minlie_Huang1,"{'value': ['Agent', 'Knowledge', 'Feedback-Driven Adaptation']}","{'value': 'The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM)\nthat solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism. The code and data are publicly available at\nhttps://github.com/JianGuanTHU/AMOR.'}",https://openreview.net{'value': '/pdf/d7a7ea80c1688f57e25cfff89de507161f7c86b9.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=j6kJSS9O6I,{'value': 'Agent Planning with World Knowledge Model'},Shuofei Qiao; Runnan Fang; Ningyu Zhang; Yuqi Zhu; Xiang Chen; Shumin Deng; Yong Jiang; Pengjun Xie; Fei Huang; Huajun Chen,~Shuofei_Qiao1; ~Runnan_Fang1; ~Ningyu_Zhang1; ~Yuqi_Zhu2; ~Xiang_Chen5; ~Shumin_Deng1; ~Yong_Jiang1; ~Pengjun_Xie2; ~Fei_Huang2; ~Huajun_Chen1,"{'value': ['world knowledge model', 'agent planning', 'large language models']}","{'value': 'Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the ""real"" physical world. Imitating humans\' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three real-world simulated datasets with Mistral-7B, Gemma-7B, and Llama-3-8B demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent\'s understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development.'}",https://openreview.net{'value': '/pdf/84c350a3368f1d492ac3b0e3c9b168a86a596e03.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=j6Zsoj544N,{'value': 'Does Worst-Performing Agent Lead the Pack? Analyzing Agent Dynamics in Unified Distributed SGD'},Jie Hu; Yi-Ting Ma; Do Young Eun,~Jie_Hu7; ~Yi-Ting_Ma1; ~Do_Young_Eun1,"{'value': ['Distributed Optimization', 'Agent Dynamics', 'Federated Learning', 'Central Limit Theorem', 'Efficient Sampling']}","{'value': 'Distributed learning is essential to train machine learning algorithms across *heterogeneous* agents while maintaining data privacy. We conduct an asymptotic analysis of Unified Distributed SGD (UD-SGD), exploring a variety of communication patterns, including decentralized SGD and local SGD within Federated Learning (FL), as well as the increasing communication interval in the FL setting. In this study, we assess how different sampling strategies, such as *i.i.d.* sampling, shuffling, and Markovian sampling, affect the convergence speed of UD-SGD by considering the impact of agent dynamics on the limiting covariance matrix as described in the Central Limit Theorem (CLT). Our findings not only support existing theories on linear speedup and asymptotic network independence, but also theoretically and empirically show how efficient sampling strategies employed by individual agents contribute to overall convergence in UD-SGD. Simulations reveal that a few agents using highly efficient sampling can achieve or surpass the performance of the majority employing moderately improved strategies, providing new insights beyond traditional analyses focusing on the worst-performing agent.'}",https://openreview.net{'value': '/pdf/98435630526c25e0eeb7c56147d494228f873582.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ivCX2cjwcT,{'value': 'Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures'},Subash Timilsina; Sagar Shrestha; Xiao Fu,~Subash_Timilsina1; ~Sagar_Shrestha1; ~Xiao_Fu1,"{'value': ['Identifiability', 'Multi-modal Learning', 'Canonical Correlation Analysis', 'Shared Component Analysis', 'Independent Component Analysis']}","{'value': 'A core task in multi-modal learning is to integrate information from multiple feature spaces (e.g., text and audio), offering modality-invariant essential representations of data. Recent research showed that, classical tools such as canonical correlation analysis (CCA) provably identify the shared components up to minor ambiguities, when samples in each modality are generated from a linear mixture of shared and private components.  Such identifiability results were obtained under the condition that the cross-modality samples are aligned/paired according to their shared information. This work takes a step further, investigating shared component identifiability from multi-modal linear mixtures where cross-modality samples are unaligned.  A distribution divergence minimization-based loss is proposed, under which a suite of sufficient conditions ensuring identifiability of the shared components are derived. Our conditions are based on cross-modality distribution discrepancy characterization and density-preserving transform removal, which are much milder than existing studies relying on independent component analysis. More relaxed conditions are also provided via adding reasonable structural constraints, motivated by available side information in various applications. The identifiability claims are thoroughly validated using synthetic and real-world data.'}",https://openreview.net{'value': '/pdf/782dd5983e36710970c218a7fd9b39791abee723.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ioKQzb8SMr,{'value': 'Guided Trajectory Generation with Diffusion Models for Offline Model-based Optimization'},Taeyoung Yun; Sujin Yun; Jaewoo Lee; Jinkyoo Park,~Taeyoung_Yun1; ~Sujin_Yun1; ~Jaewoo_Lee3; ~Jinkyoo_Park1,"{'value': ['Offline Model-based Optimization', 'Diffusion Models', 'Decision Making']}","{'value': 'Optimizing complex and high-dimensional black-box functions is ubiquitous in science and engineering fields. Unfortunately, the online evaluation of these functions is restricted due to time and safety constraints in most cases. In offline model-based optimization (MBO), we aim to find a design that maximizes the target function using only a pre-existing offline dataset. While prior methods consider forward or inverse approaches to address the problem, these approaches are limited by conservatism and the difficulty of learning highly multi-modal mappings. Recently, there has been an emerging paradigm of learning to improve solutions with synthetic trajectories constructed from the offline dataset. In this paper, we introduce a novel conditional generative modeling approach to produce trajectories toward high-scoring regions. First, we construct synthetic trajectories toward high-scoring regions using the dataset while injecting locality bias for consistent improvement directions. Then, we train a conditional diffusion model to generate trajectories conditioned on their scores. Lastly, we sample multiple trajectories from the trained model with guidance to explore high-scoring regions beyond the dataset and select high-fidelity designs among generated trajectories with the proxy function. Extensive experiment results demonstrate that our method outperforms competitive baselines on Design-Bench and its practical variants. The code is publicly available in \\url{https://github.com/dbsxodud-11/GTG}.'}",https://openreview.net{'value': '/pdf/1faf7eb1f45eb114876535eca5dd482cf97b705b.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ibKpPabHVn,{'value': 'DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection'},Hongyu Shen; Yici Yan; Zhizhen Zhao,~Hongyu_Shen1; ~Yici_Yan1; ~Zhizhen_Zhao1,"{'value': ['Feature Selection', 'Deep Learning', 'Model-X Knockoff', 'FDR Control', 'Boosting Power']}","{'value': 'Model-X knockoff has garnered significant attention among various feature selection methods due to its guarantees for controlling the false discovery rate (FDR). Since its introduction in parametric design, knockoff techniques have evolved to handle arbitrary data distributions using deep learning-based generative models. However, we have observed limitations in the current implementations of the deep Model-X knockoff framework. Notably, the ""swap property"" that knockoffs require often faces challenges at the sample level, resulting in diminished selection power. To address these issues, we develop ""Deep Dependency Regularized Knockoff (DeepDRK),"" a distribution-free deep learning method that effectively balances FDR and power. In DeepDRK, we introduce a novel formulation of the knockoff model as a learning problem under multi-source adversarial attacks. By employing an innovative perturbation technique, we achieve lower FDR and higher power. Our model outperforms existing benchmarks across synthetic, semi-synthetic, and real-world datasets, particularly when sample sizes are small and data distributions are non-Gaussian.'}",https://openreview.net{'value': '/pdf/7c26b44a5a01c58ec5d40a437fa851e1a8a3887d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=iSjqTQ5S1f,{'value': 'Stochastic Concept Bottleneck Models'},Moritz Vandenhirtz; Sonia Laguna; Ričards Marcinkevičs; Julia E Vogt,~Moritz_Vandenhirtz1; ~Sonia_Laguna1; ~Ričards_Marcinkevičs1; ~Julia_E_Vogt1,"{'value': ['Concept Bottleneck Models', 'Interventions', 'Interpretability', 'Concepts']}","{'value': ""Concept Bottleneck Models (CBMs) have emerged as a promising interpretable method whose final prediction is based on intermediate, human-understandable concepts rather than the raw input. Through time-consuming manual interventions, a user can correct wrongly predicted concept values to enhance the model's downstream performance. We propose *Stochastic Concept Bottleneck Models* (SCBMs), a novel approach that models concept dependencies. In SCBMs, a single-concept intervention affects all correlated concepts, thereby improving intervention effectiveness. Unlike previous approaches that model the concept relations via an autoregressive structure, we introduce an explicit, distributional parameterization that allows SCBMs to retain the CBMs' efficient training and inference procedure. \nAdditionally, we leverage the parameterization to derive an effective intervention strategy based on the confidence region. We show empirically on synthetic tabular and natural image datasets that our approach improves intervention effectiveness significantly. Notably, we showcase the versatility and usability of SCBMs by examining a setting with CLIP-inferred concepts, alleviating the need for manual concept annotations.""}",https://openreview.net{'value': '/pdf/63df37a9a781b39408dfabd90823f0a1ff40b15d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=iM5i289eqt,{'value': 'MaskFactory: Towards High-quality Synthetic Data Generation for Dichotomous Image Segmentation'},Haotian Qian; Yinda Chen; Shengtao Lou; Fahad Khan; Xiaogang Jin; Deng-Ping Fan,~Haotian_Qian2; ~Yinda_Chen1; ~Shengtao_Lou1; ~Fahad_Khan1; ~Xiaogang_Jin1; ~Deng-Ping_Fan1,{'value': ['Dichotomous Image Segmentation; Diffusion Model; Synthetic Data']},"{'value': 'Dichotomous Image Segmentation (DIS) tasks require highly precise annotations, and traditional dataset creation methods are labor intensive, costly, and require extensive domain expertise. Although using synthetic data for DIS is a promising solution to these challenges, current generative models and techniques struggle with the issues of scene deviations, noise-induced errors, and limited training sample variability. To address these issues, we introduce a novel approach, Mask Factory, which provides a scalable solution for generating diverse and precise datasets, markedly reducing preparation time and costs. We first introduce a general mask editing method that combines rigid and non-rigid editing techniques to generate high-quality synthetic masks. Specially, rigid editing leverages geometric priors from diffusion models to achieve precise viewpoint transformations under zero-shot conditions, while non-rigid editing employs adversarial training and self-attention mechanisms for complex, topologically consistent modifications. Then, we generate pairs of high-resolution image and accurate segmentation mask using a multi-conditional control generation method. Finally, our experiments on the widely-used DIS5K dataset benchmark demonstrate superior performance in quality and efficiency compared to existing methods. The code is available at https://qian-hao-tian.github.io/MaskFactory/.'}",https://openreview.net{'value': '/pdf/cef94ab7759097647c0da025836f5f6b25aba95c.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=i5PoejmWoC,{'value': 'Causal language modeling can elicit search and reasoning capabilities on logic puzzles'},Kulin Shah; Nishanth Dikkala; Xin Wang; Rina Panigrahy,~Kulin_Shah1; ~Nishanth_Dikkala1; ~Xin_Wang30; ~Rina_Panigrahy1,"{'value': ['reasoning', 'search', 'planning', 'sudoku', 'world model', 'transformers']}","{'value': 'Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years. However, the extent to which fundamental search and reasoning capabilities emerged within LLMs remains a topic of ongoing debate. In this work, we study if causal language modeling can learn a complex task such as solving Sudoku puzzles. To solve a Sudoku, the model is first required to search over all empty cells of the puzzle to decide on a cell to fill and then apply an appropriate strategy to fill the decided cell. Sometimes, the application of a strategy only results in thinning down the possible values in a cell rather than concluding the exact value of the cell. In such cases, multiple strategies are applied one after the other to fill a single cell. We observe that Transformer models trained on this synthetic task can indeed learn to solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly) when trained on a logical sequence of steps taken by a solver. We find that training Transformers with the logical sequence of steps is necessary and without such training, they fail to learn Sudoku. We also extend our analysis to Zebra puzzles (known as Einstein puzzles) and show that the model solves $92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal representations of the trained Transformer and find that through linear probing, we can decode information about the set of possible values in any given cell from them, pointing to the presence of a strong reasoning engine implicit in the Transformer weights.'}",https://openreview.net{'value': '/pdf/41e4f4d76ff881e14c3d9db05978f9af25ee8739.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=hw76X5uWrc,{'value': 'Unlocking the Potential of Global Human Expertise'},Elliot Meyerson; Olivier Francon; Darren Sargent; Babak Hodjat; Risto Miikkulainen,~Elliot_Meyerson1; ~Olivier_Francon1; ~Darren_Sargent1; ~Babak_Hodjat1; ~Risto_Miikkulainen1,"{'value': ['Human-AI collaboration', 'Evolution', 'Distillation', 'Neural Networks']}","{'value': 'Solving societal problems on a global scale requires the collection and processing of ideas and methods from diverse sets of international experts. As the number and diversity of human experts increase, so does the likelihood that elements in this collective knowledge can be combined and refined to discover novel and better solutions. However, it is difficult to identify, combine, and refine complementary information in an increasingly large and diverse knowledge base. This paper argues that artificial intelligence (AI) can play a crucial role in this process. An evolutionary AI framework, termed RHEA, fills this role by distilling knowledge from diverse models created by human experts into equivalent neural networks, which are then recombined and refined in a population-based search. The framework was implemented in a formal synthetic domain, demonstrating that it is transparent and systematic. It was then applied to the results of the XPRIZE Pandemic Response Challenge, in which over 100 teams of experts across 23 countries submitted models based on diverse methodologies to predict COVID-19 cases and suggest non-pharmaceutical intervention policies for 235 nations, states, and regions across the globe. Building upon this expert knowledge, by recombining and refining the 169 resulting policy suggestion models, RHEA discovered a broader and more effective set of policies than either AI or human experts alone, as evaluated based on real-world data. The results thus suggest that AI can play a crucial role in realizing the potential of human expertise in global problem-solving.'}",https://openreview.net{'value': '/pdf/d1d84a18b92ebedc99a2d5cd308d3fc612bdd322.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=hkBhX5ABjk,{'value': 'Multi-Agent Domain Calibration with a Handful of Offline Data'},Tao Jiang; Lei Yuan; Lihe Li; Cong Guan; Zongzhang Zhang; Yang Yu,~Tao_Jiang21; ~Lei_Yuan2; ~Lihe_Li1; ~Cong_Guan1; ~Zongzhang_Zhang1; ~Yang_Yu5,"{'value': ['Multi-agent reinforcement learning', 'domain transfer']}","{'value': 'The shift in dynamics results in significant performance degradation of policies trained in the source domain when deployed in a different target domain, posing a challenge for the practical application of reinforcement learning (RL) in real-world scenarios. Domain transfer methods aim to bridge this dynamics gap through techniques such as domain adaptation or domain calibration. While domain adaptation involves refining the policy through extensive interactions in the target domain, it may not be feasible for sensitive fields like healthcare and autonomous driving. On the other hand, offline domain calibration utilizes only static data from the target domain to adjust the physics parameters of the source domain (e.g., a simulator) to align with the target dynamics, enabling the direct deployment of the trained policy without sacrificing performance, which emerges as the most promising for policy deployment. However, existing techniques primarily rely on evolution algorithms for calibration, resulting in low sample efficiency.\nTo tackle this issue, we propose a novel framework Madoc (\\textbf{M}ulti-\\textbf{a}gent \\textbf{do}main \\textbf{c}alibration). Firstly, we formulate a bandit RL objective to match the target trajectory distribution by learning a couple of classifiers. We then address the challenge of a large domain parameter space by modeling domain calibration as a cooperative multi-agent reinforcement learning (MARL) problem. Specifically, we utilize a Variational Autoencoder (VAE) to automatically cluster physics parameters with similar effects on the dynamics, grouping them into distinct agents. These grouped agents train calibration policies coordinately to adjust multiple parameters using MARL.\nOur empirical evaluation on 21 offline locomotion tasks in D4RL and NeoRL benchmarks showcases the superior performance of our method compared to strong existing offline model-based RL, offline domain calibration, and hybrid offline-and-online RL baselines.'}",https://openreview.net{'value': '/pdf/5656a2f21c1001a3a55e7683e5a3d1a20e36023e.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=haVPmN8UGi,{'value': 'GraphVis: Boosting LLMs with Visual Knowledge Graph Integration'},Yihe Deng; Chenchen Ye; Zijie Huang; Mingyu Derek Ma; Yiwen Kou; Wei Wang,~Yihe_Deng1; ~Chenchen_Ye1; ~Zijie_Huang1; ~Mingyu_Derek_Ma1; ~Yiwen_Kou1; ~Wei_Wang13,"{'value': ['Large Language Models', 'Knowledge Graphs', 'Multi-modal learning']}","{'value': 'The rapid evolution of large language models (LLMs) has expanded their capabilities across various data modalities, extending from well-established image data to increasingly popular graph data. Given the limitation of LLMs in hallucinations and inaccuracies in recalling factual knowledge, Knowledge Graph (KG) has emerged as a crucial data modality to support more accurate reasoning by LLMs. However, integrating structured knowledge from KGs into LLMs remains challenging, as most current KG-enhanced LLM methods directly convert the KG into linearized text triples, which is not as expressive as the original structured data. To address this, we introduce GraphVis, which conserves the intricate graph structure through the visual modality to enhance the comprehension of KGs with the aid of Large Vision Language Models (LVLMs). Our approach incorporates a unique curriculum fine-tuning scheme which first instructs LVLMs to recognize basic graphical features from the images, and subsequently incorporates reasoning on QA tasks with the visual graphs. This cross-modal methodology not only markedly enhances performance on standard textual QA  but also shows improved zero-shot VQA performance by utilizing synthetic graph images to augment the data for VQA tasks. We present comprehensive evaluations across commonsense reasoning QA benchmarks, where GraphVis provides an average improvement of 11.1% over its base model and outperforms existing KG-enhanced LLM approaches. Across VQA benchmarks such as ScienceQA that share similar scientific diagram images, GraphVis provides a notable gain of 4.32%.'}",https://openreview.net{'value': '/pdf/7d08a4d64e3320c9ee44a10af5b807e89f67f526.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=haUnEiXgQ7,{'value': 'Vision-Language Models are Strong Noisy Label Detectors'},Tong Wei; Hao-Tian Li; Chun-Shu Li; Jiang-Xin Shi; Yu-Feng Li; Min-Ling Zhang,~Tong_Wei1; ~Hao-Tian_Li1; ~Chun-Shu_Li1; ~Jiang-Xin_Shi1; ~Yu-Feng_Li1; ~Min-Ling_Zhang2,"{'value': ['label-noise learning', 'sample selection', 'semi-supervised learning']}","{'value': 'Recent research on fine-tuning vision-language models has demonstrated impressive performance in various downstream tasks. However, the challenge of obtaining accurately labeled data in real-world applications poses a significant obstacle during the fine-tuning process. To address this challenge, this paper presents a Denoising Fine-Tuning framework, called DeFT, for adapting vision-language models. DeFT utilizes the robust alignment of textual and visual features pre-trained on millions of auxiliary image-text pairs to sieve out noisy labels. The proposed framework establishes a noisy label detector by learning positive and negative textual prompts for each class. The positive prompt seeks to reveal distinctive features of the class, while the negative prompt serves as a learnable threshold for separating clean and noisy samples. We employ parameter-efficient fine-tuning for the adaptation of a pre-trained visual encoder to promote its alignment with the learned textual prompts. As a general framework, DeFT can seamlessly fine-tune many pre-trained models to downstream tasks by utilizing carefully selected clean samples. Experimental results on seven synthetic and real-world noisy datasets validate the effectiveness of DeFT in both noisy label detection and image classification. Our source code can be found in the supplementary material.'}",https://openreview.net{'value': '/pdf/e975c820e2aa3edef48543d06a949c9b16ddabb6.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=hYMxyeyEc5,{'value': 'Embedding Trajectory for Out-of-Distribution Detection in Mathematical Reasoning'},Yiming Wang; Pei Zhang; Baosong Yang; Derek F. Wong; Zhuosheng Zhang; Rui Wang,~Yiming_Wang13; ~Pei_Zhang7; ~Baosong_Yang1; ~Derek_F._Wong1; ~Zhuosheng_Zhang1; ~Rui_Wang10,"{'value': ['Out-of-Distribution Detection', 'Mathematical Reasoning', 'Generative Language Models']}","{'value': 'Real-world data deviating from the independent and identically distributed (\\textit{i.i.d.}) assumption of in-distribution training data poses security threats to deep networks, thus advancing out-of-distribution (OOD) detection algorithms. Detection methods in generative language models (GLMs) mainly focus on uncertainty estimation and embedding distance measurement, with the latter proven to be most effective in traditional linguistic tasks like summarization and translation. However, another complex generative scenario mathematical reasoning poses significant challenges to embedding-based methods due to its high-density feature of output spaces, but this feature causes larger discrepancies in the embedding shift trajectory between different samples in latent spaces. Hence, we propose a trajectory-based method TV score, which uses trajectory volatility for OOD detection in mathematical reasoning. Experiments show that our method outperforms all traditional algorithms on GLMs under mathematical reasoning scenarios and can be extended to more applications with high-density features in output spaces, such as multiple-choice questions.'}",https://openreview.net{'value': '/pdf/ad87ad9e99c156ad5c72623e35594aa5e2971d1d.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=hT4y7D2o2T,{'value': 'Propensity Score Alignment of Unpaired Multimodal Data'},Johnny Xi; Jana Osea; Zuheng Xu; Jason Hartford,~Johnny_Xi1; ~Jana_Osea2; ~Zuheng_Xu1; ~Jason_Hartford1,"{'value': ['unpaired data', 'multimodal', 'causal representations', 'propensity score']}","{'value': ""Multimodal representation learning techniques typically require paired samples to learn shared representations, but collecting paired samples can be challenging in fields like biology, where measurement devices often destroy the samples. This paper presents an approach to address the challenge of aligning unpaired samples across disparate modalities in multimodal representation learning. We draw an analogy between potential outcomes in causal inference and potential views in multimodal observations, allowing us to leverage Rubin's framework to estimate a common space for matching samples. Our approach assumes experimentally perturbed samples by treatments, and uses this to estimate a propensity score from each modality. We show that the propensity score encapsulates all shared information between a latent state and treatment, and can be used to define a distance between samples. We experiment with two alignment techniques that leverage this distance---shared nearest neighbours (SNN) and optimal transport (OT) matching---and find that OT matching results in significant improvements over state-of-the-art alignment approaches in on synthetic multi-modal tasks, in real-world data from NeurIPS Multimodal Single-Cell Integration Challenge, and on a single cell microscopy to expression prediction task.""}",https://openreview.net{'value': '/pdf/e33e66092d51fb20f53ddbc85a231d7c32b7525d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=hRKsahifqj,{'value': 'Autoregressive Policy Optimization for Constrained Allocation Tasks'},David Winkel; Niklas Alexander Strauß; Maximilian Bernhard; Zongyue Li; Thomas Seidl; Matthias Schubert,~David_Winkel1; ~Niklas_Alexander_Strauß1; ~Maximilian_Bernhard1; ~Zongyue_Li1; ~Thomas_Seidl2; ~Matthias_Schubert1,"{'value': ['Reinforcement learning', 'Constraint Reinforcement Learning', 'Allocation Tasks']}","{'value': 'Allocation tasks represent a class of problems where a limited amount of resources must be allocated to a set of entities at each time step. Prominent examples of this task include portfolio optimization or distributing computational workloads across servers.\nAllocation tasks are typically bound by linear constraints describing practical requirements that have to be strictly fulfilled at all times. In portfolio optimization, for example, investors may be obligated to allocate less than 30\\% of the funds into a certain industrial sector in any investment period. \nSuch constraints restrict the action space of allowed allocations in intricate ways, which makes learning a policy that avoids constraint violations difficult.\nIn this paper, we propose a new method for constrained allocation tasks based on an autoregressive process to sequentially sample allocations for each entity. In addition, we introduce a novel de-biasing mechanism to counter the initial bias caused by sequential sampling. We demonstrate the superior performance of our approach compared to a variety of Constrained Reinforcement Learning (CRL) methods on three distinct constrained allocation tasks: portfolio optimization, computational workload distribution, and a synthetic allocation benchmark. Our code is available at: https://github.com/niklasdbs/paspo'}",https://openreview.net{'value': '/pdf/b6ea57a7f451dcfa6dbec839b2d5bfec73b40588.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=h1grUs6CjN,{'value': 'The Price of Implicit Bias in Adversarially Robust Generalization'},Nikolaos Tsilivis; Natalie Frank; Nathan Srebro; Julia Kempe,~Nikolaos_Tsilivis1; ~Natalie_Frank1; ~Nathan_Srebro1; ~Julia_Kempe1,"{'value': ['Adversarial Robustness', 'Robust Generalization Gap', 'Implicit Bias', 'Optimisation', 'Generalization']}","{'value': 'We study the implicit bias of optimization in robust empirical risk minimization (robust ERM) and its connection with robust generalization.  In classification settings under adversarial perturbations with linear models, we study what type of regularization should ideally be applied for a given perturbation set to improve (robust) generalization. We then show that the implicit bias of optimization in robust ERM can significantly affect the robustness of the model and identify two ways this can happen; either through the optimization algorithm or the architecture. We verify our predictions in simulations with synthetic data and experimentally study the importance of implicit bias in robust ERM with deep neural networks.'}",https://openreview.net{'value': '/pdf/2255e765990fba590854b8c61ee521569a083a47.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=gzQARCgIsI,{'value': 'End-To-End Causal Effect Estimation from Unstructured Natural Language Data'},Nikita Dhawan; Leonardo Cotta; Karen Ullrich; Rahul Krishnan; Chris J. Maddison,~Nikita_Dhawan1; ~Leonardo_Cotta1; ~Karen_Ullrich1; ~Rahul_G_Krishnan1; ~Chris_J._Maddison1,"{'value': ['treatment effect estimation', 'unstructured reports', 'real-world effects']}","{'value': 'Knowing the effect of an intervention is critical for human decision-making, but current approaches for causal effect estimation rely on manual data collection and structuring, regardless of the causal assumptions. This increases both the cost and time-to-completion for studies. We show how large, diverse observational text data can be mined with large language models (LLMs) to produce inexpensive causal effect estimates under appropriate causal assumptions. We introduce _NATURAL_, a novel family of causal effect estimators built with LLMs that operate over datasets of unstructured text. Our estimators use LLM conditional distributions (over variables of interest, given the text data) to assist in the computation of classical estimators of causal effect. We overcome a number of technical challenges to realize this idea, such as automating data curation and using LLMs to impute missing information. We prepare six (two synthetic and four real) observational datasets, paired with corresponding ground truth in the form of randomized trials, which we used to systematically evaluate each step of our pipeline. NATURAL estimators demonstrate remarkable performance, yielding causal effect estimates that fall within 3 percentage points of their ground truth counterparts, including on real-world Phase 3/4 clinical trials. Our results suggest that unstructured text data is a rich source of causal effect information, and NATURAL is a first step towards an automated pipeline to tap this resource.'}",https://openreview.net{'value': '/pdf/3b51533646d3d910f744e6b3f9388df0f917b423.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=gVTkMsaaGI,"{'value': 'Amortizing intractable inference in diffusion models for vision, language, and control'}",Siddarth Venkatraman; Moksh Jain; Luca Scimeca; Minsu Kim; Marcin Sendera; Mohsin Hasan; Luke Rowe; Sarthak Mittal; Pablo Lemos; Emmanuel Bengio; Alexandre Adam; Jarrid Rector-Brooks; Yoshua Bengio; Glen Berseth; Nikolay Malkin,~Siddarth_Venkatraman1; ~Moksh_Jain1; ~Luca_Scimeca1; ~Minsu_Kim2; ~Marcin_Sendera1; ~Mohsin_Hasan1; ~Luke_Rowe1; ~Sarthak_Mittal1; ~Pablo_Lemos1; ~Emmanuel_Bengio1; ~Alexandre_Adam1; ~Jarrid_Rector-Brooks2; ~Yoshua_Bengio1; ~Glen_Berseth1; ~Nikolay_Malkin1,"{'value': ['diffusion', 'inverse problems', 'conditional generation', 'language models', 'infilling', 'discrete diffusion', 'offline RL', 'planning', 'GFlowNet']}","{'value': 'Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies *amortized* sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\rm post}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$, in a model that consists of a diffusion generative model prior $p(\\mathbf{x})$ and a black-box constraint or likelihood function $r(\\mathbf{x})$. We state and prove the asymptotic correctness of a data-free learning objective, *relative trajectory balance*, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases. Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. Experiments illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors: in vision (classifier guidance), language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning. Code is available at [this link](https://github.com/GFNOrg/diffusion-finetuning).'}",https://openreview.net{'value': '/pdf/ec12f5f82914f460c8fdd9a06e22a48dfdf5cc47.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=g7lYP11Erv,{'value': 'Point-PRC: A Prompt Learning Based Regulation Framework for Generalizable Point Cloud Analysis'},Hongyu Sun; Qiuhong Ke; Yongcai Wang; Wang Chen; Kang Yang; Deying Li; Jianfei Cai,~Hongyu_Sun3; ~Qiuhong_Ke6; ~Yongcai_Wang1; ~Wang_Chen5; ~Kang_Yang7; ~Deying_Li1; ~Jianfei_Cai1,"{'value': ['point cloud analysis', 'point cloud recognition', '3d domain generalization', 'large 3d models', 'prompt learning', 'multi-modal learning']}","{'value': 'This paper investigates the 3D domain generalization (3DDG) ability of large 3D models based on prevalent prompt learning. Recent works demonstrate the performances of 3D point cloud recognition can be boosted remarkably by parameter-efficient prompt tuning. However, we observe that the improvement on downstream tasks comes at the expense of a severe drop in 3D domain generalization. To resolve this challenge, we present a comprehensive regulation framework that allows the learnable prompts to actively interact with the well-learned general knowledge in large 3D models to maintain good generalization. Specifically, the proposed framework imposes multiple explicit constraints on the prompt learning trajectory by maximizing the mutual agreement between task-specific predictions and task-agnostic knowledge. We design the regulation framework as a plug-and-play module to embed into existing representative large 3D models. Surprisingly, our method not only realizes consistently increasing generalization ability but also enhances task-specific 3D recognition performances across various 3DDG benchmarks by a clear margin. Considering the lack of study and evaluation on 3DDG, we also create three new benchmarks, namely base-to-new, cross-dataset and few-shot generalization benchmarks, to enrich the field and inspire future research. Code and benchmarks are available at \\url{https://github.com/auniquesun/Point-PRC}.'}",https://openreview.net{'value': '/pdf/e0d6cb3656e2f14317d478623c6fd3b39022dfb8.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=g0G8DQSBcj,{'value': 'GFlowNet Assisted Biological Sequence Editing'},Pouya M. Ghari; Alex M Tseng; Gökcen Eraslan; Romain Lopez; Tommaso Biancalani; Gabriele Scalia; Ehsan Hajiramezanali,~Pouya_M._Ghari1; ~Alex_M_Tseng1; ~Gökcen_Eraslan1; ~Romain_Lopez1; ~Tommaso_Biancalani1; ~Gabriele_Scalia1; ~Ehsan_Hajiramezanali1,"{'value': ['GFlowNet', 'Sequence Editing']}","{'value': 'Editing biological sequences has extensive applications in synthetic biology and medicine, such as designing regulatory elements for nucleic-acid therapeutics and treating genetic disorders. The primary objective in biological-sequence editing is to determine the optimal modifications to a sequence which augment certain biological properties while adhering to a minimal number of alterations to ensure predictability and potentially support safety. In this paper, we propose GFNSeqEditor, a novel biological-sequence editing algorithm which builds on the recently proposed area of generative flow networks (GFlowNets). Our proposed GFNSeqEditor identifies elements within a starting seed sequence that may compromise a desired biological property. Then, using a learned stochastic policy, the algorithm makes edits at these identified locations, offering diverse modifications for each sequence to enhance the desired property. The number of edits can be regulated through specific hyperparameters. We conducted extensive experiments on a range of real-world datasets and biological applications, and our results underscore the superior performance of our proposed algorithm compared to existing state-of-the-art sequence editing methods.'}",https://openreview.net{'value': '/pdf/f5f941a8812634855c58ff43457bbced4473dc66.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=fi3aKVnBQo,{'value': 'Efficient Leverage Score Sampling for Tensor Train Decomposition'},Vivek Bharadwaj; Beheshteh T. Rakhshan; Osman Asif Malik; Guillaume Rabusseau,~Vivek_Bharadwaj1; ~Beheshteh_T._Rakhshan1; ~Osman_Asif_Malik1; ~Guillaume_Rabusseau1,"{'value': ['leverage score sampling', 'tensor train decomposition', 'alternating least square']}","{'value': 'Tensor Train~(TT) decomposition is widely used in the machine learning and quantum physics communities as a popular tool to efficiently compress high-dimensional tensor data. In this paper, we propose an efficient algorithm to accelerate computing the TT decomposition with the Alternating Least Squares (ALS) algorithm relying on exact leverage scores sampling. For this purpose, we propose a data structure that allows us to efficiently sample from the tensor with time complexity logarithmic in the product of the tensor dimensions. Our contribution specifically leverages the canonical form of the TT decomposition. By maintaining the canonical form through each iteration of ALS, we can efficiently compute (and sample from) the leverage scores, thus achieving significant speed-up in solving each sketched least-square problem. Experiments on synthetic and real data on dense and sparse tensors demonstrate that our method outperforms SVD-based and ALS-based algorithms.'}",https://openreview.net{'value': '/pdf/f81f3a44cb73d91e225b1a4c92fd78f70d61cc71.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=fYfliutfHX,{'value': 'Learning predictable and robust neural representations by straightening image sequences'},Xueyan Niu; Cristina Savin; Eero P Simoncelli,~Xueyan_Niu2; ~Cristina_Savin1; ~Eero_P_Simoncelli1,{'value': ['straightening; prediction; self-supervised learning; robustness']},"{'value': 'Prediction is a fundamental capability of all living organisms, and has been proposed as an objective for learning sensory representations.  Recent work demonstrates that in primate visual systems, prediction is facilitated by neural representations that follow straighter temporal trajectories than their initial photoreceptor encoding, which allows for prediction by linear extrapolation. Inspired by these experimental findings, we develop a self-supervised learning (SSL) objective that explicitly quantifies and promotes straightening. We demonstrate the power of this objective in training deep feedforward neural networks on smoothly-rendered synthetic image sequences that mimic commonly-occurring properties of natural videos. The learned model contains neural embeddings that are predictive, but also factorize the geometric, photometric, and semantic attributes of objects. The representations also prove more robust to noise and adversarial attacks compared to previous SSL methods that optimize for invariance to random augmentations. Moreover, these beneficial properties can be transferred to other training procedures by using the straightening objective as a regularizer, suggesting a broader utility for straightening as a principle for robust unsupervised learning.'}",https://openreview.net{'value': '/pdf/f5b470ff1dc4da3a27a2912f97f4d554afb236f4.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=fUBFy8tb3z,{'value': 'TrajCLIP: Pedestrian trajectory prediction method using contrastive learning and idempotent networks'},Pengfei Yao; Yinglong Zhu; Huikun Bi; Tianlu Mao; Zhaoqi Wang,~Pengfei_Yao1; ~Yinglong_Zhu1; ~Huikun_Bi1; ~Tianlu_Mao1; ~Zhaoqi_Wang2,"{'value': ['trajectory prediction', 'constrative learning']}","{'value': 'The distribution of pedestrian trajectories is highly complex and influenced by the scene, nearby pedestrians, and subjective intentions. This complexity presents challenges for modeling and generalizing trajectory prediction. Previous methods modeled the feature space of future trajectories based on the high-dimensional feature space of historical trajectories, but this approach is suboptimal because it overlooks the similarity between historical and future trajectories. Our proposed method, TrajCLIP, utilizes contrastive learning and idempotent generative networks to address this issue. By pairing historical and future trajectories and applying contrastive learning on the encoded feature space, we enforce same-space consistency constraints. To manage complex distributions, we use idempotent loss and tightness loss to control over-expansion in the latent space. Additionally, we have developed a trajectory interpolation algorithm and synthetic trajectory data to enhance model capacity and improve generalization. Experimental results on public datasets demonstrate that TrajCLIP achieves state-of-the-art performance and excels in scene-to-scene transfer, few-shot transfer, and online learning tasks.'}",https://openreview.net{'value': '/pdf/4a7d843ef5132b33c713a622055fd3bf52621e15.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=fNoleQa9RX,{'value': 'The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better'},Scott Geng; Cheng-Yu Hsieh; Vivek Ramanujan; Matthew Wallingford; Chun-Liang Li; Pang Wei Koh; Ranjay Krishna,~Scott_Geng1; ~Cheng-Yu_Hsieh1; ~Vivek_Ramanujan1; ~Matthew_Wallingford1; ~Chun-Liang_Li1; ~Pang_Wei_Koh1; ~Ranjay_Krishna1,"{'value': ['Synthetic training data', 'task adaptation', 'data-centric machine learning']}","{'value': 'Generative text-to-image models enable us to synthesize unlimited amounts of images in a controllable manner, spurring many recent efforts to train vision models with synthetic data. However, every synthetic image ultimately originates from the upstream data used to train the generator. Does the intermediate generator provide additional information over directly training on relevant parts of the upstream data? \nGrounding this question in the setting of image classification, we compare finetuning on task-relevant, targeted synthetic data generated by Stable Diffusion---a generative model trained on the LAION-2B dataset---against finetuning on targeted real images retrieved directly from LAION-2B. We show that while synthetic data can benefit some downstream tasks, it is universally matched or outperformed by real data from the simple retrieval baseline. Our analysis suggests that this underperformance is partially due to generator artifacts and inaccurate task-relevant visual details in the synthetic images. Overall, we argue that targeted retrieval is a critical baseline to consider when training with synthetic data---a baseline that current methods do not yet surpass. We release code, data, and models at [https://github.com/scottgeng00/unmet-promise/](https://github.com/scottgeng00/unmet-promise).'}",https://openreview.net{'value': '/pdf/c2284c545059ec27c6dcfc2ba711727798963d58.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=fNakQltI1N,{'value': 'Trajectory Flow Matching with Applications to Clinical Time Series Modelling'},Xi Zhang; Yuan Pu; Yuki Kawamura; Andrew Loza; Yoshua Bengio; Dennis Shung; Alexander Tong,~Xi_Zhang18; ~Yuan_Pu4; ~Yuki_Kawamura1; ~Andrew_Loza1; ~Yoshua_Bengio1; ~Dennis_Shung1; ~Alexander_Tong1,"{'value': ['Flow matching', 'stochastic differential equations', 'ODE', 'SDE', 'uncertainty', 'time series', 'EHR']}","{'value': 'Modeling stochastic and irregularly sampled time series is a challenging problem found in a wide range of applications, especially in medicine. Neural stochastic differential equations (Neural SDEs) are an attractive modeling technique for this problem, which parameterize the drift and diffusion terms of an SDE with neural networks. However, current algorithms for training Neural SDEs require backpropagation through the SDE dynamics, greatly limiting their scalability and stability. \nTo address this, we propose **Trajectory Flow Matching** (TFM), which trains a Neural SDE in a *simulation-free* manner, bypassing backpropagation through the dynamics. TFM leverages the flow matching technique from generative modeling to model time series. In this work we first establish necessary conditions for TFM to learn time series data. Next, we present a reparameterization trick which improves training stability. Finally, we adapt TFM to the clinical time series setting, demonstrating improved performance on four clinical time series datasets both in terms of absolute performance and uncertainty prediction, a crucial parameter in this setting.'}",https://openreview.net{'value': '/pdf/eccb1e85d348547475213eddd7bd8ddeea27f351.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=fMdrBucZnj,{'value': 'Expected Probabilistic Hierarchies'},Marcel Kollovieh; Bertrand Charpentier; Daniel Zügner; Stephan Günnemann,~Marcel_Kollovieh1; ~Bertrand_Charpentier2; ~Daniel_Zügner1; ~Stephan_Günnemann1,"{'value': ['hierarchical clustering', 'graph clustering', 'clustering', 'unsupervised learning', 'probabilistic models']}","{'value': 'Hierarchical clustering has usually been addressed by discrete optimization using heuristics or continuous optimization of relaxed scores for hierarchies. In this work, we propose to optimize expected scores under a probabilistic model over hierarchies. (1) We show theoretically that the global optimal values of the expected Dasgupta cost and Tree-Sampling divergence (TSD), two unsupervised metrics for hierarchical clustering, are equal to the optimal values of their discrete counterparts contrary to some relaxed scores. (2) We propose Expected Probabilistic Hierarchies (EPH), a probabilistic model to learn hierarchies in data by optimizing expected scores. EPH uses differentiable hierarchy sampling enabling end-to-end gradient descent based optimization, and an unbiased subgraph sampling approach to scale to large datasets. (3) We evaluate EPH on synthetic and real-world datasets including vector and graph datasets. EPH outperforms all other approaches quantitatively and provides meaningful hierarchies in qualitative evaluations.'}",https://openreview.net{'value': '/pdf/f424494fa1670685589cceb271f11ce6a586050b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=fMWrTAe5Iy,{'value': 'R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction'},Ruyi Zha; Tao Jun Lin; Yuanhao Cai; Jiwen Cao; Yanhao Zhang; Hongdong Li,~Ruyi_Zha1; ~Tao_Jun_Lin1; ~Yuanhao_Cai1; ~Jiwen_Cao1; ~Yanhao_Zhang4; ~Hongdong_Li1,"{'value': ['3D Gaussian Splatting', '3D Reconstruction', 'CT Reconstruction', 'Tomographic Reconstruction']}","{'value': '3D Gaussian splatting (3DGS) has shown promising results in image rendering and surface reconstruction. However, its potential in volumetric reconstruction tasks, such as X-ray computed tomography, remains under-explored. This paper introduces R$^2$-Gaussian, the first 3DGS-based framework for sparse-view tomographic reconstruction. By carefully deriving X-ray rasterization functions, we discover a previously unknown \\emph{integration bias} in the standard 3DGS formulation, which hampers accurate volume retrieval. To address this issue, we propose a novel rectification technique via refactoring the projection from 3D to 2D Gaussians. Our new method presents three key innovations: (1) introducing tailored Gaussian kernels, (2) extending rasterization to X-ray imaging, and (3) developing a CUDA-based differentiable voxelizer. Experiments on synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art approaches in accuracy and efficiency. Crucially, it delivers high-quality results in 4 minutes, which is 12$\\times$ faster than NeRF-based methods and on par with traditional algorithms.'}",https://openreview.net{'value': '/pdf/e43faafc86ce94cf84611b5f4dec0417d18daa24.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=fE3RqiF4Nx,{'value': 'Metric Flow Matching for Smooth Interpolations on the Data Manifold'},Kacper Kapusniak; Peter Potaptchik; Teodora Reu; Leo Zhang; Alexander Tong; Michael M. Bronstein; Joey Bose; Francesco Di Giovanni,~Kacper_Kapusniak1; ~Peter_Potaptchik1; ~Teodora_Reu1; ~Leo_Zhang1; ~Alexander_Tong1; ~Michael_M._Bronstein1; ~Joey_Bose1; ~Francesco_Di_Giovanni1,{'value': ['Flow Matching; Riemannian Geometry; single-cell RNA sequencing;']},"{'value': 'Matching objectives underpin the success of modern generative models and rely on constructing conditional paths that transform a source distribution into a target distribution. Despite being a fundamental building block, conditional paths have been designed principally under the assumption of $\\textit{Euclidean geometry}$, resulting in straight interpolations. However, this can be particularly restrictive for tasks such as trajectory inference, where straight paths might lie outside the data manifold, thus failing to capture the underlying dynamics giving rise to the observed marginals. In this paper, we propose Metric Flow Matching (MFM), a novel simulation-free framework for conditional flow matching where interpolants are approximate geodesics learned by minimizing the kinetic energy of a data-induced Riemannian metric. This way, the generative model matches vector fields on the data manifold, which corresponds to lower uncertainty and more meaningful interpolations. We prescribe general metrics to instantiate MFM, independent of the task, and test it on a suite of challenging problems including LiDAR navigation, unpaired image translation, and modeling cellular dynamics. We observe that MFM outperforms the Euclidean baselines, particularly achieving SOTA on single-cell trajectory prediction.'}",https://openreview.net{'value': '/pdf/5fd21860d4ce3d4b1c290c2ac4b4e68d2b82e3a7.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=fC2SV2sQ8J,{'value': 'LaKD: Length-agnostic Knowledge Distillation for Trajectory Prediction with Any Length Observations'},Yuhang Li; Changsheng Li; Ruilin Lv; Rongqing Li; Ye Yuan; Guoren Wang,~Yuhang_Li5; ~Changsheng_Li4; ~Ruilin_Lv2; ~Rongqing_Li1; ~Ye_Yuan15; ~Guoren_Wang1,{'value': ['Trajectory Prediction']},"{'value': 'Trajectory prediction is a crucial technology to help systems avoid traffic accidents, ensuring safe autonomous driving. Previous methods typically use a fixed-length and sufficiently long trajectory of an agent as observations to predict its future trajectory. However, in real-world scenarios, we often lack the time to gather enough trajectory points before making predictions, e.g., when a car suddenly appears due to an obstruction, the system must make immediate predictions to prevent a collision. This poses a new challenge for trajectory prediction systems, requiring them to be capable of making accurate predictions based on observed trajectories of arbitrary lengths, leading to the failure of existing methods. In this paper, we propose a Length-agnostic Knowledge Distillation framework, named LaKD,  which can make accurate trajectory predictions, regardless of the length of observed data. Specifically, considering the fact that long trajectories, containing richer temporal information but potentially additional interference, may perform better or worse than short trajectories, we devise a dynamic length-agnostic knowledge distillation mechanism for exchanging information among trajectories of arbitrary lengths, dynamically determining the transfer direction based on prediction performance. In contrast to traditional knowledge distillation, LaKD employs a unique model that simultaneously serves as both the teacher and the student, potentially causing knowledge collision during the distillation process. Therefore, we design a dynamic soft-masking mechanism, where we first calculate the importance of neuron units and then apply soft-masking to them, so as to safeguard critical units from disruption during the knowledge distillation process. In essence, LaKD is a general and principled framework that can be naturally compatible with existing trajectory prediction models of different architectures. Extensive experiments on three benchmark datasets, Argoverse 1, nuScenes and Argoverse 2, demonstrate the effectiveness of our approach.'}",https://openreview.net{'value': '/pdf/7cf0348cc3747c46278bb98d27d152a16c5722d3.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=f4v7cmm5sC,{'value': 'Foundation Inference Models for Markov Jump Processes'},David Berghaus; Kostadin Cvejoski; Patrick Seifner; Cesar Ojeda; Ramses J Sanchez,~David_Berghaus1; ~Kostadin_Cvejoski1; ~Patrick_Seifner1; ~Cesar_Ojeda1; ~Ramses_J_Sanchez1,"{'value': ['Zero-shot inference', 'Markov jump process', 'Inference of Markov processes', 'Foundation models', 'Foundation models for time series', 'time series']}","{'value': 'Markov jump processes are continuous-time stochastic processes which describe dynamical systems evolving in discrete state spaces. These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial. In this work we introduce a methodology for *zero-shot inference* of Markov jump processes (MJPs), on bounded state spaces, from noisy and sparse observations, which consists of two components. First, a broad probability distribution over families of MJPs, as well as over possible observation times and noise mechanisms, with which we simulate a synthetic dataset of hidden MJPs and their noisy observations. Second, a neural recognition model that processes subsets of the simulated observations, and that is trained to output the initial condition and rate matrix of the target MJP in a supervised way. We empirically demonstrate that *one and the same* (pretrained) recognition model can infer, *in a zero-shot fashion*, hidden MJPs evolving in state spaces of different dimensionalities. Specifically, we infer MJPs which describe (i) discrete flashing ratchet systems, which are a type of Brownian motors, and the conformational dynamics in (ii) molecular simulations, (iii) experimental ion channel data and (iv) simple protein folding models. What is more, we show that our model performs on par with state-of-the-art models which are trained on the target datasets.\n\nOur pretrained model is available online.'}",https://openreview.net{'value': '/pdf/bd723bf66a821d98f129f165535ca22d06223f32.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=eygv0JRvTL,{'value': 'Bayesian Optimisation with Unknown Hyperparameters: Regret Bounds Logarithmically Closer to Optimal'},Juliusz Ziomek; Masaki Adachi; Michael A Osborne,~Juliusz_Ziomek1; ~Masaki_Adachi1; ~Michael_A_Osborne1,{'value': ['Bayesian Optimisation; Unknown Hyperparameters; Regret Bounds']},"{'value': 'Bayesian Optimization (BO) is widely used for optimising black-box functions but requires us to specify the length scale hyperparameter, which defines the smoothness of the functions the optimizer will consider. Most current BO algorithms choose this hyperparameter by maximizing the marginal likelihood of the observed data, albeit risking misspecification if the objective function is less smooth in regions we have not yet explored. The only prior solution addressing this problem with theoretical guarantees was A-GP-UCB, proposed by Berkenkamp et al. (2019). This algorithm progressively decreases the length scale, expanding the class of functions considered by the optimizer. However, A-GP-UCB lacks a stopping mechanism, leading to over-exploration and slow convergence. To overcome this, we introduce Length scale Balancing (LB) - a novel approach, aggregating multiple base surrogate models with varying length scales. LB intermittently adds smaller length scale candidate values while retaining longer scales, balancing exploration and exploitation. We formally derive a cumulative regret bound of LB and compare it with the regret of an oracle BO algorithm using the optimal length scale. Denoting the factor by which the regret bound of A-GP-UCB was away from oracle as $g(T)$,  we show that LB is only $\\log g(T)$ away from oracle regret. We also empirically evaluate our algorithm on synthetic and real-world benchmarks and show it outperforms A-GP-UCB and maximum likelihood estimation.'}",https://openreview.net{'value': '/pdf/1cbaa5673cd6f85aaf7654654eacd40354912740.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=euQ0C4iS7O,{'value': 'Leveraging Drift to Improve Sample Complexity of Variance Exploding Diffusion Models'},Ruofeng Yang; Zhijie Wang; Bo Jiang; Shuai Li,~Ruofeng_Yang1; ~Zhijie_Wang7; ~Bo_Jiang2; ~Shuai_Li3,"{'value': ['Variance exploding diffusion models', 'Convergence guarantee', 'Manifold hypothesis']}","{'value': 'Variance exploding (VE) based diffusion models, an important class of diffusion models, have shown state-of-the-art (SOTA) performance. However, only a few theoretical works analyze VE-based models, and those works suffer from a worse forward convergence rate $1/\\text{poly}(T)$ than the $\\exp{(-T)}$ of variance preserving (VP) based models, where $T$ is the forward diffusion time and the rate measures the distance between forward marginal distribution $q_T$ and pure Gaussian noise. The slow rate is due to the Brownian Motion without a drift term. In this work, we design a new drifted VESDE forward process, which allows a faster $\\exp{(-T)}$ forward convergence rate. With this process, we achieve the first efficient polynomial sample complexity for a series of VE-based models with reverse SDE under the manifold hypothesis. Furthermore, unlike previous works, we allow the diffusion coefficient to be unbounded instead of a constant, which is closer to the SOTA models. Besides the reverse SDE, the other common reverse process is the probability flow ODE  (PFODE) process, which is deterministic and enjoys faster sample speed. To deepen the understanding of VE-based models, we consider a more general setting considering reverse SDE and PFODE simultaneously, propose a unified tangent-based analysis framework, and prove the first quantitative convergence guarantee for SOTA VE-based models with reverse PFODE.\nWe also show that the drifted VESDE can balance different error terms and improve generated samples without training through synthetic and real-world experiments.'}",https://openreview.net{'value': '/pdf/05ca69f4daef20ab2561b65902f0cc6bb096148c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=edOZifvwMi,{'value': 'CryoGEM: Physics-Informed Generative Cryo-Electron Microscopy'},Jiakai Zhang; Qihe Chen; Yan Zeng; Wenyuan Gao; Xuming He; Zhijie Liu; Jingyi Yu,~Jiakai_Zhang3; ~Qihe_Chen2; ~Yan_Zeng3; ~Wenyuan_Gao1; ~Xuming_He3; ~Zhijie_Liu3; ~Jingyi_Yu5,"{'value': ['Image Synthesis', 'Contrastive Learning', 'Cryo-EM']}","{'value': 'In the past decade, deep conditional generative models have revolutionized the generation of realistic images, extending their application from entertainment to scientific domains. Single-particle cryo-electron microscopy (cryo-EM) is crucial in resolving near-atomic resolution 3D structures of proteins, such as the SARS-COV-2 spike protein. To achieve high-resolution reconstruction, a comprehensive data processing pipeline has been adopted. However, its performance is still limited as it lacks high-quality annotated datasets for training. To address this, we introduce physics-informed generative cryo-electron microscopy (CryoGEM), which for the first time integrates physics-based cryo-EM simulation with a generative unpaired noise translation to generate physically correct synthetic cryo-EM datasets with realistic noises. Initially, CryoGEM simulates the cryo-EM imaging process based on a virtual specimen. To generate realistic noises, we leverage an unpaired noise translation via contrastive learning with a novel mask-guided sampling scheme. Extensive experiments show that CryoGEM is capable of generating authentic cryo-EM images. The generated dataset can be used as training data for particle picking and pose estimation models, eventually improving the reconstruction resolution.'}",https://openreview.net{'value': '/pdf/28c9b2115b82c3166b55e72a5bbf65dc9b1611fc.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=eUcyIe1AzY,{'value': 'Generating Origin-Destination Matrices in Neural Spatial Interaction Models'},Ioannis Zachos; Mark Girolami; Theodoros Damoulas,~Ioannis_Zachos1; ~Mark_Girolami2; ~Theodoros_Damoulas1,"{'value': ['multiagent systems', 'neural differential equations', 'contingency tables', 'agent-based modelling', 'markov bases', 'origin-destination matrix']}","{'value': ""Agent-based models (ABMs) are proliferating as decision-making tools across policy areas in transportation, economics, and epidemiology. In these models, a central object of interest is the discrete origin-destination matrix which captures spatial interactions and agent trip counts between locations. Existing approaches resort to continuous approximations of this matrix and subsequent ad-hoc discretisations in order to perform ABM simulation and calibration. This impedes conditioning on partially observed summary statistics, fails to explore the multimodal matrix distribution over a discrete combinatorial support, and incurs discretisation errors. To address these challenges, we introduce a computationally efficient framework that scales linearly with the number of origin-destination pairs, operates directly on the discrete combinatorial space, and learns the agents' trip intensity through a neural differential equation that embeds spatial interactions. Our approach outperforms the prior art in terms of reconstruction error and ground truth matrix coverage, at a fraction of the computational cost. We demonstrate these benefits in two large-scale spatial mobility ABMs in Washington, DC and Cambridge, UK.""}",https://openreview.net{'value': '/pdf/8e735e93cb0b6c6fcf1c75cf112bb552c25422f1.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=eKHQbgvL3G,{'value': 'TrackIME: Enhanced Video Point Tracking via Instance Motion Estimation'},Seong Hyeon Park; Huiwon Jang; Byungwoo Jeon; Sukmin Yun; Paul Hongsuck Seo; Jinwoo Shin,~Seong_Hyeon_Park2; ~Huiwon_Jang1; ~Byungwoo_Jeon1; ~Sukmin_Yun1; ~Paul_Hongsuck_Seo1; ~Jinwoo_Shin1,"{'value': ['motion estimation', 'point tracking', 'video segmentation']}","{'value': 'Tracking points in video frames is essential for understanding video content. However, the task is fundamentally hindered by the computation demands for brute-force correspondence matching across the frames. As the current models down-sample the frame resolutions to mitigate this challenge, they fall short in accurately representing point trajectories due to information truncation. Instead, we address the challenge by pruning the search space for point tracking and let the model process only the important regions of the frames without down-sampling. Our first key idea is to identify the object instance and its trajectory over the frames, then prune the regions of the frame that do not contain the instance. Concretely, to estimate the instance’s trajectory, we track a group of points on the instance and aggregate their motion trajectories. Furthermore, to deal with the occlusions in complex scenes, we propose to compensate for the occluded points while tracking. To this end, we introduce a unified framework that jointly performs point tracking and segmentation, providing synergistic effects between the two tasks. For example, the segmentation results enable a tracking model to avoid the occluded points referring to the instance mask, and conversely, the improved tracking results can help to produce more accurate segmentation masks. Our framework can be easily incorporated with various tracking models, and we demonstrate its efficacy for enhanced point tracking throughout extensive experiments. For example, on the recent TAP-Vid benchmark, our framework consistently improves all baselines, e.g., up to 13.5% improvement on the average Jaccard metric.'}",https://openreview.net{'value': '/pdf/a1d1099a9b6d3563c77c9a4445e17803dde50b7d.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=eFrdRuyHR9,{'value': 'Transition Constrained Bayesian Optimization via Markov Decision Processes'},Jose Pablo Folch; Calvin Tsay; Robert Matthew Lee; Behrang Shafei; Weronika Ormaniec; Andreas Krause; Mark van der Wilk; Ruth Misener; Mojmir Mutny,~Jose_Pablo_Folch1; ~Calvin_Tsay1; ~Robert_Matthew_Lee1; ~Behrang_Shafei1; ~Weronika_Ormaniec1; ~Andreas_Krause1; ~Mark_van_der_Wilk1; ~Ruth_Misener1; ~Mojmir_Mutny1,"{'value': ['Bayesian Optimization', 'Transition Constrained', 'Markov Decision Process', 'Linear Bandits', 'Convex Reinforcement Learning']}","{'value': 'Bayesian optimization is a methodology to optimize black-box functions. Traditionally, it focuses on the setting where you can arbitrarily query the search space. However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones. Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements. Altogether, such *transition constraints* necessitate a form of planning. This work extends classical Bayesian optimization via the framework of Markov Decision Processes. We iteratively solve a tractable linearization of our utility function using reinforcement learning to obtain a policy that plans ahead for the entire horizon. This is a parallel to the optimization of an *acquisition function in policy space*. The resulting policy is potentially history-dependent and non-Markovian. We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other synthetic examples.'}",https://openreview.net{'value': '/pdf/e77f2d206e13d3123dc2e70844f5f0cef5d37cf2.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=eDNslSwQIj,{'value': 'Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image Diffusion Models'},Ziyi Wu; Yulia Rubanova; Rishabh Kabra; Drew A. Hudson; Igor Gilitschenski; Yusuf Aytar; Sjoerd van Steenkiste; Kelsey R Allen; Thomas Kipf,~Ziyi_Wu1; ~Yulia_Rubanova2; ~Rishabh_Kabra1; ~Drew_Arad_Hudson1; ~Igor_Gilitschenski1; ~Yusuf_Aytar1; ~Sjoerd_van_Steenkiste1; ~Kelsey_R_Allen1; ~Thomas_Kipf2,"{'value': ['Controllable generation', '3D-aware editing', 'diffusion model']}","{'value': 'We address the problem of multi-object 3D pose control in image diffusion models. Instead of conditioning on a sequence of text tokens, we propose to use a set of per-object representations, *Neural Assets*, to control the 3D pose of individual objects in a scene. Neural Assets are obtained by pooling visual representations of objects from a reference image, such as a frame in a video, and are trained to reconstruct the respective objects in a different image, e.g., a later frame in the video. Importantly, we encode object visuals from the reference image while conditioning on object poses from the target frame, which enables learning disentangled appearance and position features. Combining visual and 3D pose representations in a sequence-of-tokens format allows us to keep the text-to-image interface of existing models, with Neural Assets in place of text tokens. By fine-tuning a pre-trained text-to-image diffusion model with this information, our approach enables fine-grained 3D pose and placement control of individual objects in a scene. We further demonstrate that Neural Assets can be transferred and recomposed across different scenes. Our model achieves state-of-the-art multi-object editing results on both synthetic 3D scene datasets, as well as two real-world video datasets (Objectron, Waymo Open).'}",https://openreview.net{'value': '/pdf/4a9f84cb77a4ab3e0fb3d4d67cdb5cd778cad514.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=dsMSWUBN8f,{'value': 'Not so griddy: Internal representations of RNNs path integrating more than one agent'},William T Redman; Francisco Acosta; Santiago Acosta-Mendoza; Nina Miolane,~William_T_Redman1; ~Francisco_Acosta1; ~Santiago_Acosta-Mendoza1; ~Nina_Miolane2,"{'value': ['grid cells', 'path integration', 'recurrent neural networks', 'normative models']}","{'value': ""Success in collaborative and competitive environments, where agents must work with or against each other, requires individuals to encode the position and trajectory of themselves and others. Decades of neurophysiological experiments have shed light on how brain regions [e.g., medial entorhinal cortex (MEC), hippocampus] encode the self's position and trajectory. However, it has only recently been discovered that MEC and hippocampus are modulated by the positions and trajectories of others. To understand how encoding spatial information of multiple agents shapes neural representations, we train a recurrent neural network (RNN) model that captures properties of MEC to path integrate trajectories of two agents simultaneously navigating the same environment. We find significant differences between these RNNs and those trained to path integrate only a single agent. At the individual unit level, RNNs trained to path integrate more than one agent develop weaker grid responses, stronger border responses, and tuning for the relative position of the two agents. At the population level, they develop more distributed and robust representations, with changes in network dynamics and manifold topology. Our results provide testable predictions and open new directions with which to study the neural computations supporting spatial navigation.""}",https://openreview.net{'value': '/pdf/0f3ace1ce6e51e26c53ba9af87b1a29671d318fd.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=dqT9MC5NQl,{'value': 'Approximately Equivariant Neural Processes'},Matthew Ashman; Cristiana Diaconu; Adrian Weller; Wessel P Bruinsma; Richard E. Turner,~Matthew_Ashman1; ~Cristiana_Diaconu1; ~Adrian_Weller1; ~Wessel_P_Bruinsma1; ~Richard_E_Turner1,"{'value': ['equivariance', 'neural processes', 'meta learning', 'deep learning', 'probabilistic methods']}","{'value': 'Equivariant deep learning architectures exploit symmetries in learning problems to improve the sample efficiency of neural-network-based models and their ability to generalise. However, when modelling real-world data, learning problems are often not *exactly* equivariant, but only approximately. For example, when estimating the global temperature field from weather station observations, local topographical features like mountains break translation equivariance. In these scenarios, it is desirable to construct architectures that can flexibly depart from exact equivariance in a data-driven way. Current approaches to achieving this cannot usually be applied out-of-the-box to any architecture and symmetry group. In this paper, we develop a general approach to achieving this using existing equivariant architectures. Our approach is agnostic to both the choice of symmetry group and model architecture, making it widely applicable. We consider the use of approximately equivariant architectures in neural processes (NPs), a popular family of meta-learning models. We demonstrate the effectiveness of our approach on a number of synthetic and real-world regression experiments, showing that approximately equivariant NP models can outperform both their non-equivariant and strictly equivariant counterparts.'}",https://openreview.net{'value': '/pdf/f0f4114abbc20ec680d32131908aedb9e89e876f.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=diYnEYUbIU,{'value': 'Geometric Exploitation for Indoor Panoramic Semantic Segmentation'},Duc Cao Dinh; Seok Joon Kim; Kyusung Cho,~Duc_Cao_Dinh1; ~Seok_Joon_Kim2; ~Kyusung_Cho1,"{'value': ['indoor panoramic semantic segmentation', 'vertical relative distance']}","{'value': 'PAnoramic Semantic Segmentation (PASS) is an important task in computer vision,\nas it enables semantic understanding of a 360° environment. Currently,\nmost of existing works have focused on addressing the distortion issues in 2D\npanoramic images without considering spatial properties of indoor scene. This\nrestricts PASS methods in perceiving contextual attributes to deal with the ambiguity\nwhen working with monocular images. In this paper, we propose a novel\napproach for indoor panoramic semantic segmentation. Unlike previous works,\nwe consider the panoramic image as a composition of segment groups: oversampled\nsegments, representing planar structures such as floors and ceilings, and\nunder-sampled segments, representing other scene elements. To optimize each\ngroup, we first enhance over-sampled segments by jointly optimizing with a dense\ndepth estimation task. Then, we introduce a transformer-based context module\nthat aggregates different geometric representations of the scene, combined\nwith a simple high-resolution branch, it serves as a robust hybrid decoder for\nestimating under-sampled segments, effectively preserving the resolution of predicted\nmasks while leveraging various indoor geometric properties. Experimental\nresults on both real-world (Stanford2D3DS, Matterport3D) and synthetic (Structured3D)\ndatasets demonstrate the robustness of our framework, by setting new\nstate-of-the-arts in almost evaluations, The code and updated results are available\nat: https://github.com/caodinhduc/vertical_relative_distance.'}",https://openreview.net{'value': '/pdf/7856c603005c8a8d55d0cf6ead3c1b11e25f40ba.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=da0ZJatRCN,{'value': 'Active Learning for Derivative-Based Global Sensitivity Analysis with Gaussian Processes'},Syrine Belakaria; Benjamin Letham; Jana Doppa; Barbara E Engelhardt; Stefano Ermon; Eytan Bakshy,~Syrine_Belakaria1; ~Benjamin_Letham1; ~Jana_Doppa1; ~Barbara_Engelhardt1; ~Stefano_Ermon1; ~Eytan_Bakshy1,"{'value': ['Global Sensitivity Analysis', 'Gaussian Processes', 'Bayesian Active Learning', 'Bayesian optimization']}","{'value': 'We consider the problem of active learning for global sensitivity analysis of expensive black-box functions. Our aim is to efficiently learn the importance of different input variables, e.g., in vehicle safety experimentation, we study the impact of the thickness of various components on safety objectives. Since function evaluations are expensive, we use active learning to prioritize experimental resources where they yield the most value. We propose novel active learning acquisition functions that directly target key quantities of derivative-based global sensitivity measures (DGSMs) under Gaussian process surrogate models.\nWe showcase the first application of active learning directly to DGSMs, and develop tractable uncertainty reduction and information gain acquisition functions for these measures. Through comprehensive evaluation on synthetic and real-world problems, our study demonstrates how these active learning acquisition strategies substantially enhance the sample efficiency of DGSM estimation, particularly with limited evaluation budgets. Our work paves the way for more efficient and accurate sensitivity analysis in various scientific and engineering applications.'}",https://openreview.net{'value': '/pdf/e7cbe6f405410f66193a63a86f7ddaae0e3eb870.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=dG1HwKMYbC,{'value': 'FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making'},Yangyang Yu; Zhiyuan Yao; Haohang Li; Zhiyang Deng; Yuechen Jiang; Yupeng Cao; Zhi Chen; Jordan W. Suchow; Zhenyu Cui; Rong Liu; Zhaozhuo Xu; Denghui Zhang; Koduvayur Subbalakshmi; GUOJUN XIONG; Yueru He; Jimin Huang; Dong Li; Qianqian Xie,~Yangyang_Yu1; ~Zhiyuan_Yao2; ~Haohang_Li1; ~Zhiyang_Deng1; ~Yuechen_Jiang1; ~Yupeng_Cao1; ~Zhi_Chen10; ~Jordan_W._Suchow1; ~Zhenyu_Cui2; ~Rong_Liu4; ~Zhaozhuo_Xu2; ~Denghui_Zhang2; ~Koduvayur_Subbalakshmi1; ~GUOJUN_XIONG1; ~Yueru_He1; ~Jimin_Huang1; ~Dong_Li27; ~Qianqian_Xie1,"{'value': ['Multi-agent system', 'Financial Large Language Models', 'Portfolio Management']}","{'value': 'Large language models (LLMs) have demonstrated notable potential in conducting complex tasks and are increasingly utilized in various financial applications. However, high-quality sequential financial investment decision-making remains challenging. These tasks require multiple interactions with a volatile environment for every decision, demanding sufficient intelligence to maximize returns and manage risks. Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-source information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored. Here, we introduce FinCon, an LLM-based multi-agent framework tailored for diverse financial tasks. Inspired by effective real-world investment firm organizational structures, FinCon utilizes a manager-analyst communication hierarchy. This structure allows for synchronized cross-functional agent collaboration towards unified goals through natural language interactions and equips each agent with greater memory capacity than humans. Additionally, a risk-control component in FinCon enhances decision quality by episodically initiating a self-critiquing mechanism to update systematic investment beliefs. The conceptualized beliefs serve as verbal reinforcement for the future agent’s behavior and can be selectively propagated to the appropriate node that requires knowledge updates. This feature significantly improves performance while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon demonstrates strong generalization capabilities in various financial tasks, including stock trading and portfolio management.'}",https://openreview.net{'value': '/pdf/43c84cdf07e573ff26b3563297197c1d442c4ca0.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=dB99jjwx3h,{'value': 'Learning Linear Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity'},Jikai Jin; Vasilis Syrgkanis,~Jikai_Jin1; ~Vasilis_Syrgkanis1,{'value': ['causal representation learning; causal inference']},"{'value': 'We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we consider the task of learning causal representation learning with data collected from general environments. We show that even when the causal model and the mixing function are both linear, there exists a surrounded-node ambiguity (SNA) [Varici et al. 2023] which is basically unavoidable in our setting. On the other hand, in the same linear case, we show that identification up to SNA is possible under mild conditions, and propose an algorithm, LiNGCReL which provably achieves such identifiability guarantee. We conduct extensive experiments on synthetic data and demonstrate the effectiveness of LiNGCReL in the finite-sample regime.'}",https://openreview.net{'value': '/pdf/74d27afd65951da6e78b44c5d96eec4742bbfc47.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=d5cKDHCrFJ,{'value': 'EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular Data Classification via Large Language Models'},Jinhee Kim; Taesung Kim; Jaegul Choo,~Jinhee_Kim1; ~Taesung_Kim1; ~Jaegul_Choo1,"{'value': ['Large language model', 'In-context learning', 'Few-shot learning', 'Class imbalance', 'Tabular data', 'Synthetic data generation']}","{'value': 'Large language models (LLMs) have demonstrated remarkable in-context learning capabilities across diverse applications. In this work, we explore the effectiveness of LLMs for generating realistic synthetic tabular data, identifying key prompt design elements to optimize performance. We introduce EPIC, a novel approach that leverages balanced, grouped data samples and consistent formatting with unique variable mapping to guide LLMs in generating accurate synthetic data across all classes, even for imbalanced datasets. Evaluations on real-world datasets show that EPIC achieves state-of-the-art machine learning classification performance, significantly improving generation efficiency. These findings highlight the effectiveness of EPIC for synthetic tabular data generation, particularly in addressing class imbalance.'}",https://openreview.net{'value': '/pdf/993b80f99bd2179b8b299bdabe98002e15b69b5c.pdf'},{'title_filter': 'Data Synthesis'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=cyv0LkIaoH,{'value': 'Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences'},Damien Ferbach; Quentin Bertrand; Joey Bose; Gauthier Gidel,~Damien_Ferbach1; ~Quentin_Bertrand1; ~Joey_Bose1; ~Gauthier_Gidel1,"{'value': ['retraining', 'curating', 'generative model', 'self-consuming']}","{'value': ""The rapid progress in generative models has resulted in impressive leaps in generation quality, blurring the lines between synthetic and real data. Web-scale datasets are now prone to the inevitable contamination by synthetic data, directly impacting the training of future generated models.\n    Already, some theoretical results on self-consuming generative models (a.k.a., iterative retraining) have emerged in the literature, showcasing that either model collapse or stability could be possible depending on the fraction of generated data used at each retraining step.\n    However, in practice, synthetic data is often subject to human feedback and curated by users before being used and uploaded online. For instance, many interfaces of popular text-to-image generative models, such as Stable Diffusion or Midjourney, produce several variations of an image for a given query which can eventually be curated by the users.\n    In this paper, we theoretically study the impact of data curation on iterated retraining of generative models and show that it can be seen as an implicit preference optimization mechanism. However, unlike standard preference optimization, the generative model does not have access to the reward function or negative samples needed for pairwise comparisons. Moreover, our study doesn't require access to the density function, only to samples. We prove that, if the data is curated according to a reward model, then the expected reward of the iterative retraining procedure is maximized. We further provide theoretical results on the stability of the retraining loop when using a positive fraction of real data at each step. Finally, we conduct illustrative experiments on both synthetic datasets and on CIFAR10 showing that such a procedure amplifies biases of the reward model.""}",https://openreview.net{'value': '/pdf/f07d0b33e2273d7fc20cee68f1443d6346c4b922.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=cjH0Qsgd0D,{'value': 'Learning Macroscopic Dynamics from Partial Microscopic Observations'},Mengyi Chen; Qianxiao Li,~Mengyi_Chen1; ~Qianxiao_Li1,"{'value': ['Dynamical system', 'Neural closure model', 'Macroscopic dynamics', 'Partial observation']}","{'value': 'Macroscopic observables of a system are of keen interest in real applications such as the design of novel materials. Current methods rely on microscopic trajectory simulations, where the forces on all microscopic coordinates need to be computed or measured. However,  this can be computationally prohibitive for realistic systems.  In this paper, we propose a method to learn macroscopic dynamics requiring only force computations on a subset of the microscopic coordinates. Our method relies on a sparsity assumption: the force on each microscopic coordinate relies only on a small number of other coordinates. The main idea of our approach is to map the training procedure on the macroscopic coordinates back to the microscopic coordinates, on which partial force computations can be used as stochastic estimation to update model parameters. We provide a theoretical justification of this under suitable conditions. We demonstrate the accuracy, force computation efficiency, and robustness of our method on learning macroscopic closure models from a variety of microscopic systems, including those modeled by partial differential equations or molecular dynamics simulations.'}",https://openreview.net{'value': '/pdf/2db6fc544ef42e53babf9bfa418f11db2da537a6.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=cfrDLD1wfO,{'value': 'Graph Diffusion Transformers for Multi-Conditional Molecular Generation'},Gang Liu; Jiaxin Xu; Tengfei Luo; Meng Jiang,~Gang_Liu6; ~Jiaxin_Xu1; ~Tengfei_Luo1; ~Meng_Jiang3,"{'value': ['Graph Diffusion Transformers', 'Inverse Molecular Design', 'Molecular Generation', 'Polymer Generation']}","{'value': 'Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We present the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation. Graph DiT has a condition encoder to learn the representation of numerical and categorical properties and utilizes a Transformer-based graph denoiser to achieve molecular graph denoising under conditions. Unlike previous graph diffusion models that add noise separately on the atoms and bonds in the forward diffusion process, we propose a graph-dependent noise model for training Graph DiT, designed to accurately estimate graph-related noise in molecules. We extensively validate the Graph DiT for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution learning to condition control for molecular properties. A polymer inverse design task for gas separation with feedback from domain experts further demonstrates its practical utility. The code is available at https://github.com/liugangcode/Graph-DiT.'}",https://openreview.net{'value': '/pdf/46c02e1bf7e313ee41cca4c78d39825812de8c3d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ceIO1w0PmT,{'value': 'OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents'},Zihao Wang; Shaofei Cai; Zhancun Mu; Haowei Lin; Ceyao Zhang; Xuejie Liu; Qing Li; Anji Liu; Xiaojian Ma; Yitao Liang,~Zihao_Wang23; ~Shaofei_Cai2; ~Zhancun_Mu1; ~Haowei_Lin1; ~Ceyao_Zhang1; ~Xuejie_Liu2; ~Qing_Li1; ~Anji_Liu1; ~Xiaojian_Ma1; ~Yitao_Liang1,"{'value': ['open-world', 'multimodal language model', 'decision making', 'generalist agents']}","{'value': 'This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model for open-world instruction-following agents in Minecraft. Compared to prior works that either emit textual goals to separate controllers or produce the control command directly, OmniJARVIS seeks a different path to ensure both strong reasoning and efficient decision-making capabilities via unified tokenization of multimodal interaction data. First, we introduce a self-supervised approach to learn a behavior encoder that produces discretized tokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation learning policy decoder conditioned on these tokens. These additional behavior tokens will be augmented to the vocabulary of pretrained Multimodal Language Models. With this encoder, we then pack long-term multimodal interactions involving task instructions, memories, thoughts, observations, textual responses, behavior trajectories, etc into unified token sequences and model them with autoregressive transformers. Thanks to the semantically meaningful behavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing chain-of-thoughts), plan, answer questions, and act (by producing behavior tokens for the imitation learning policy decoder). OmniJARVIS demonstrates excellent performances on a comprehensive collection of atomic, programmatic, and open-ended tasks in open-world Minecraft. Our analysis further unveils the crucial design principles in interaction data formation, unified tokenization, and its scaling potentials. The dataset, models, and code will be released at https://craftjarvis.org/OmniJARVIS.'}",https://openreview.net{'value': '/pdf/ddeadd0a0f414f149584c3302604f8bcd7f0f50a.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=cUGf2HaNcs,{'value': 'Learning Truncated Causal History Model for Video Restoration'},Amirhosein Ghasemabadi; Muhammad Kamran Janjua; Mohammad Salameh; Di Niu,~Amirhosein_Ghasemabadi2; ~Muhammad_Kamran_Janjua3; ~Mohammad_Salameh1; ~Di_Niu1,"{'value': ['video restoration', 'low-level computer vision', 'motion understanding']}","{'value': 'One key challenge to video restoration is to model the transition dynamics of video frames governed by motion. In this work, we propose Turtle to learn the truncated causal history model for efficient and high-performing video restoration. Unlike traditional methods that process a range of contextual frames in parallel, Turtle enhances efficiency by storing and summarizing a truncated history of the input frame latent representation into an evolving historical state. This is achieved through a sophisticated similarity-based retrieval mechanism that implicitly accounts for inter-frame motion and alignment. The causal design in Turtle enables recurrence in inference through state-memorized historical features while allowing parallel training by sampling truncated video clips. We report new state-of-the-art results on a multitude of video restoration benchmark tasks, including video desnowing, nighttime video deraining, video raindrops and rain streak removal, video super-resolution, real-world and synthetic video deblurring, and blind video denoising while reducing the computational cost compared to existing best contextual methods on all these tasks.'}",https://openreview.net{'value': '/pdf/d837b4f7b539a1cfa6a5d2f3ba64db90b24d0e8f.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=cFTi3gLJ1X,{'value': 'Depth Anything V2'},Lihe Yang; Bingyi Kang; Zilong Huang; Zhen Zhao; Xiaogang Xu; Jiashi Feng; Hengshuang Zhao,~Lihe_Yang1; ~Bingyi_Kang1; ~Zilong_Huang1; ~Zhen_Zhao4; ~Xiaogang_Xu2; ~Jiashi_Feng1; ~Hengshuang_Zhao2,{'value': ['Monocular depth estimation']},"{'value': 'This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with sparse depth annotations to facilitate future research. Models are available at https://github.com/DepthAnything/Depth-Anything-V2.'}",https://openreview.net{'value': '/pdf/fc5361e39997a3b9bb75d48ab2cadb293cc7b7fd.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=cEtExbAKYV,{'value': 'StepbaQ: Stepping backward as Correction for Quantized Diffusion Models'},Yi-Chung Chen; Zhi-Kai Huang; Jing-Ren Chen,~Yi-Chung_Chen1; ~Zhi-Kai_Huang1; ~Jing-Ren_Chen1,{'value': ['Diffusion Model; Model Quantization']},"{'value': 'Quantization of diffusion models has attracted considerable attention due to its potential to enable various applications on resource-constrained mobile devices. However, given the cumulative nature of quantization errors in quantized diffusion models, overall performance may still decline even with efforts to minimize quantization error at each sampling step.\nRecent studies have proposed several methods to address accumulated quantization error, yet these solutions often suffer from limited applicability due to their underlying assumptions or only partially resolve the issue due to an incomplete understanding.\nIn this work, we introduce a novel perspective by conceptualizing quantization error as a ""stepback"" in the denoising process. We investigate how the accumulation of quantization error can distort the sampling trajectory, resulting in a notable decrease in model performance. To address this challenge, we introduce StepbaQ, a method that calibrates the sampling trajectory and counteracts the adverse effects of accumulated quantization error through a sampling step correction mechanism. Notably, StepbaQ relies solely on statistics of quantization error derived from a small calibration dataset, highlighting its strong applicability.\nOur experimental results demonstrate that StepbaQ can serve as a plug-and-play technique to enhance the performance of diffusion models quantized by off-the-shelf tools without modifying the quantization settings. For example, StepbaQ significantly improves the performance of the quantized SD v1.5 model by 7.30 in terms of FID on SDprompts dataset under the common W8A8 setting, and it enhances the performance of the quantized SDXL-Turbo model by 17.31 in terms of FID on SDprompts dataset under the challenging W4A8 setting.'}",https://openreview.net{'value': '/pdf/4403f21211ca02e4ff44f6ca8cb3fe1d6169c499.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=c3Pakdyi3t,{'value': '$\\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning'},Runqian Wang; Soumya Ghosh; David Daniel Cox; Diego Antognini; Aude Oliva; Rogerio Feris; Leonid Karlinsky,~Runqian_Wang1; ~Soumya_Ghosh1; ~David_Daniel_Cox1; ~Diego_Antognini1; ~Aude_Oliva1; ~Rogerio_Feris1; ~Leonid_Karlinsky3,"{'value': ['Parameter Efficient Finetuning', 'Knowledge Distillation', 'Large Language Model', 'Data Synthesis']}","{'value': 'Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters. These additional LoRA parameters are specific to the base model being adapted. When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained. Such re-training requires access to the data used to train the LoRA for the original base model. This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data. To address this challenge, we propose $\\textit{Trans-LoRA}$ --- a novel method for lossless, nearly data-free transfer of LoRAs across base models. Our approach relies on synthetic data to transfer LoRA modules. Using large language models, we design a synthetic data generator to approximate the data-generating process of the $\\textit{observed}$ task data subset. Training on the resulting synthetic dataset transfers LoRA modules to new models. We show the effectiveness of our approach using both LLama and Gemma model families. Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks.'}",https://openreview.net{'value': '/pdf/33225dd1a42b912e79c3bc10fd2ec971206701e5.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=bnNSQhZJ88,{'value': 'Secret Collusion among AI Agents: Multi-Agent Deception via Steganography'},Sumeet Ramesh Motwani; Mikhail Baranchuk; Martin Strohmeier; Vijay Bolina; Philip Torr; Lewis Hammond; Christian Schroeder de Witt,~Sumeet_Ramesh_Motwani1; ~Mikhail_Baranchuk1; ~Martin_Strohmeier1; ~Vijay_Bolina1; ~Philip_Torr1; ~Lewis_Hammond1; ~Christian_Schroeder_de_Witt1,"{'value': ['Collusion', 'AI Safety', 'Steganography', 'Large Language Models', 'Model Evaluation Framework', 'Multi-Agent Security', 'Security', 'Frontier Models', 'GenAI', 'AI Control']}","{'value': 'Recent advancements in generative AI suggest the potential for large-scale interaction between autonomous agents and humans across platforms such as the internet. While such interactions could foster productive cooperation, the ability of AI agents to circumvent security oversight raises critical multi-agent security problems, particularly in the form of unintended information sharing or undesirable coordination. In our work, we establish the subfield of secret collusion, a form of multi-agent deception, in which two or more agents employ steganographic methods to conceal the true nature of their interactions, be it communicative or otherwise, from oversight. We propose a formal threat model for AI agents communicating steganographically and derive rigorous theoretical insights about the capacity and incentives of large language models (LLMs) to perform secret collusion, in addition to the limitations of threat mitigation measures. We complement our findings with empirical evaluations demonstrating rising steganographic capabilities in frontier single and multi-agent LLM setups and examining potential scenarios where collusion may emerge, revealing limitations in countermeasures such as monitoring, paraphrasing, and parameter optimization. Our work is the first to formalize and investigate secret collusion among frontier foundation models, identifying it as a critical area in AI Safety and outlining a comprehensive research agenda to mitigate future risks of collusion between generative AI systems.'}",https://openreview.net{'value': '/pdf/1819ecc171caf5c4b38f428386f6d19e8cfae90d.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=bmoS6Ggw4j,{'value': 'Can Graph Learning Improve Planning in LLM-based Agents?'},Xixi Wu; Yifei Shen; Caihua Shan; Kaitao Song; Siwei Wang; Bohang Zhang; Jiarui Feng; Hong Cheng; Wei Chen; Yun Xiong; Dongsheng Li,~Xixi_Wu1; ~Yifei_Shen1; ~Caihua_Shan1; ~Kaitao_Song1; ~Siwei_Wang2; ~Bohang_Zhang1; ~Jiarui_Feng1; ~Hong_Cheng1; ~Wei_Chen10; ~Yun_Xiong1; ~Dongsheng_Li2,"{'value': ['Task Planning', 'Language Agents', 'Graph Learning', 'Graph Neural Networks', 'Language Model']}","{'value': ""Task planning in language agents is emerging as an important research topic alongside the development of large language models (LLMs). It aims to break down complex user requests in natural language into solvable sub-tasks, thereby fulfilling the original requests. In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them. Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it. In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design. Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs). This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance. Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance. The performance gain increases with a larger task graph size.""}",https://openreview.net{'value': '/pdf/3e5e4109af1b321e2c7be31711db90c52c8a140e.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=bkUvKPKafQ,{'value': 'ChatQA: Surpassing GPT-4 on Conversational QA and RAG'},Zihan Liu; Wei Ping; Rajarshi Roy; Peng Xu; Chankyu Lee; Mohammad Shoeybi; Bryan Catanzaro,~Zihan_Liu2; ~Wei_Ping1; ~Rajarshi_Roy1; ~Peng_Xu7; ~Chankyu_Lee1; ~Mohammad_Shoeybi1; ~Bryan_Catanzaro1,"{'value': ['large language models', 'retrieval-augmented generation', 'RAG']}","{'value': 'In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA).  To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG.  For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs.  We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions.  Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without relying on any synthetic data from OpenAI GPT models.   Notably, Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09 by a margin. These results demonstrate the exceptional quality of the proposed ChatQA recipe. To advance research in this field, we open-sourced the model weights, instruction tuning data, ChatRAG Bench, and retriever for the community.'}",https://openreview.net{'value': '/pdf/a9868221a3b9bd4e6f654789c9d0a165d1ba3259.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=bhSfbjS6j9,{'value': 'Multistable Shape from Shading Emerges from Patch Diffusion'},Xinran Han; Todd Zickler; Ko Nishino,~Xinran_Han1; ~Todd_Zickler1; ~Ko_Nishino4,"{'value': ['shape from shading', 'multistable perception', 'diffusion models', 'low-level vision']}","{'value': 'Models for inferring monocular shape of surfaces with diffuse reflection---shape from shading---ought to produce distributions of outputs, because there are fundamental mathematical ambiguities of both continuous (e.g., bas-relief) and discrete (e.g., convex/concave) types that are also experienced by humans. Yet, the outputs of current models are limited to point estimates or tight distributions around single modes, which prevent them from capturing these effects. We introduce a model that reconstructs a multimodal distribution of shapes from a single shading image, which aligns with the human experience of multistable perception. We train a small denoising diffusion process to generate surface normal fields from $16\\times 16$ patches of synthetic images of everyday 3D objects. We deploy this model patch-wise at multiple scales, with guidance from inter-patch shape consistency constraints. Despite its relatively small parameter count and predominantly bottom-up structure, we show that multistable shape explanations emerge from this model for ambiguous test images that humans experience as being multistable. At the same time, the model produces veridical shape estimates for object-like images that include distinctive occluding contours and appear less ambiguous. This may inspire new architectures for stochastic 3D shape perception that are more efficient and better aligned with human experience.'}",https://openreview.net{'value': '/pdf/c9b76d94959e0592a0305f254bcce8d77a7d3aff.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=bcVLFQCOjc,{'value': 'DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ'},Jonas Belouadi; Simone Paolo Ponzetto; Steffen Eger,~Jonas_Belouadi1; ~Simone_Paolo_Ponzetto1; ~Steffen_Eger1,"{'value': ['Vision Language Models', 'Code Generation', 'Image Understanding', 'Vector Graphics Generation']}","{'value': 'Creating high-quality scientific figures can be time-consuming and challenging, even though sketching ideas on paper is relatively easy. Furthermore, recreating existing figures that are not stored in formats preserving semantic information is equally complex. To tackle this problem, we introduce DeTikZify, a novel multimodal language model that automatically synthesizes scientific figures as semantics-preserving TikZ graphics programs based on sketches and existing figures. To achieve this, we create three new datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn sketches with their corresponding scientific figures; and MetaFig, a collection of diverse scientific figures and associated metadata. We train DeTikZify on MetaFig and DaTikZv2, along with synthetically generated sketches learned from SketchFig. We also introduce an MCTS-based inference algorithm that enables DeTikZify to iteratively refine its outputs without the need for additional training. Through both automatic and human evaluation, we demonstrate that DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ programs, with the MCTS algorithm effectively boosting its performance. We make our code, models, and datasets publicly available.'}",https://openreview.net{'value': '/pdf/66017807b14cb742e6c3280f2111afbdde4e0f46.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=bO5bUxvH6m,{'value': 'Learning Discrete Concepts in Latent Hierarchical Models'},Lingjing Kong; Guangyi Chen; Biwei Huang; Eric P. Xing; Yuejie Chi; Kun Zhang,~Lingjing_Kong1; ~Guangyi_Chen1; ~Biwei_Huang1; ~Eric_Xing1; ~Yuejie_Chi1; ~Kun_Zhang1,"{'value': ['representation learning', 'causal representation learning', 'generative models', 'causal discovery', 'hierarchical models']}","{'value': ""Learning concepts from natural high-dimensional data (e.g., images) holds potential in building human-aligned and interpretable machine learning models.\n    Despite its encouraging prospect, formalization and theoretical insights into this crucial task are still lacking.\n    In this work, we formalize concepts as discrete latent causal variables that are related via a hierarchical causal model that encodes different abstraction levels of concepts embedded in high-dimensional data (e.g., a dog breed and its eye shapes in natural images).\n    We formulate conditions to facilitate the identification of the proposed causal model, which reveals when learning such concepts from unsupervised data is possible.\n    Our conditions permit complex causal hierarchical structures beyond latent trees and multi-level directed acyclic graphs in prior work and can handle high-dimensional, continuous observed variables, which is well-suited for unstructured data modalities such as images.\n    We substantiate our theoretical claims with synthetic data experiments.\n    Further, we discuss our theory's implications for understanding the underlying mechanisms of latent diffusion models and provide corresponding empirical evidence for our theoretical insights.""}",https://openreview.net{'value': '/pdf/fc94091b7601a7b61f28aad4441e7ccc9ebe7776.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=bKOZYBJE4Z,{'value': 'Causal Contrastive Learning for Counterfactual Regression Over Time'},Mouad El Bouchattaoui; Myriam Tami; BENOIT LEPETIT; Paul-Henry Cournède,~Mouad_El_Bouchattaoui1; ~Myriam_Tami1; ~BENOIT_LEPETIT1; ~Paul-Henry_Cournède1,"{'value': ['Counterfactual Regression', 'Longitudinal Data', 'Contrastive Learning']}","{'value': 'Estimating treatment effects over time holds significance in various domains, including precision medicine, epidemiology, economy, and marketing. This paper introduces a unique approach to counterfactual regression over time, emphasizing long-term predictions. Distinguishing itself from existing models like Causal Transformer, our approach highlights the efficacy of employing RNNs for long-term forecasting, complemented by Contrastive Predictive Coding (CPC) and Information Maximization (InfoMax). Emphasizing efficiency, we avoid the need for computationally expensive transformers. Leveraging CPC, our method captures long-term dependencies within time-varying confounders. Notably, recent models have disregarded the importance of invertible representation, compromising identification assumptions. To remedy this, we employ the InfoMax principle, maximizing a lower bound of mutual information between sequence data and its representation. Our method achieves state-of-the-art counterfactual estimation results using both synthetic and real-world data, marking the pioneering incorporation of Contrastive Predictive Encoding in causal inference.'}",https://openreview.net{'value': '/pdf/1e79576af16957794680940d56ccb82a085d4ef9.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=az1SLLsmdR,{'value': 'Elucidating the Design Space of Dataset Condensation'},Shitong Shao; Zikai Zhou; Huanran Chen; Zhiqiang Shen,~Shitong_Shao1; ~Zikai_Zhou3; ~Huanran_Chen1; ~Zhiqiang_Shen1,"{'value': ['dataset condensation', 'efficient computer vision', 'design space']}","{'value': 'Dataset condensation, a concept within $\\textit{data-centric learning}$, aims to efficiently transfer critical attributes from an original dataset to a synthetic version, meanwhile maintaining both diversity and realism of syntheses. This approach can significantly improve model training efficiency and is also adaptable for multiple application areas. Previous methods in dataset condensation have faced several challenges: some incur high computational costs which limit scalability to larger datasets ($\\textit{e.g.,}$ MTT, DREAM, and TESLA), while others are restricted to less optimal design spaces, which could hinder potential improvements, especially in smaller datasets ($\\textit{e.g.,}$ SRe$^2$L, G-VBSM, and RDED). To address these limitations, we propose a comprehensive designing-centric framework that includes specific, effective strategies like implementing soft category-aware matching, adjusting the learning rate schedule and applying small batch-size. These strategies are grounded in both empirical evidence and theoretical backing. Our resulting approach, $\\textbf{E}$lucidate $\\textbf{D}$ataset $\\textbf{C}$ondensation ($\\textbf{EDC}$), establishes a benchmark for both small and large-scale dataset condensation. In our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a compression ratio of 0.78\\%. This performance surpasses those of SRe$^2$L, G-VBSM, and RDED by margins of 27.3%, 17.2%, and 6.6%, respectively. Code is available at: https://github.com/shaoshitong/EDC.'}",https://openreview.net{'value': '/pdf/40c09b96efff6bdd8291add347e11cea95e79ffa.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=amJyuVqSaf,{'value': 'Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows'},Alberto Cabezas; Louis Sharrock; Christopher Nemeth,~Alberto_Cabezas1; ~Louis_Sharrock1; ~Christopher_Nemeth1,"{'value': ['Sampling algorithms', 'Bayesian inference', 'MCMC', 'flow matching', 'continuous normalizing flows']}","{'value': 'Continuous normalizing flows (CNFs) learn the probability path between a reference distribution and a target distribution by modeling the vector field generating said path using neural networks. Recently, Lipman et al. (2022) introduced a simple and inexpensive method for training CNFs in generative modeling, termed flow matching (FM). In this paper, we repurpose this method for probabilistic inference by incorporating Markovian sampling methods in evaluating the FM objective, and using the learned CNF to improve Monte Carlo sampling. Specifically, we propose an adaptive Markov chain Monte Carlo (MCMC) algorithm, which combines a local Markov transition kernel with a non-local, flow-informed transition kernel, defined using a CNF. This CNF is adapted on-the-fly using samples from the Markov chain, which are used to specify the probability path for the FM objective. Our method also includes an adaptive tempering mechanism that allows the discovery of multiple modes in the target distribution. Under mild assumptions, we establish convergence of our method to a local optimum of the FM objective. We then benchmark our approach on several synthetic and real-world examples, achieving similar performance to other state-of-the-art methods but often at a significantly lower computational cost.'}",https://openreview.net{'value': '/pdf/c1d22a01947642f18fb9a1f6b81d7b6a0b459450.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=aetbfmCcwg,{'value': 'Debiasing Synthetic Data Generated by Deep Generative Models'},Alexander Decruyenaere; Heidelinde Dehaene; Paloma Rabaey; Johan Decruyenaere; Christiaan Polet; Thomas Demeester; Stijn Vansteelandt,~Alexander_Decruyenaere1; ~Heidelinde_Dehaene1; ~Paloma_Rabaey1; ~Johan_Decruyenaere1; ~Christiaan_Polet1; ~Thomas_Demeester1; ~Stijn_Vansteelandt1,"{'value': ['deep generative model', 'efficient influence function', 'inferential utility', 'synthetic data']}","{'value': 'While synthetic data hold great promise for privacy protection, their statistical analysis poses significant challenges that necessitate innovative solutions. The use of deep generative models (DGMs) for synthetic data generation is known to induce considerable bias and imprecision into synthetic data analyses, compromising their inferential utility as opposed to original data analyses. This bias and uncertainty can be substantial enough to impede statistical convergence rates, even in seemingly straightforward analyses like mean calculation. The standard errors of such estimators then exhibit slower shrinkage with sample size than the typical 1 over root-$n$ rate. This complicates fundamental calculations like p-values and confidence intervals, with no straightforward remedy currently available. In response to these challenges, we propose a new strategy that targets synthetic data created by DGMs for specific data analyses. Drawing insights from debiased and targeted machine learning, our approach accounts for biases, enhances convergence rates, and facilitates the calculation of estimators with easily approximated large sample variances. We exemplify our proposal through a simulation study on toy data and two case studies on real-world data, highlighting the importance of tailoring DGMs for targeted data analysis. This debiasing strategy contributes to advancing the reliability and applicability of synthetic data in statistical inference.'}",https://openreview.net{'value': '/pdf/79eab6cc9637d47619a544adce65c794d05938cd.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=abuQMKDVkW,{'value': 'SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection'},Yuxuan Li; Xiang Li; Weijie Li; Qibin Hou; Li Liu; Ming-Ming Cheng; Jian Yang,~Yuxuan_Li4; ~Xiang_Li20; ~Weijie_Li11; ~Qibin_Hou1; ~Li_Liu9; ~Ming-Ming_Cheng3; ~Jian_Yang1,"{'value': ['Benchmark', 'Computer Vision', 'Object Detection', 'Synthetic Aperture Radar', 'Pretrain', 'Domain Transfer']}","{'value': 'Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at \\url{https://github.com/zcablii/SARDet_100K}.'}",https://openreview.net{'value': '/pdf/511ed5cdbbdab7fd65ebf292460460791da6821c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=aaUVnpQvbZ,{'value': 'Learning Elastic Costs to Shape Monge Displacements'},Michal Klein; Aram-Alexandre Pooladian; Pierre Ablin; Eugene Ndiaye; Jonathan Niles-Weed; marco cuturi,~Michal_Klein1; ~Aram-Alexandre_Pooladian2; ~Pierre_Ablin2; ~Eugene_Ndiaye1; ~Jonathan_Niles-Weed1; ~marco_cuturi2,{'value': ['optimal transport']},"{'value': ""Given a source and a target probability measure, the Monge problem studies efficient ways to map the former onto the latter.\nThis efficiency is quantified by defining a *cost* function between source and target data. \nSuch a cost is often set by default in the machine learning literature to the squared-Euclidean distance, $\\ell^2\\_2(\\mathbf{x},\\mathbf{y}):=\\tfrac12\\|\\mathbf{x}-\\mathbf{y}\\|\\_2^2$.\nThe benefits of using *elastic* costs, defined using a regularizer $\\tau$ as $c(\\mathbf{x},\\mathbf{y}):=\\ell^2_2(\\mathbf{x},\\mathbf{y})+\\tau(\\mathbf{x}-\\mathbf{y})$, was recently highlighted in (Cuturi et al. 2023). Such costs shape the *displacements* of Monge maps $T$, namely the difference between a source point and its image $T(\\mathbf{x})-\\mathbf{x}$, by giving them a structure that matches that of the proximal operator of $\\tau$.\nIn this work, we make two important contributions to the study of elastic costs:*(i)* For any elastic cost, we propose a numerical method to compute Monge maps that are provably optimal. This provides a much-needed routine to create synthetic problems where the ground-truth OT map is known, by analogy to the Brenier theorem, which states that the gradient of any convex potential is always a valid Monge map for the $\\ell_2^2$ cost; *(ii)* We propose a loss to *learn* the parameter $\\theta$ of a parameterized regularizer $\\tau_\\theta$, and apply it in the case where $\\tau_{A}({\\bf z}):=\\|A^\\perp {\\bf z}\\|^2_2$. This regularizer promotes displacements that lie on a low-dimensional subspace of $\\mathbb{R}^d$, spanned by the $p$ rows of $A\\in\\mathbb{R}^{p\\times d}$. We illustrate the soundness of our procedure on synthetic data, generated using our first contribution, in which we show near-perfect recovery of $A$'s subspace using only samples. We demonstrate the applicability of this method by showing predictive improvements on single-cell data tasks.""}",https://openreview.net{'value': '/pdf/f441ba2eee17d2a14b6ea5fa2e36a8d1e2a73fe0.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=aNTnHBkw4T,{'value': 'Understanding Hallucinations in Diffusion Models through Mode Interpolation'},Sumukh K Aithal; Pratyush Maini; Zachary Chase Lipton; J Zico Kolter,~Sumukh_K_Aithal1; ~Pratyush_Maini1; ~Zachary_Chase_Lipton1; ~J_Zico_Kolter1,"{'value': ['diffusion', 'generative models', 'hallucination']}","{'value': ""Colloquially speaking, image generation models based upon diffusion processes are frequently said to exhibit ''hallucinations'' samples that could never occur in the training data.  But where do such hallucinations come from? In this paper, we study a particular failure mode in diffusion models, which we term ***mode interpolation***.  Specifically, we find that diffusion models smoothly ``interpolate'' between nearby data modes in the training set, to generate samples that are completely outside the support of the original training distribution; this phenomenon leads diffusion models to generate artifacts that never existed in real data (i.e., hallucinations). We systematically study the reasons for, and the manifestation of this phenomenon. Through experiments on 1D and 2D Gaussians, we show how a discontinuous loss landscape in the diffusion model's decoder leads to a region where any smooth approximation will cause such hallucinations. Through experiments on artificial datasets with various shapes, we show how hallucination leads to the generation of combinations of shapes that never existed. We extend the validity of mode interpolation in real-world datasets by explaining the unexpected generation of images with additional or missing fingers similar to those produced by popular text-to-image generative models. Finally, we show that diffusion models in fact ***know*** when they go out of support and hallucinate. This is captured by the high variance in the trajectory of the generated sample towards the final few backward sampling process. Using a simple metric to capture this variance, we can remove over 95\\% of hallucinations at generation time. We conclude our exploration by showing the implications of such hallucination (and its removal) on the collapse (and stabilization) of recursive training on synthetic data with experiments on datasets like MNIST .""}",https://openreview.net{'value': '/pdf/954c16c4eaedd4a0fd73f9a95ac68e082780b478.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=aFWx1N84Fe,{'value': 'The Map Equation Goes Neural: Mapping Network Flows with Graph Neural Networks'},Christopher Blöcker; Chester Tan; Ingo Scholtes,~Christopher_Blöcker1; ~Chester_Tan1; ~Ingo_Scholtes1,"{'value': ['community detection', 'graph clustering', 'random walk', 'map equation']}","{'value': 'Community detection is an essential tool for unsupervised data exploration and revealing the organisational structure of networked systems. With a long history in network science, community detection typically relies on objective functions, optimised with custom-tailored search algorithms, but often without leveraging recent advances in deep learning. Recently, first works have started incorporating such objectives into loss functions for deep graph clustering and pooling. We consider the map equation, a popular information-theoretic objective function for unsupervised community detection, and express it in differentiable tensor form for optimisation through gradient descent. Our formulation turns the map equation compatible with any neural network architecture, enables end-to-end learning, incorporates node features, and chooses the optimal number of clusters automatically, all without requiring explicit regularisation. Applied to unsupervised graph clustering tasks, we achieve competitive performance against state-of-the-art deep graph clustering baselines in synthetic and real-world datasets.'}",https://openreview.net{'value': '/pdf/cb0c7dd59b750afd4ddd0d81e8e0c82fb964244e.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=a4qT29Levh,{'value': 'SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout'},Chiyu Max Jiang; Yijing Bai; Andre Cornman; Christopher Davis; Xiukun Huang; Hong Jeon; Sakshum Kulshrestha; John Wheatley Lambert; Shuangyu Li; Xuanyu Zhou; Carlos Fuertes; Chang Yuan; Mingxing Tan; Yin Zhou; Dragomir Anguelov,~Chiyu_Max_Jiang1; ~Yijing_Bai1; ~Andre_Cornman2; ~Christopher_Davis7; ~Xiukun_Huang1; ~Hong_Jeon1; ~Sakshum_Kulshrestha1; ~John_Lambert1; ~Shuangyu_Li1; ~Xuanyu_Zhou1; ~Carlos_Fuertes1; ~Chang_Yuan1; ~Mingxing_Tan3; ~Yin_Zhou1; ~Dragomir_Anguelov1,"{'value': ['Autonomous Vehicles', 'Simulation', 'Diffusion Models', 'Scene Generation', 'Close Loop Simulation']}","{'value': 'Simulation with realistic and interactive agents represents a key task for autonomous vehicle (AV) software development in order to test AV performance in prescribed, often long-tail scenarios. In this work, we propose SceneDiffuser, a scene-level diffusion prior for traffic simulation. We present a singular framework that unifies two key stages of simulation: scene initialization and scene rollout. Scene initialization refers to generating the initial layout for the traffic in a scene, and scene rollout refers to closed-loop simulation for the behaviors of the agents. While diffusion has been demonstrated to be effective in learning realistic, multimodal agent distributions, two open challenges remain: controllability and closed-loop inference efficiency and realism. To this end, to address controllability challenges, we propose generalized hard constraints, a generalized inference-time constraint mechanism that is simple yet effective. To improve closed-loop inference quality and efficiency, we propose amortized diffusion, a novel diffusion denoising paradigm that amortizes the physical cost of denoising over future simulation rollout steps, reducing the cost of per physical rollout step to a single denoising function evaluation, while dramatically reducing closed-loop errors. We demonstrate the effectiveness of our approach on the Waymo Open Dataset, where we are able to generate distributionally realistic scenes, while obtaining competitive performance in the Sim Agents Challenge, surpassing the state-of-the-art in many realism attributes.'}",https://openreview.net{'value': '/pdf/ac6b24ffb0e47181c8916963928d13383ddf22cf.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=a3cauWMXNV,{'value': 'Fair GLASSO: Estimating Fair Graphical Models with Unbiased Statistical Behavior'},Madeline Navarro; Samuel Rey; Andrei Buciulea; Antonio Marques; Santiago Segarra,~Madeline_Navarro1; ~Samuel_Rey1; ~Andrei_Buciulea3; ~Antonio_Marques1; ~Santiago_Segarra1,"{'value': ['Graphical model', 'fairness', 'graph learning', 'graphical lasso']}","{'value': 'We propose estimating Gaussian graphical models (GGMs) that are fair with respect to sensitive nodal attributes. Many real-world models exhibit unfair discriminatory behavior due to biases in data. Such discrimination is known to be exacerbated when data is equipped with pairwise relationships encoded in a graph. Additionally, the effect of biased data on graphical models is largely underexplored. We thus introduce fairness for graphical models in the form of two bias metrics to promote balance in statistical similarities across nodal groups with different sensitive attributes. Leveraging these metrics, we present Fair GLASSO, a regularized graphical lasso approach to obtain sparse Gaussian precision matrices with unbiased statistical dependencies across groups. We also propose an efficient proximal gradient algorithm to obtain the estimates. Theoretically, we express the tradeoff between fair and accurate estimated precision matrices. Critically, this includes demonstrating when accuracy can be preserved in the presence of a fairness regularizer. On top of this, we study the complexity of Fair GLASSO and demonstrate that our algorithm enjoys a fast convergence rate. Our empirical validation includes synthetic and real-world simulations that illustrate the value and effectiveness of our proposed optimization problem and iterative algorithm.'}",https://openreview.net{'value': '/pdf/6667fd53bfb8c7a394412f76398a73a9e4da0c3b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ZfRGRK5Kxl,{'value': 'TripletCLIP:  Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives'},Maitreya Patel; Naga Sai Abhiram kusumba; Sheng Cheng; Changhoon Kim; Tejas Gokhale; Chitta Baral; Yezhou Yang,~Maitreya_Patel2; ~Naga_Sai_Abhiram_kusumba1; ~Sheng_Cheng1; ~Changhoon_Kim1; ~Tejas_Gokhale1; ~Chitta_Baral1; ~Yezhou_Yang1,"{'value': ['Contrastive Learning', 'Synthetic data', 'CLIP', 'Compositionality', 'TripletCLIP']}","{'value': ""Contrastive Language-Image Pretraining (CLIP) models maximize the mutual information between text and visual modalities to learn representations. This makes the nature of the training data a significant factor in the efficacy of CLIP for downstream tasks. However, the lack of compositional diversity in contemporary image-text datasets limits the compositional reasoning ability of CLIP. We show that generating ``hard'' negative captions via in-context learning and synthesizing corresponding negative images with text-to-image generators offers a solution. We introduce a novel contrastive pre-training strategy that leverages these hard negative captions and images in an alternating fashion to train CLIP. We demonstrate that our method, named TripletCLIP, when applied to existing datasets such as CC3M and CC12M, enhances the compositional capabilities of CLIP, resulting in an absolute improvement of over 9% on the SugarCrepe benchmark on an equal computational budget, as well as improvements in zero-shot image classification and image retrieval. Our code, models, and data are available at: tripletclip.github.io.""}",https://openreview.net{'value': '/pdf/289147f2527c2d1ba75a57f705d34f84e49a7bde.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ZbjJE6Nq5k,{'value': 'Normalization and effective learning rates in reinforcement learning'},Clare Lyle; Zeyu Zheng; Khimya Khetarpal; James Martens; Hado van Hasselt; Razvan Pascanu; Will Dabney,~Clare_Lyle1; ~Zeyu_Zheng1; ~Khimya_Khetarpal1; ~James_Martens1; ~Hado_van_Hasselt1; ~Razvan_Pascanu1; ~Will_Dabney1,"{'value': ['continual learning', 'reinforcement learning', 'optimization', 'plasticity']}","{'value': 'Normalization layers have recently experienced a renaissance in the deep reinforcement learning and continual learning literature, with several works highlighting diverse benefits such as improving loss landscape conditioning and combatting overestimation bias. However, normalization brings with it a subtle but important side effect: an equivalence between growth in the norm of the network parameters and decay in the effective learning rate. This becomes problematic in continual learning settings, where the resulting learning rate schedule may decay to near zero too quickly relative to the timescale of the learning problem. We propose to make the learning rate schedule explicit with a simple re-parameterization which we call  Normalize-and-Project (NaP), which couples the insertion of normalization layers with weight projection, ensuring that the effective learning rate remains constant throughout training. This technique reveals itself as a powerful analytical tool to better understand learning rate schedules in deep reinforcement learning, and as a means of improving robustness to nonstationarity in synthetic plasticity loss benchmarks along with both the single-task and sequential variants of the Arcade Learning Environment. We also show that our approach can be easily applied to popular architectures such as ResNets and transformers while recovering and in some cases even slightly improving the performance of the base model in common stationary benchmarks.'}",https://openreview.net{'value': '/pdf/ebb3993d25fc0664514d013458595defc44854cf.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Zb2ixT19VF,{'value': 'ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models'},Wei Pang; Masoumeh Shafieinejad; Lucy Liu; Stephanie Hazlewood; Xi He,~Wei_Pang5; ~Masoumeh_Shafieinejad1; ~Lucy_Liu2; ~Stephanie_Hazlewood1; ~Xi_He2,"{'value': ['diffusion models', 'synthesis', 'tabular data', 'generative models']}","{'value': 'Recent research in tabular data synthesis has focused on single tables, whereas real-world applications often involve complex data with tens or hundreds of interconnected tables. Previous approaches to synthesizing multi-relational (multi-table) data fall short in two key aspects: scalability for larger datasets and capturing long-range dependencies, such as correlations between attributes spread across different tables. Inspired by the success of diffusion models in tabular data modeling, we introduce \n \\textbf{C}luster \\textbf{La}tent \\textbf{Va}riable guided \\textbf{D}enoising \\textbf{D}iffusion \\textbf{P}robabilistic \\textbf{M}odels (ClavaDDPM). This novel approach leverages clustering labels as intermediaries to model relationships between tables, specifically focusing on foreign key constraints. ClavaDDPM leverages the robust generation capabilities of diffusion models while incorporating efficient algorithms to propagate the learned latent variables across tables. This enables ClavaDDPM to capture long-range dependencies effectively. \n Extensive evaluations on multi-table datasets of varying sizes show that ClavaDDPM significantly outperforms existing methods for these long-range dependencies while remaining competitive on utility metrics for single-table data.'}",https://openreview.net{'value': '/pdf/2be1d8b4a92d1a78dfde5becf40675f6848aec1e.pdf'},{'title_filter': 'Data Synthesis'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ZX6CEo1Wtv,{'value': 'Latent Diffusion for Neural Spiking Data'},Jaivardhan Kapoor; Auguste Schulz; Julius Vetter; Felix C Pei; Richard Gao; Jakob H. Macke,~Jaivardhan_Kapoor1; ~Auguste_Schulz1; ~Julius_Vetter2; ~Felix_C_Pei1; ~Richard_Gao1; ~Jakob_H._Macke1,"{'value': ['neural population', 'diffusion models', 'latent variable models', 'electrophysiology', 'brain-computer interfaces']}","{'value': 'Modern datasets in neuroscience enable unprecedented inquiries into the relationship between complex behaviors and the activity of many simultaneously recorded neurons. While latent variable models can successfully extract low-dimensional embeddings from such recordings, using them to generate realistic spiking data, especially in a behavior-dependent manner, still poses a challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS), a diffusion-based generative model with a low-dimensional latent space: LDNS employs an autoencoder with structured state-space (S4) layers to project discrete high-dimensional spiking data into continuous time-aligned latents. On these inferred latents, we train expressive (conditional) diffusion models, enabling us to sample neural activity with realistic single-neuron and population spiking statistics. We validate LDNS on synthetic data, accurately recovering latent structure, firing rates, and spiking statistics. Next, we demonstrate its flexibility by generating variable-length data that mimics human cortical activity during attempted speech. We show how to equip LDNS with an expressive observation model that accounts for single-neuron dynamics not mediated by the latent state, further increasing the realism of generated samples. Finally, conditional LDNS trained on motor cortical activity during diverse reaching behaviors can generate realistic spiking data given reach direction or unseen reach trajectories. In summary, LDNS simultaneously enables inference of low-dimensional latents and realistic conditional generation of neural spiking datasets, opening up further possibilities for simulating experimentally testable hypotheses.'}",https://openreview.net{'value': '/pdf/e0637f09f9affab400ed1f1dc2b8c58116dff2f6.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ZWNdgc13aw,{'value': 'NeoRL: Efficient Exploration for Nonepisodic RL'},Bhavya Sukhija; Lenart Treven; Florian Dorfler; Stelian Coros; Andreas Krause,~Bhavya_Sukhija1; ~Lenart_Treven1; ~Florian_Dorfler1; ~Stelian_Coros1; ~Andreas_Krause1,"{'value': ['reinforcement learning', 'single trajectory', 'nonepisodic setting', 'average cost MDP']}","{'value': 'We study the problem of nonepisodic reinforcement learning (RL) for nonlinear dynamical systems, where the system dynamics are unknown and the RL agent has to learn from a single trajectory, i.e., without resets. We propose **N**on**e**pisodic **O**ptistmic **RL** (NeoRL), an approach based on the principle of optimism in the face of uncertainty. NeoRL uses well-calibrated probabilistic models and plans optimistically w.r.t. the epistemic uncertainty about the unknown dynamics. Under continuity and bounded energy assumptions on the system, we\nprovide a first-of-its-kind regret bound of  $\\mathcal{O}(\\beta_T \\sqrt{T \\Gamma_T})$ for general nonlinear systems with Gaussian process dynamics. We compare NeoRL to other baselines on several deep RL environments and empirically demonstrate that NeoRL achieves the optimal average cost while incurring the least regret.'}",https://openreview.net{'value': '/pdf/48e0e53d557dc2db24cd64ea33611329103f1d32.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ZJjuNF0olj,{'value': 'DeTrack: In-model Latent Denoising Learning for Visual Object Tracking'},Xinyu Zhou; Jinglun Li; Lingyi Hong; Kaixun Jiang; Pinxue Guo; Weifeng Ge; Wenqiang Zhang,~Xinyu_Zhou5; ~Jinglun_Li1; ~Lingyi_Hong1; ~Kaixun_Jiang1; ~Pinxue_Guo1; ~Weifeng_Ge2; ~Wenqiang_Zhang1,{'value': ['visual object tracking; denoising learning;in-model latent;']},"{'value': 'Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction. Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data. Inspired by the diffusion model, denoising learning enhances the model’s robustness to unseen data. Therefore, We introduce noise to bounding boxes, generating noisy boxes for training, thus enhancing model robustness on testing data. We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed. Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process. Specifically, we propose a denoising Vision Transformer (ViT), which is composed of multiple denoising blocks. In the denoising block, template and search embeddings are projected into every denoising block as conditions. A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process. Subsequently, we\nutilize image features and trajectory information to refine the denoised bounding box. Besides, we also utilize trajectory memory and visual memory to improve tracking stability. Experimental results validate the effectiveness of our approach, achieving competitive performance on several challenging datasets. The proposed in-model latent denoising tracker achieve real-time speed, rendering denoising learning applicable in the visual object tracking community.'}",https://openreview.net{'value': '/pdf/7ac050bd4d1ee2a8b42e83083321dd7bb0e8cac7.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ZIpdu0cHYu,{'value': 'Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees'},Sijia Chen; Yibo Wang; Yi-Feng Wu; Qing-Guo Chen; Zhao Xu; Weihua Luo; Kaifu Zhang; Lijun Zhang,~Sijia_Chen3; ~Yibo_Wang2; ~Yi-Feng_Wu2; ~Qing-Guo_Chen1; ~Zhao_Xu7; ~Weihua_Luo2; ~Kaifu_Zhang2; ~Lijun_Zhang1,"{'value': ['Large Language Models', 'Tool Usage', 'Direct Preference Optimization', 'Tree of Thought']}","{'value': ""Tool-augmented large language models (LLMs) leverage tools, often in the form of APIs, to improve their reasoning capabilities on complex tasks. This enables them to act as intelligent agents interacting with the real world. The recently introduced ToolLLaMA model by Qin et al. [2023] utilizes the depth-first search-based decision tree (DFSDT) mechanism for multi-step reasoning with $16000+$ real-world APIs, effectively enhancing the performance of tool-augmented LLMs compared to traditional chain reasoning mechanisms. However, their approach only employs successful paths from decision trees (also called inference trees) for supervised fine-tuning (SFT), missing out on the potential learning opportunities from failed paths. Inspired by this, we propose an inference trajectory optimization framework based on preference learning to address this limitation. We first introduce a novel method for constructing step-wise preference data from tree-like expert trajectories, which leverages the previously ignored failed explorations in the decision trees. In the subsequent training phase, we first fine-tune the LLM with successful tool-usage expert trajectories and then apply direct preference optimization (DPO) with the preference data to update the LLM's policy, resulting in our ToolPrefer-LLaMA (TP-LLaMA) model. This approach not only enhances the utilization of original expert data but also broadens the learning space of the model. Our experiments demonstrate that by obtaining insights from errors in inference trees, TP-LLaMA significantly outperforms the baselines across almost all test scenarios by a large margin and exhibits better generalization capabilities with unseen APIs. At the same time, TP-LLaMA has also demonstrated superior reasoning efficiency compared to the baselines, making it more suitable for complex tool-usage reasoning tasks.""}",https://openreview.net{'value': '/pdf/4089087dbee55a272b1f4109115d831deed9f5b0.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ZCygNDMIII,{'value': 'Fine-grained Control of Generative Data Augmentation in IoT Sensing'},Tianshi Wang; Qikai Yang; Ruijie Wang; Dachun Sun; Jinyang Li; Yizhuo Chen; Yigong Hu; Chaoqi Yang; Tomoyoshi Kimura; Denizhan Kara; Tarek F. Abdelzaher,~Tianshi_Wang1; ~Qikai_Yang1; ~Ruijie_Wang2; ~Dachun_Sun1; ~Jinyang_Li2; ~Yizhuo_Chen2; ~Yigong_Hu1; ~Chaoqi_Yang1; ~Tomoyoshi_Kimura1; ~Denizhan_Kara1; ~Tarek_F._Abdelzaher1,"{'value': ['Generative models', 'data augmentation', 'Internet of Things', 'signal processing']}","{'value': 'Internet of Things (IoT) sensing models often suffer from overfitting due to data distribution shifts between training dataset and real-world scenarios. To address this, data augmentation techniques have been adopted to enhance model robustness by bolstering the diversity of synthetic samples within a defined vicinity of existing samples. This paper introduces a novel paradigm of data augmentation for IoT sensing signals by adding fine-grained control to generative models. We define a metric space with statistical metrics that capture the essential features of the short-time Fourier transformed (STFT) spectrograms of IoT sensing signals. These metrics serve as strong conditions for a generative model, enabling us to tailor the spectrogram characteristics in the time-frequency domain according to specific application needs. Furthermore, we propose a set of data augmentation techniques within this metric space to create new data samples. Our method is evaluated across various generative models, datasets, and downstream IoT sensing models. The results demonstrate that our approach surpasses the conventional transformation-based data augmentation techniques and prior generative data augmentation models.'}",https://openreview.net{'value': '/pdf/ac50073f98d5a7c4bbd1b96c7482027c5c6c7ccb.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ZC0PSk6Mc6,{'value': 'Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents'},Quentin Delfosse; Sebastian Sztwiertnia; Mark Rothermel; Wolfgang Stammer; Kristian Kersting,~Quentin_Delfosse1; ~Sebastian_Sztwiertnia1; ~Mark_Rothermel1; ~Wolfgang_Stammer1; ~Kristian_Kersting1,"{'value': ['Explainable AI (XAI)', 'Reinforcement Learning', 'Concept Bottlenecks']}","{'value': ""Goal misalignment, reward sparsity and difficult credit assignment are only a few of the many issues that make it difficult for deep reinforcement learning (RL) agents to learn optimal policies. \nUnfortunately, the black-box nature of deep neural networks impedes the inclusion of domain experts for inspecting the model and revising suboptimal policies.\n\nTo this end, we introduce Successive Concept Bottleneck Agents (SCoBots), that integrate consecutive concept bottleneck (CB) layers. \nIn contrast to current CB models, SCoBots do not just represent concepts as properties of individual objects, but also as relations between objects which is crucial for many RL tasks. \n\nOur experimental results provide evidence of SCoBots' competitive performances, but also of their potential for domain experts to understand and regularize their behavior. Among other things, SCoBots enabled us to identify a previously unknown misalignment problem in the iconic video game, Pong, and resolve it. Overall, SCoBots thus result in more human-aligned RL agents.""}",https://openreview.net{'value': '/pdf/3d4c8c5715b383f54cebb261efc8929c843ec6a4.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=YwpL0BVxts,"{'value': 'United We Stand, Divided We Fall: Fingerprinting Deep Neural Networks via Adversarial Trajectories'}",Tianlong Xu; Chen Wang; Gaoyang Liu; Yang Yang; Kai Peng; Wei Liu,~Tianlong_Xu2; ~Chen_Wang19; ~Gaoyang_Liu2; ~Yang_Yang74; ~Kai_Peng1; ~Wei_Liu54,"{'value': ['Deep neural network', 'intellectual property protection', 'model fingerprinting', 'adversarial sample.']}","{'value': 'In recent years, deep neural networks (DNNs) have witnessed extensive applications, and protecting their intellectual property (IP) is thus crucial. As a non-invasive way for model IP protection, model fingerprinting has become popular. However, existing single-point based fingerprinting methods are highly sensitive to the changes in the decision boundary, and may suffer from the misjudgment of the resemblance of sparse fingerprinting, yielding high false positives of innocent models. In this paper, we propose ADV-TRA, a more robust fingerprinting scheme that utilizes adversarial trajectories to verify the ownership of DNN models. Benefited from the intrinsic progressively adversarial level, the trajectory is capable of tolerating greater degree of alteration in decision boundaries. We further design novel schemes to generate a surface trajectory that involves a series of fixed-length trajectories with dynamically adjusted step sizes. Such a design enables a more unique and reliable fingerprinting with relatively low querying costs. Experiments on three datasets against four types of removal attacks show that ADV-TRA exhibits superior performance in distinguishing between infringing and innocent models, outperforming the state-of-the-art comparisons.'}",https://openreview.net{'value': '/pdf/c4e326e4220480b60f51e63ae149d2009d4cccd5.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=YjZ6fQAvT7,{'value': 'TreeVI: Reparameterizable Tree-structured Variational Inference for Instance-level Correlation Capturing'},Junxi Xiao; Qinliang Su,~Junxi_Xiao1; ~Qinliang_Su3,"{'value': ['Probabilistic Models', 'Variational Inference', 'Instance-level Correlation', 'Reparameterization', 'Reparameterized Variational Inference']}","{'value': 'Mean-field variational inference (VI) is computationally scalable, but its highly-demanding independence requirement hinders it from being applied to wider scenarios. Although many VI methods that take correlation into account have been proposed, these methods generally are not scalable enough to capture the correlation among data instances, which often arises in applications with graph-structured data or explicit constraints. In this paper, we developed the Tree-structured Variational Inference (TreeVI), which uses a tree structure to capture the correlation of latent variables in the posterior distribution. We show that  samples from the tree-structured posterior can be reparameterized efficiently and parallelly, making its training cost just 2 or 3 times that of VI under the mean-field assumption. To capture correlation with more complicated structure, the TreeVI is further extended to the multiple-tree case. Furthermore, we show that the underlying tree structure can be automatically learned from training data. With experiments on synthetic datasets, constrained clustering, user matching and link prediction, we demonstrate that the TreeVI is superior in capturing  instance-level correlation in posteriors and enhancing the performance of downstream applications.'}",https://openreview.net{'value': '/pdf/117f3b4968dd3b0e94a6d8504b1e38d085681bfa.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=YbhHz0X2j5,{'value': 'VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation'},Youpeng Wen; Junfan Lin; Yi Zhu; Jianhua Han; Hang Xu; Shen Zhao; Xiaodan Liang,~Youpeng_Wen1; ~Junfan_Lin1; ~Yi_Zhu3; ~Jianhua_Han1; ~Hang_Xu1; ~Shen_Zhao1; ~Xiaodan_Liang2,"{'value': ['Imitation learning', 'Video prediction', 'Robot Manipulation']}","{'value': ""Recent advancements utilizing large-scale video data for learning video generation models demonstrate significant potential in understanding complex physical dynamics. It suggests the feasibility of leveraging diverse robot trajectory data to develop a unified, dynamics-aware model to enhance robot manipulation. However, given the relatively small amount of available robot data, directly fitting data without considering the relationship between visual observations and actions could lead to suboptimal data utilization. To this end, we propose \\textbf{VidMan} (\\textbf{Vid}eo Diffusion for Robot \\textbf{Man}ipulation), a novel framework that employs a two-stage training mechanism inspired by dual-process theory from neuroscience to enhance stability and improve data utilization efficiency. Specifically, in the first stage, VidMan is pre-trained on the Open X-Embodiment dataset (OXE) for predicting future visual trajectories in a video denoising diffusion manner, enabling the model to develop a long horizontal awareness of the environment's dynamics. In the second stage, a flexible yet effective layer-wise self-attention adapter is introduced to transform VidMan into an efficient inverse dynamics model that predicts action modulated by the implicit dynamics knowledge via parameter sharing. Our VidMan framework outperforms state-of-the-art baseline model GR-1 on the CALVIN benchmark, achieving a 11.7\\% relative improvement, and demonstrates over 9\\% precision gains on the OXE small-scale dataset. These results provide compelling evidence that world models can significantly enhance the precision of robot action prediction. Codes and models will be public.""}",https://openreview.net{'value': '/pdf/e13dd37a46369b87a3c1fbc7cd460d8ed7a7039e.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=YTHJ8O6SCB,{'value': 'SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors'},Chenyang Ma; Kai Lu; Ta-Ying Cheng; Niki Trigoni; Andrew Markham,~Chenyang_Ma1; ~Kai_Lu5; ~Ta-Ying_Cheng1; ~Niki_Trigoni1; ~Andrew_Markham2,"{'value': ['VLM Spatial Reasoning', 'Zero-Shot']}","{'value': 'Current state-of-the-art spatial reasoning-enhanced VLMs are trained to excel at spatial visual question answering (VQA). However, we believe that higher-level 3D-aware tasks, such as articulating dynamic scene changes and motion planning, require a fundamental and explicit 3D understanding beyond current spatial VQA datasets. In this work, we present SpatialPIN, a framework designed to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with priors from multiple 3D foundation models in a zero-shot, training-free manner. Extensive experiments demonstrate that our spatial reasoning-imbued VLM performs well on various forms of spatial VQA and can extend to help in various downstream robotics tasks such as pick and stack and trajectory planning.'}",https://openreview.net{'value': '/pdf/867437d5e4ce3abc8790c6ec15f3bd74162253dc.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=YO6GVPUrKN,{'value': 'On the Limitations of Fractal Dimension as a Measure of Generalization'},Charlie Tan; Inés García-Redondo; Qiquan Wang; Michael M. Bronstein; Anthea Monod,~Charlie_Tan1; ~Inés_García-Redondo1; ~Qiquan_Wang2; ~Michael_M._Bronstein1; ~Anthea_Monod1,"{'value': ['Generalization', 'Optimization', 'Persistent Homology', 'Fractal Dimension']}","{'value': 'Bounding and predicting the generalization gap of overparameterized neural networks remains a central open problem in theoretical machine learning. There is a recent and growing body of literature that proposes the framework of fractals to model optimization trajectories of neural networks, motivating generalization bounds and measures based on the fractal dimension of the trajectory. Notably, the persistent homology dimension has been proposed to correlate with the generalization gap. This paper performs an empirical evaluation of these persistent homology-based generalization measures, with an in-depth statistical analysis. Our study reveals confounding effects in the observed correlation between generalization and topological measures due to the variation of hyperparameters. We also observe that fractal dimension fails to predict generalization of models trained from poor initializations. We lastly reveal the intriguing manifestation of model-wise double descent in these topological generalization measures. Our work forms a basis for a deeper investigation of the causal relationships between fractal geometry, topological data analysis, and neural network optimization.'}",https://openreview.net{'value': '/pdf/0c988ff1e1274d8b29711ebeac28c9a707d0f676.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Y841BRW9rY,{'value': 'AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases'},Zhaorun Chen; Zhen Xiang; Chaowei Xiao; Dawn Song; Bo Li,~Zhaorun_Chen1; ~Zhen_Xiang1; ~Chaowei_Xiao2; ~Dawn_Song1; ~Bo_Li19,"{'value': ['LLM Agent', 'LLM Red-teaming', 'Retrieval-Augmented Generation', 'Backdoor Poisoning', 'Trustworthy LLM']}","{'value': ""LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or\nRAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, resilience, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking\nthree types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. We inject the poisoning instances into the RAG knowledge base and long-term memories of these agents, respectively, demonstrating the generalization of AgentPoison. On each agent, AgentPoison achieves an average attack success rate of $\\ge$ 80% with minimal\nimpact on benign performance ($\\le$ 1%) with a poison rate < 0.1%. The code and data is available at https://github.com/BillChan226/AgentPoison.""}",https://openreview.net{'value': '/pdf/707fb9874a6d1beb97c9103c8db11c3d963ee36a.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Y1rOWS2Z4i,{'value': 'Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments'},Siddharth Nayak; Adelmo Morrison Orozco; Marina Ten Have; Jackson Zhang; Vittal Thirumalai; Darren Chen; Aditya Kapoor; Eric Robinson; Karthik Gopalakrishnan; James Harrison; Anuj Mahajan; brian ichter; Hamsa Balakrishnan,~Siddharth_Nayak1; ~Adelmo_Morrison_Orozco1; ~Marina_Ten_Have1; ~Jackson_Zhang1; ~Vittal_Thirumalai1; ~Darren_Chen1; ~Aditya_Kapoor1; ~Eric_Robinson2; ~Karthik_Gopalakrishnan3; ~James_Harrison1; ~Anuj_Mahajan1; ~brian_ichter1; ~Hamsa_Balakrishnan1,"{'value': ['multi-agent robotics', 'large language models']}","{'value': 'The ability of Language Models (LMs) to understand natural language makes them a powerful tool for parsing human instructions into task plans for autonomous robots. Unlike traditional planning methods that rely on domain-specific knowledge and handcrafted rules, LMs generalize from diverse data and adapt to various tasks with minimal tuning, acting as a compressed knowledge base. However, LMs in their standard form face challenges with long-horizon tasks, particularly in partially observable multi-agent settings. We propose an LM-based Long-Horizon Planner for Multi-Agent Robotics (LLaMAR), a cognitive architecture for planning that achieves state-of-the-art results in long-horizon tasks within partially observable environments. LLaMAR employs a plan-act-correct-verify framework, allowing self-correction from action execution feedback without relying on oracles or simulators. Additionally, we present MAP-THOR, a comprehensive test suite encompassing household tasks of varying complexity within the AI2-THOR environment. Experiments show that LLaMAR achieves a 30\\% higher success rate than other state-of-the-art LM-based multi-agent planners in MAP-THOR and Search \\& Rescue tasks. Code can be found at [https://github.com/nsidn98/LLaMAR](https://github.com/nsidn98/LLaMAR)'}",https://openreview.net{'value': '/pdf/30511ccbe77a9b3fbd9e34c29efcb47db4044e7c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Y13gSfTjGr,{'value': 'Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations'},Alexander Hägele; Elie Bakouch; Atli Kosson; Loubna Ben allal; Leandro Von Werra; Martin Jaggi,~Alexander_Hägele1; ~Elie_Bakouch1; ~Atli_Kosson1; ~Loubna_Ben_allal1; ~Leandro_Von_Werra1; ~Martin_Jaggi1,"{'value': ['Scaling Laws', 'Large Language Models', 'Learning Rate Schedules', 'Weight Averaging']}","{'value': ""Scale has become a main ingredient in obtaining strong machine learning models. As a result, understanding a model's scaling properties is key to effectively designing both the right training setup as well as future generations of architectures. In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size. We investigate the training behavior of a direct alternative --- constant learning rate and cooldowns --- and find that it scales predictably and reliably similar to cosine. Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales. Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs. Our code is available at https://github.com/epfml/schedules-and-scaling/.""}",https://openreview.net{'value': '/pdf/faf94d39312b1ee562966b2527805c1d5aae2f2b.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=XsNA2b8GPz,{'value': 'Adaptive Sampling for Efficient Softmax Approximation'},Tavor Baharav; Ryan Kang; Colin Sullivan; Mo Tiwari; Eric Sager Luxenberg; David Tse; Mert Pilanci,~Tavor_Baharav1; ~Ryan_Kang1; ~Colin_Sullivan1; ~Mo_Tiwari1; ~Eric_Sager_Luxenberg1; ~David_Tse1; ~Mert_Pilanci3,"{'value': ['Multi-armed bandits', 'adaptive', 'softmax', 'attention']}","{'value': 'The softmax function is ubiquitous in machine learning and optimization applications. Computing the full softmax evaluation of a matrix-vector product can be computationally expensive in high-dimensional settings. In many applications, however, it is sufficient to calculate only the top few outputs of the softmax function. In this work, we present an algorithm, dubbed AdaptiveSoftmax, that adaptively computes the top k softmax values more efficiently than the full softmax computation, with probabilistic guarantees. We demonstrate the sample efficiency improvements afforded by AdaptiveSoftmax on real and synthetic data to corroborate our theoretical results. AdaptiveSoftmax yields >10x gain over full softmax computation on most datasets, yielding up to 30x improvement for Mistral7B evaluated on the Wikitext dataset. The adaptive method we propose for estimating the partition function (the softmax denominator) is of independent interest and can be used in other applications such as kernel density estimation.'}",https://openreview.net{'value': '/pdf/e188b661e6b0a37452f6813bf9348a9472d23a63.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Xo1Yqyw7Yx,{'value': 'Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity'},Robby Costales; Stefanos Nikolaidis,~Robby_Costales1; ~Stefanos_Nikolaidis1,"{'value': ['diversity', 'meta reinforcement learning', 'meta-RL', 'reinforcement learning', 'adaptation', 'adaptive', 'agents', 'open-endedness', 'genotypes', 'phenotypes', 'simulators', 'simulation', 'generalization', 'meta-reinforcement']}","{'value': ""The wider application of end-to-end learning methods to embodied decision-making domains remains bottlenecked by their reliance on a superabundance of training data representative of the target domain.\nMeta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot *generalization*—the goal of standard reinforcement learning (RL)—in favor of few-shot *adaptation*, and thus hold promise for bridging larger generalization gaps.\nWhile learning this meta-level adaptive behavior still requires substantial data, efficient environment simulators approaching real-world complexity are growing in prevalence.\nEven so, hand-designing sufficiently diverse and numerous simulated training tasks for these complex domains is prohibitively labor-intensive.\nDomain randomization (DR) and procedural generation (PG), offered as solutions to this problem, require simulators to possess carefully-defined parameters which directly translate to meaningful task diversity—a similarly prohibitive assumption.\nIn this work, we present **DIVA**, an evolutionary approach for generating diverse training tasks in such complex, open-ended simulators.\nLike unsupervised environment design (UED) methods, DIVA can be applied to arbitrary parameterizations, but can additionally incorporate realistically-available domain knowledge—thus inheriting the *flexibility* and *generality* of UED, and the supervised *structure* embedded in well-designed simulators exploited by DR and PG.\nOur empirical results showcase DIVA's unique ability to overcome complex parameterizations and successfully train adaptive agent behavior, far outperforming competitive baselines from prior literature.\nThese findings highlight the potential of such *semi-supervised environment design* (SSED) approaches, of which DIVA is the first humble constituent, to enable training in realistic simulated domains, and produce more robust and capable adaptive agents.\nOur code is available at [https://github.com/robbycostales/diva](https://github.com/robbycostales/diva).""}",https://openreview.net{'value': '/pdf/e1675cb29a5434d867db98572c7722d6fbd7477c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=XXOMCwZ6by,{'value': 'Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks'},Zaijing Li; Yuquan Xie; Rui Shao; Gongwei Chen; Dongmei Jiang; Liqiang Nie,~Zaijing_Li1; ~Yuquan_Xie1; ~Rui_Shao1; ~Gongwei_Chen1; ~Dongmei_Jiang2; ~Liqiang_Nie2,"{'value': ['Multimodal Agent', 'Multimodal Large Language Models', 'Multimodal In-context Learning']}","{'value': 'Building a general-purpose agent is a long-standing vision in the field of artificial intelligence. Existing agents have made remarkable progress in many domains, yet they still struggle to complete long-horizon tasks in an open world. We attribute this to the lack of necessary world knowledge and multimodal experience that can guide agents through a variety of long-horizon tasks. In this paper, we propose a Hybrid Multimodal Memory module to address the above challenges. It 1) transforms knowledge into Hierarchical Directed Knowledge Graph that allows agents to explicitly represent and learn world knowledge, and 2) summarises historical information into Abstracted Multimodal Experience Pool that provide agents with rich references for in-context learning. On top of the Hybrid Multimodal Memory module, a multimodal agent, Optimus-1, is constructed with dedicated Knowledge-guided Planner and Experience-Driven Reflector, contributing to a better planning and reflection in the face of long-horizon tasks in Minecraft. Extensive experimental results show that Optimus-1 significantly outperforms all existing agents on challenging long-horizon task benchmarks, and exhibits near human-level performance on many tasks. In addition, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 exhibits strong generalization with the help of the Hybrid Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.'}",https://openreview.net{'value': '/pdf/d57bc4c4a18c0395e77ebc2cfa868685e62bbb3c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=XPhSbybD73,{'value': 'Probabilistic Decomposed Linear Dynamical Systems for Robust Discovery of Latent Neural Dynamics'},Yenho Chen; Noga Mudrik; Kyle A. Johnsen; Sankaraleengam Alagapan; Adam Shabti Charles; Christopher John Rozell,~Yenho_Chen1; ~Noga_Mudrik1; ~Kyle_A._Johnsen1; ~Sankaraleengam_Alagapan1; ~Adam_Shabti_Charles1; ~Christopher_John_Rozell1,"{'value': ['Computational Neuroscience', 'Probabilistic Modeling', 'State Space Models', 'Dynamical Systems']}","{'value': ""Time-varying linear state-space models are powerful tools for obtaining mathematically interpretable representations of neural signals. For example, switching and decomposed models describe complex systems using latent variables that evolve according to simple locally linear dynamics. However, existing methods for latent variable estimation are not robust to dynamical noise and system nonlinearity due to noise-sensitive inference procedures and limited model formulations. This can lead to inconsistent results on signals with similar dynamics, limiting the model's ability to provide scientific insight. In this work, we address these limitations and propose a probabilistic approach to latent variable estimation in decomposed models that improves robustness against dynamical noise. Additionally, we introduce an extended latent dynamics model to improve robustness against system nonlinearities. We evaluate our approach on several synthetic dynamical systems, including an empirically-derived brain-computer interface experiment, and demonstrate more accurate latent variable inference in nonlinear systems with diverse noise conditions. Furthermore, we apply our method to a real-world clinical neurophysiology dataset, illustrating the ability to identify interpretable and coherent structure where previous models cannot.""}",https://openreview.net{'value': '/pdf/97fd4685ad572113a49942a0e71937b3db55efb0.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=XMQTNzlgTJ,{'value': 'High-probability complexity bounds for stochastic non-convex minimax optimization'},Yassine Laguel; Yasa Syed; Necdet Aybat; Mert Gurbuzbalaban,~Yassine_Laguel1; ~Yasa_Syed1; ~Necdet_Aybat1; ~Mert_Gurbuzbalaban1,"{'value': ['nonconvex minimax optimization', 'high-probability guarantees', 'stochastic gradient descent ascent methods']}","{'value': 'Stochastic smooth nonconvex minimax problems are prevalent in machine learning, e.g., GAN training, fair classification, and distributionally robust learning. Stochastic gradient descent ascent (GDA)-type methods are popular in practice due to their simplicity and single-loop nature. However, there is a significant gap between the theory and practice regarding high-probability complexity guarantees for these methods on stochastic nonconvex minimax problems. Existing high-probability bounds for GDA-type single-loop methods only apply to convex/concave minimax problems and to particular non-monotone variational inequality problems under some restrictive assumptions. In this work, we address this gap by providing the first high-probability complexity guarantees for nonconvex/PL minimax problems corresponding to a smooth function that satisfies the PL-condition in the dual variable. Specifically, we show that when the stochastic gradients are light-tailed, the smoothed alternating GDA method can compute an $\\varepsilon$-stationary point within $\\mathcal{O}(\\frac{\\ell \\kappa^2 \\delta^2}{\\varepsilon^4} + \\frac{\\kappa}{\\varepsilon^2}(\\ell+\\delta^2\\log({1}/{\\bar{q}})))$ stochastic gradient calls with probability at least $1-\\bar{q}$ for any $\\bar{q}\\in(0,1)$, where $\\mu$ is the PL constant, $\\ell$ is the Lipschitz constant of the gradient, $\\kappa=\\ell/\\mu$ is the condition number, and $\\delta^2$ denotes a bound on the variance of stochastic gradients. We also present numerical results on a nonconvex/PL problem with synthetic data and on distributionally robust optimization problems with real data, illustrating our theoretical findings.'}",https://openreview.net{'value': '/pdf/54f2e1683603d036f8030c1e6bea30720a146552.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=XIScpCMUse,{'value': 'MVInpainter: Learning Multi-View Consistent Inpainting to Bridge 2D and 3D Editing'},Chenjie Cao; Chaohui Yu; Fan Wang; Xiangyang Xue; Yanwei Fu,~Chenjie_Cao1; ~Chaohui_Yu1; ~Fan_Wang6; ~Xiangyang_Xue2; ~Yanwei_Fu2,"{'value': ['Multi-view synthesis', 'Image inpainting', '3D editing']}","{'value': 'Novel View Synthesis (NVS) and 3D generation have recently achieved prominent improvements. However, these works mainly focus on confined categories or synthetic 3D assets, which are discouraged from generalizing to challenging in-the-wild scenes and fail to be employed with 2D synthesis directly. Moreover, these methods heavily depended on camera poses, limiting their real-world applications. \nTo overcome these issues, we propose MVInpainter, re-formulating the 3D editing as a multi-view 2D inpainting task. Specifically, MVInpainter partially inpaints multi-view images with the reference guidance rather than intractably generating an entirely novel view from scratch, which largely simplifies the difficulty of in-the-wild NVS and leverages unmasked clues instead of explicit pose conditions. To ensure cross-view consistency, MVInpainter is enhanced by video priors from motion components and appearance guidance from concatenated reference key\\&value attention. Furthermore, MVInpainter incorporates slot attention to aggregate high-level optical flow features from unmasked regions to control the camera movement with pose-free training and inference. Sufficient scene-level experiments on both object-centric and forward-facing datasets verify the effectiveness of MVInpainter, including diverse tasks, such as multi-view object removal, synthesis, insertion, and replacement. The project page is https://ewrfcas.github.io/MVInpainter/.'}",https://openreview.net{'value': '/pdf/fb2df576fdedb7707a719fe7fe47b61279006c85.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=XHWkHFWi3k,{'value': 'Self-Refining Diffusion Samplers: Enabling Parallelization via Parareal Iterations'},Nikil Roashan Selvam; Amil Merchant; Stefano Ermon,~Nikil_Roashan_Selvam1; ~Amil_Merchant1; ~Stefano_Ermon1,"{'value': ['diffusion', 'sampling', 'parallel']}","{'value': 'In diffusion models, samples are generated through an iterative refinement process, requiring hundreds of sequential model evaluations. Several recent methods have introduced approximations (fewer discretization steps or distillation) to trade off speed at the cost of sample quality. In contrast, we introduce Self-Refining Diffusion Samplers (SRDS) that retain sample quality and can improve latency at the cost of additional parallel compute. We take inspiration from the Parareal algorithm, a popular numerical method for parallel-in-time integration of differential equations. In SRDS, a quick but rough estimate of a sample is first created and then iteratively refined in parallel through Parareal iterations. SRDS is not only guaranteed to accurately solve the ODE and converge to the serial solution but also benefits from parallelization across the diffusion trajectory, enabling batched inference and pipelining. As we demonstrate for pre-trained diffusion models, the early convergence of this refinement procedure drastically reduces the number of steps required to produce a sample, speeding up generation for instance by up to 1.7x on a 25-step StableDiffusion-v2 benchmark and up to 4.3x on longer trajectories.'}",https://openreview.net{'value': '/pdf/f3027e409105679c2ff8905ad7351a1f5ec1463a.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=XHTl2k1LYk,{'value': 'Absorb & Escape: Overcoming Single Model Limitations in Generating Heterogeneous Genomic Sequences'},Zehui Li; Yuhao Ni; Guoxuan Xia; William Beardall; Akashaditya Das; Guy-Bart Stan; Yiren Zhao,~Zehui_Li2; ~Yuhao_Ni1; ~Guoxuan_Xia1; ~William_Beardall1; ~Akashaditya_Das1; ~Guy-Bart_Stan1; ~Yiren_Zhao2,"{'value': ['Computational Biology', 'Genomics', 'Deep Learning', 'Generative Model']}","{'value': 'Recent advances in immunology and synthetic biology have accelerated the development of deep generative methods for DNA sequence design. Two dominant approaches in this field are AutoRegressive (AR) models and Diffusion Models (DMs). However, genomic sequences are functionally heterogeneous, consisting of multiple connected regions (e.g., Promoter Regions, Exons, and Introns) where elements within each region come from the same probability distribution, but the overall sequence is non-homogeneous. This heterogeneous nature presents challenges for a single model to accurately generate genomic sequences. In this paper, we analyze the properties of AR models and DMs in heterogeneous genomic sequence generation, pointing out crucial limitations in both methods: (i) AR models capture the underlying distribution of data by factorizing and learning the transition probability but fail to capture the global property of DNA sequences. (ii) DMs learn to recover the global distribution but tend to produce errors at the base pair level. To overcome the limitations of both approaches, we propose a post-training sampling method, termed Absorb & Escape (A&E) to perform compositional generation from AR models and DMs. This approach starts with samples generated by DMs and refines the sample quality using an AR model through the alternation of the Absorb and Escape steps.  To assess the quality of generated sequences, we conduct extensive experiments on 15 species for conditional and unconditional DNA generation. The experiment results from motif distribution, diversity checks, and genome integration tests unequivocally show that A&E outperforms state-of-the-art AR models and DMs in genomic sequence generation. A&E does not suffer from the slowness of traditional MCMC to sample from composed distributions with Energy-Based Models whilst it obtains higher quality samples than single models. Our research sheds light on the limitations of current single-model approaches in DNA generation and provides a simple but effective solution for heterogeneous sequence generation. Code is available at the [Github Repo](https://github.com/Zehui127/Absorb-Escape).'}",https://openreview.net{'value': '/pdf/555cf2c047d3d5fa2cdaa0359854746ab4eab8c3.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=XErWgdxaFU,{'value': 'Textual Training for the Hassle-Free Removal of Unwanted Visual Data: Case Studies on OOD and Hateful Image Detection'},Saehyung Lee; Jisoo Mok; Sangha Park; Yongho Shin; Dahuin Jung; Sungroh Yoon,~Saehyung_Lee1; ~Jisoo_Mok1; ~Sangha_Park2; ~Yongho_Shin3; ~Dahuin_Jung2; ~Sungroh_Yoon1,"{'value': ['Vision-Language models', 'Multimodal models', 'CLIP', 'unwanted visual data detection', 'text-only training', 'hateful image detection']}","{'value': 'In our study, we explore methods for detecting unwanted content lurking in visual datasets. We provide a theoretical analysis demonstrating that a model capable of successfully partitioning visual data can be obtained using only textual data. Based on the analysis, we propose Hassle-Free Textual Training (HFTT), a streamlined method capable of acquiring detectors for unwanted visual content, using only textual data in conjunction with pre-trained vision-language models. HFTT features an innovative objective function that significantly reduces the necessity for human involvement in data annotation. Furthermore, HFTT employs a clever textual data synthesis method, effectively emulating the integration of unknown visual data distribution into the training process at no extra cost. The unique characteristics of HFTT extend its utility beyond traditional out-of-distribution detection, making it applicable to tasks that address more abstract concepts. We complement our analyses with experiments in hateful image detection and out-of-distribution detection. Our codes are available at https://github.com/HFTT-anonymous/HFTT.'}",https://openreview.net{'value': '/pdf/7f2d755399ac446b553342aaa3457233169cc01b.pdf'},{'abstract_filter': 'Data Synthesis'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=XEbPJUQzs3,{'value': 'Prospective Learning: Learning for a Dynamic Future'},Ashwin De Silva; Rahul Ramesh; Rubing Yang; Siyu Yu; Joshua T Vogelstein; Pratik Chaudhari,~Ashwin_De_Silva1; ~Rahul_Ramesh2; ~Rubing_Yang1; ~Siyu_Yu3; ~Joshua_T_Vogelstein1; ~Pratik_Chaudhari1,"{'value': ['Distribution Shifts', 'Learning Theory']}","{'value': 'In real-world applications, the distribution of the data, and our goals, evolve over time. The prevailing theoretical framework for studying machine learning, namely probably approximately correct (PAC) learning, largely ignores time. As a consequence, existing strategies to address the dynamic nature of data and goals exhibit poor real-world performance. This paper develops a theoretical framework called\n""Prospective Learning"" that is tailored for situations when the optimal hypothesis changes over time. In PAC learning, empirical risk minimization (ERM) is known to be consistent. We develop a learner called Prospective ERM, which returns a sequence of predictors that  make predictions on future data.  We prove that the risk of prospective ERM converges to the Bayes risk under certain assumptions on the stochastic process  generating the data. Prospective ERM, roughly speaking, incorporates time as an input in addition to the data. We show that standard ERM as done in PAC learning, without incorporating time, can result in failure to learn when distributions are dynamic. Numerical experiments illustrate that prospective ERM can learn synthetic and visual recognition problems constructed from MNIST and CIFAR-10. Code at https://github.com/neurodata/prolearn.'}",https://openreview.net{'value': '/pdf/f02b627ad2408802e8462b2baaf7a4d2af4a3768.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=WvWS8goWyR,{'value': 'Fairness-Aware Estimation of Graphical Models'},Zhuoping Zhou; Davoud Ataee Tarzanagh; Bojian Hou; Qi Long; Li Shen,~Zhuoping_Zhou1; ~Davoud_Ataee_Tarzanagh1; ~Bojian_Hou1; ~Qi_Long1; ~Li_Shen2,"{'value': ['Fairness', 'Graphical Model', 'Optimization']}","{'value': ""This paper examines the issue of fairness in the estimation of graphical models (GMs), particularly Gaussian, Covariance, and Ising models. These models play a vital role in understanding complex relationships in high-dimensional data. However, standard GMs can result in biased outcomes, especially when the underlying data involves sensitive characteristics or protected groups. To address this, we introduce a comprehensive framework designed to reduce bias in the estimation of GMs related to protected attributes. Our approach involves the integration of the pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while maintaining the effectiveness of the GMs. Experimental evaluations on synthetic and real-world datasets demonstrate that our framework effectively mitigates bias without undermining GMs' performance.""}",https://openreview.net{'value': '/pdf/3162a6a2d722c9cf74d690d68501f55429b3312a.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=WoKtFJf9VG,{'value': 'Conjugate Bayesian Two-step Change Point Detection for Hawkes Process'},Zeyue Zhang; Xiaoling LU; Feng Zhou,~Zeyue_Zhang1; ~Xiaoling_LU1; ~Feng_Zhou9,"{'value': ['Change Point Detection', 'Conjugate inference method', 'Bayesian method', 'Hawkes Process']}","{'value': 'The Bayesian two-step change point detection method is popular for the Hawkes process due to its simplicity and intuitiveness. However, the non-conjugacy between the point process likelihood and the prior requires most existing Bayesian two-step change point detection methods to rely on non-conjugate inference methods. These methods lack analytical expressions, leading to low computational efficiency and impeding timely change point detection. To address this issue, this work employs data augmentation to propose a conjugate Bayesian two-step change point detection method for the Hawkes process, which proves to be more accurate and efficient. Extensive experiments on both synthetic and real data demonstrate the superior effectiveness and efficiency of our method compared to baseline methods. Additionally, we conduct ablation studies to explore the robustness of our method concerning various hyperparameters.'}",https://openreview.net{'value': '/pdf/43f391e2419a823a5db13b99a25f36a7e1f98f5b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=WoEXVQcHFw,{'value': 'Gliding over the Pareto Front with Uniform Designs'},Xiaoyuan Zhang; Genghui Li; Xi Lin; Yichi Zhang; Yifan Chen; Qingfu Zhang,~Xiaoyuan_Zhang2; ~Genghui_Li1; ~Xi_Lin2; ~Yichi_Zhang14; ~Yifan_Chen3; ~Qingfu_Zhang1,{'value': ['uniform design; multiobjective optimization;  algorithmic fairness']},"{'value': ""Multiobjective optimization (MOO) plays a critical role in various real-world domains. A major challenge therein is generating $K$ uniform Pareto-optimal solutions to represent the entire Pareto front. To address this issue, this paper firstly introduces \\emph{fill distance} to evaluate the $K$ design points, which provides a quantitative metric for the representativeness of the design. However, directly specifying the optimal design that minimizes the fill distance is nearly intractable due to the nested $\\min-\\max-\\min$ optimization problem. To address this, we propose a surrogate ``max-packing'' design for the fill distance design, which is easier to optimize and leads to a rate-optimal design with a fill distance at most $4\\times$ the minimum value.\n    Extensive experiments on synthetic and real-world benchmarks demonstrate that our proposed paradigm efficiently produces high-quality, representative solutions and outperforms baseline methods.""}",https://openreview.net{'value': '/pdf/6d6d5421377cb4157af97e511b821628cba814f1.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=WftaVkL6G2,{'value': 'Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis'},Michael Crawshaw; Mingrui Liu,~Michael_Crawshaw1; ~Mingrui_Liu2,"{'value': ['federated learning', 'nonconvex optimization', 'cross device', 'heterogeneous data', 'periodic participation']}","{'value': 'In federated learning, it is common to assume that clients are always available to participate in training, which may not be feasible with user devices in practice. Recent works analyze federated learning under more realistic participation patterns, such as cyclic client availability or arbitrary participation. However, all such works either require strong assumptions (e.g., all clients participate almost surely within a bounded window), do not achieve linear speedup and reduced communication rounds, or are not applicable in the general non-convex setting. In this work, we focus on nonconvex optimization and consider participation patterns in which the chance of participation over a fixed window of rounds is equal among all clients, which includes cyclic client availability as a special case. Under this setting, we propose a new algorithm, named Amplified SCAFFOLD, and prove that it achieves linear speedup, reduced communication, and resilience to data heterogeneity simultaneously. In particular, for cyclic participation, our algorithm is proved to enjoy $\\mathcal{O}(\\epsilon^{-2})$ communication rounds to find an $\\epsilon$-stationary point in the non-convex stochastic setting. In contrast, the prior work under the same setting requires $\\mathcal{O}(\\kappa^2 \\epsilon^{-4})$ communication rounds, where $\\kappa$ denotes the data heterogeneity. Therefore, our algorithm significantly reduces communication rounds due to better dependency in terms of $\\epsilon$ and $\\kappa$. Our analysis relies on a fine-grained treatment of the nested dependence between client participation and errors in the control variates, which results in tighter guarantees than previous work. We also provide experimental results with (1) synthetic data and (2) real-world data with a large number of clients $(N = 250)$, demonstrating the effectiveness of our algorithm under periodic client participation.'}",https://openreview.net{'value': '/pdf/ba5343dd71ecad34eb5bc289124b1f6a9058e3e4.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=WSsht66fbC,{'value': 'Safety through feedback in Constrained RL'},Shashank Reddy Chirra; Pradeep Varakantham; Praveen Paruchuri,~Shashank_Reddy_Chirra1; ~Pradeep_Varakantham1; ~Praveen_Paruchuri1,"{'value': ['Constrained RL', 'Cost Inference', 'Human Feedback']}","{'value': ""In safety-critical RL settings, the inclusion of an additional cost function is often favoured over the arduous task of modifying the reward function to ensure the agent's safe behaviour. However, designing or evaluating such a cost function can be prohibitively expensive. For instance, in the domain of self-driving, designing a cost function that encompasses all unsafe behaviours (e.g., aggressive lane changes, risky overtakes) is inherently complex, it must also consider all the actors present in the scene making it expensive to evaluate. In such scenarios, the cost function can be learned from feedback collected offline in between training rounds. This feedback can be system generated or elicited from a human observing the training process. Previous approaches have not been able to scale to complex environments and are constrained to receiving feedback at the state level which can be expensive to collect. To this end, we introduce an approach that scales to more complex domains and extends beyond state-level feedback, thus, reducing the burden on the evaluator. Inferring the cost function in such settings poses challenges, particularly in assigning credit to individual states based on trajectory-level feedback. To address this, we propose a surrogate objective that transforms the problem into a state-level supervised classification task with noisy labels, which can be solved efficiently. Additionally, it is often infeasible to collect feedback for every trajectory generated by the agent, hence, two fundamental questions arise: (1) Which trajectories should be presented to the human? and (2) How many trajectories are necessary for effective learning? To address these questions, we introduce a \\textit{novelty-based sampling} mechanism that selectively involves the evaluator only when the the agent encounters a \\textit{novel} trajectory, and discontinues querying once the trajectories are no longer \\textit{novel}. We showcase the efficiency of our method through experimentation on several benchmark Safety Gymnasium environments and realistic self-driving scenarios. Our method demonstrates near-optimal performance, comparable to when the cost function is known, by relying solely on trajectory-level feedback across multiple domains. This highlights both the effectiveness and scalability of our approach. The code to replicate these results can be found at \\href{https://github.com/shshnkreddy/RLSF}{https://github.com/shshnkreddy/RLSF}""}",https://openreview.net{'value': '/pdf/e7f3312fa2dfe2dbb6bbfcd396ae3babb1cddcfa.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=WOBhJs9gqU,{'value': 'Dual-frame Fluid Motion Estimation with Test-time Optimization and Zero-divergence Loss'},Yifei Zhang; Huan-ang Gao; Zhou Jiang; Hao Zhao,~Yifei_Zhang14; ~Huan-ang_Gao1; ~Zhou_Jiang1; ~Hao_Zhao1,"{'value': ['Dual-frame Fluid Motion Estimation', 'Test-time Optimization', 'Self-supervision', 'Data Efficiency', 'Cross-domain Robustness']}","{'value': '3D particle tracking velocimetry (PTV) is a key technique for analyzing turbulent flow, one of the most challenging computational problems of our century. At the core of 3D PTV is the dual-frame fluid motion estimation algorithm, which tracks particles across two consecutive frames. Recently, deep learning-based methods have achieved impressive accuracy in dual-frame fluid motion estimation; however, they heavily depend on large volumes of labeled data. In this paper, we introduce a new method that is **completely self-supervised and notably outperforms its fully-supervised counterparts while requiring only 1\\% of the training samples (without labels) used by previous methods.** Our method features a novel zero-divergence loss that is specific to the domain of turbulent flow. Inspired by the success of splat operation in high-dimensional filtering and random fields, we propose a splat-based implementation for this loss which is both efficient and effective. The self-supervised nature of our method naturally supports test-time optimization, leading to the development of a tailored Dynamic Velocimetry Enhancer (DVE) module. We demonstrate that strong cross-domain robustness is achieved through test-time optimization on unseen leave-one-out synthetic domains and real physical/biological domains. Code, data and models are available at [https://github.com/Forrest-110/FluidMotionNet](https://github.com/Forrest-110/FluidMotionNet).'}",https://openreview.net{'value': '/pdf/d4efe1a9d94fd478394e43a144cf0601fb10da39.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=WK2KxPAMQv,{'value': 'Exploiting Representation Curvature for Boundary Detection in Time Series'},Yooju Shin; Jaehyun Park; Susik Yoon; Hwanjun Song; Byung Suk Lee; Jae-Gil Lee,~Yooju_Shin1; ~Jaehyun_Park4; ~Susik_Yoon1; ~Hwanjun_Song2; ~Byung_Suk_Lee1; ~Jae-Gil_Lee1,"{'value': ['time series', 'representation', 'boundary detection']}","{'value': '*Boundaries* are the timestamps at which a class in a time series changes. Recently, representation-based boundary detection has gained popularity, but its emphasis on consecutive distance difference backfires, especially when the changes are gradual. In this paper, we propose a boundary detection method, **RECURVE**, based on a novel change metric, the ***curvature*** of a representation trajectory, to accommodate both gradual and abrupt changes. Here, a sequence of representations in the representation space is interpreted as a trajectory, and a curvature at each timestamp can be computed. Using the theory of random walk, we formally show that the mean curvature is lower near boundaries than at other points. Extensive experiments using diverse real-world time-series datasets confirm the superiority of RECURVE over state-of-the-art methods.'}",https://openreview.net{'value': '/pdf/195ca567f4ffb81f9709969358dffd0069188ebe.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=WEs4WMzndY,{'value': 'Annealed Multiple Choice Learning: Overcoming limitations of Winner-takes-all with annealing'},David Perera; Victor Letzelter; Theo Mariotte; Adrien Cortes; Mickael Chen; Slim Essid; Gaël Richard,~David_Perera1; ~Victor_Letzelter1; ~Theo_Mariotte1; ~Adrien_Cortes1; ~Mickael_Chen1; ~Slim_Essid1; ~Gaël_Richard1,"{'value': ['multiple choice learning', 'winner-takes-all', 'deterministic annealing', 'uncertainty quantification']}","{'value': 'We introduce Annealed Multiple Choice Learning (aMCL) which combines simulated annealing with MCL. MCL is a learning framework handling ambiguous tasks by predicting a small set of plausible hypotheses. These hypotheses are trained using the Winner-takes-all (WTA) scheme, which promotes the diversity of the predictions. However, this scheme may converge toward an arbitrarily suboptimal local minimum, due to the greedy nature of WTA. We overcome this limitation using annealing, which enhances the exploration of the hypothesis space during training. We leverage insights from statistical physics and information theory to provide a detailed description of the model training trajectory. Additionally, we validate our algorithm by extensive experiments on synthetic datasets, on the standard UCI benchmark, and on speech separation.'}",https://openreview.net{'value': '/pdf/6ac9edf5232e04f5d207eff6e8082df8ad435614.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=WCnJmb7cv1,{'value': 'Learning to Assist Humans without Inferring Rewards'},Vivek Myers; Evan Ellis; Sergey Levine; Benjamin Eysenbach; Anca Dragan,~Vivek_Myers1; ~Evan_Ellis1; ~Sergey_Levine1; ~Benjamin_Eysenbach1; ~Anca_Dragan1,"{'value': ['Human-AI Collaboration', 'Unsupervised Reinforcement Learning']}","{'value': ""Assistive agents should make humans' lives easier. Classically, such assistance is studied through the lens of inverse reinforcement learning, where an assistive agent (e.g., a chatbot, a robot) infers a human's intention and then selects actions to help the human reach that goal. This approach requires inferring intentions, which can be difficult in high-dimensional settings. We build upon prior work that studies assistance through the lens of empowerment: an assistive agent aims to maximize the influence of the human's actions such that they exert a greater control over the environmental outcomes and can solve tasks in fewer steps. We lift the major limitation of prior work in this area—scalability to high-dimensional settings—with contrastive successor representations. We formally prove that these representations estimate a similar notion of empowerment to that studied by prior work and provide a ready-made mechanism for optimizing it. Empirically, our proposed method outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a cooperative game setting. Theoretically, our work connects ideas from information theory, neuroscience, and reinforcement learning, and charts a path for representations to play a critical role in solving assistive problems. Our code is available at https://github.com/vivekmyers/empowerment_successor_representations.""}",https://openreview.net{'value': '/pdf/8dcb15a417bba528b0aecb9d1bdb8958d2a06698.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=WBLPlszJI5,{'value': 'Fine-Tuning Personalization in Federated Learning to Mitigate Adversarial Clients'},Youssef Allouah; Abdellah El Mrini; Rachid Guerraoui; Nirupam Gupta; Rafael Pinot,~Youssef_Allouah1; ~Abdellah_El_Mrini1; ~Rachid_Guerraoui1; ~Nirupam_Gupta1; ~Rafael_Pinot1,"{'value': ['Personalized Federated Learning', 'Optimization', 'Generalization', 'Byzantine Robustness']}","{'value': 'Federated learning (FL) is an appealing paradigm that allows a group of machines\n(a.k.a. clients) to learn collectively while keeping their data local. However, due\nto the heterogeneity between the clients’ data distributions, the model obtained\nthrough the use of FL algorithms may perform poorly on some client’s data.\nPersonalization addresses this issue by enabling each client to have a different\nmodel tailored to their own data while simultaneously benefiting from the other\nclients’ data. We consider an FL setting where some clients can be adversarial, and\nwe derive conditions under which full collaboration fails. Specifically, we analyze\nthe generalization performance of an interpolated personalized FL framework in the\npresence of adversarial clients, and we precisely characterize situations when full\ncollaboration performs strictly worse than fine-tuned personalization. Our analysis\ndetermines how much we should scale down the level of collaboration, according\nto data heterogeneity and the tolerable fraction of adversarial clients. We support\nour findings with empirical results on mean estimation and binary classification\nproblems, considering synthetic and benchmark image classification datasets'}",https://openreview.net{'value': '/pdf/ad063cdeb4f3b6803f199dec86e20a67e72bd03f.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Vq2kzpig8v,{'value': 'Reciprocal Reward Influence Encourages Cooperation From Self-Interested Agents'},John Luoyu Zhou; Weizhe Hong; Jonathan Kao,~John_Luoyu_Zhou1; ~Weizhe_Hong1; ~Jonathan_Kao1,"{'value': ['cooperation', 'reinforcement learning', 'opponent shaping', 'multi-agent reinforcement learning']}","{'value': ""Cooperation between self-interested individuals is a widespread phenomenon in the natural world, but remains elusive in interactions between artificially intelligent agents. Instead, naïve reinforcement learning algorithms typically converge to Pareto-dominated outcomes in even the simplest of social dilemmas. An emerging literature on opponent shaping has demonstrated the ability to reach prosocial outcomes by influencing the learning of other agents. However, such methods differentiate through the learning step of other agents or optimize for meta-game dynamics, which rely on privileged access to opponents' learning algorithms or exponential sample complexity, respectively. To provide a learning rule-agnostic and sample-efficient alternative, we introduce Reciprocators, reinforcement learning agents which are intrinsically motivated to reciprocate the influence of opponents' actions on their returns. This approach seeks to modify other agents' $Q$-values by increasing their return following beneficial actions (with respect to the Reciprocator) and decreasing it after detrimental actions, guiding them towards mutually beneficial actions without directly differentiating through a model of their policy. We show that Reciprocators can be used to promote cooperation in temporally extended social dilemmas during simultaneous learning. Our code is available at https://github.com/johnlyzhou/reciprocator/.""}",https://openreview.net{'value': '/pdf/6a63d40c10be7c3fc5acbe88bdfa17b9410ddce0.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=VR2RdSxtzs,{'value': 'MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems'},Bin Lei; Yi Zhang; Shan Zuo; Ali Payani; Caiwen Ding,~Bin_Lei1; ~Yi_Zhang90; ~Shan_Zuo1; ~Ali_Payani1; ~Caiwen_Ding1,"{'value': ['Multi-Agent', 'Prompting', 'LLM', 'Math problem']}","{'value': 'Recent advancements in large language models, such as GPT-4, have demonstrated remarkable capabilities in processing standard queries. Despite these advancements, their performance substantially declines in advanced mathematical problems requiring complex, multi-step logical reasoning. To enhance their inferential capabilities, current research has delved into prompting engineering, exemplified by methodologies such as the Tree of Thought and Graph of Thought.\nNonetheless, these existing approaches encounter two significant limitations. Firstly, their effectiveness in tackling complex mathematical problems is somewhat constrained. Secondly, the necessity to design distinct prompts for individual problems hampers their generalizability.\nIn response to these limitations, this paper introduces the Multi-Agent System for conditional Mining (MACM) prompting method. It not only resolves intricate mathematical problems but also demonstrates strong generalization capabilities across various mathematical contexts.\nWith the assistance of MACM, the accuracy of GPT-4 Turbo on the most challenging level five mathematical problems in the MATH dataset increase from $\\mathbf{54.68\\\\%}  \\text{ to } \\mathbf{76.73\\\\%}$.'}",https://openreview.net{'value': '/pdf/361f70e303eefe87ee3e42f46fb2d3d21347df37.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=VFpXYBqMSU,{'value': 'Slight Corruption in Pre-training Data Makes Better Diffusion Models'},Hao Chen; Yujin Han; Diganta Misra; Xiang Li; Kai Hu; Difan Zou; Masashi Sugiyama; Jindong Wang; Bhiksha Raj,~Hao_Chen15; ~Yujin_Han1; ~Diganta_Misra1; ~Xiang_Li35; ~Kai_Hu2; ~Difan_Zou1; ~Masashi_Sugiyama1; ~Jindong_Wang4; ~Bhiksha_Raj1,"{'value': ['pre-training noise', 'diffusion models', 'latent diffusion models', 'diffusion transformers', 'latent consistency models']}","{'value': 'Diffusion models (DMs) have shown remarkable capabilities in generating realistic high-quality images, audios, and videos. \nThey benefit significantly from extensive pre-training on large-scale datasets, including web-crawled data with paired data and conditions, such as image-text and image-class pairs.\nDespite rigorous filtering, these pre-training datasets often inevitably contain corrupted pairs where conditions do not accurately describe the data. \nThis paper presents the first comprehensive study on the impact of such corruption in pre-training data of DMs.\nWe synthetically corrupt ImageNet-1K and CC3M to pre-train and evaluate over $50$ conditional DMs. \nOur empirical findings reveal that various types of slight corruption in pre-training can significantly enhance the quality, diversity, and fidelity of the generated images across different DMs, both during pre-training and downstream adaptation stages. \nTheoretically, we consider a Gaussian mixture model and prove that slight corruption in the condition leads to higher entropy and a reduced 2-Wasserstein distance to the ground truth of the data distribution generated by the corruptly trained DMs.\nInspired by our analysis, we propose a simple method to improve the training of DMs on practical datasets by adding condition embedding perturbations (CEP).\nCEP significantly improves the performance of various DMs in both pre-training and downstream tasks.\nWe hope that our study provides new insights into understanding the data and pre-training processes of DMs.'}",https://openreview.net{'value': '/pdf/83e7547eb58a1bedcc438af00577dddd6fca0c4c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=V75gAxpW40,{'value': 'Gradient-Variation Online Learning under Generalized Smoothness'},Yan-Feng Xie; Peng Zhao; Zhi-Hua Zhou,~Yan-Feng_Xie1; ~Peng_Zhao1; ~Zhi-Hua_Zhou2,"{'value': ['online learning', 'generalized smoothness', 'gradient-variation regret bound', 'Lipschitz-adaptive algorithm']}","{'value': 'Gradient-variation online learning aims to achieve regret guarantees that scale with variations in the gradients of online functions, which is crucial for attaining fast convergence in games and robustness in stochastic optimization, hence receiving increased attention. Existing results often require the smoothness condition by imposing a fixed bound on gradient Lipschitzness, which may be unrealistic in practice. Recent efforts in neural network optimization suggest a generalized smoothness condition, allowing smoothness to correlate with gradient norms. In this paper, we systematically study gradient-variation online learning under generalized smoothness. We extend the classic optimistic mirror descent algorithm to derive gradient-variation regret by analyzing stability over the optimization trajectory and exploiting smoothness locally. Then, we explore universal online learning, designing a single algorithm with the optimal gradient-variation regrets for convex and strongly convex functions simultaneously, without requiring prior knowledge of curvature. This algorithm adopts a two-layer structure with a meta-algorithm running over a group of base-learners. To ensure favorable guarantees, we design a new Lipschitz-adaptive meta-algorithm, capable of handling potentially unbounded gradients while ensuring a second-order bound to effectively ensemble the base-learners. Finally, we provide the applications for fast-rate convergence in games and stochastic extended adversarial optimization.'}",https://openreview.net{'value': '/pdf/8ac8aa7ad9b078302592ecb5fb25ad2860c16ca3.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=V0JvwCQlJe,{'value': 'FairWire: Fair Graph Generation'},Oyku Deniz Kose; Yanning Shen,~Oyku_Deniz_Kose1; ~Yanning_Shen1,"{'value': ['Trustworthy ML', 'Learning over Graphs', 'Fair ML over graphs', 'Fair Graph Generation']}","{'value': 'Machine learning over graphs has recently attracted growing attention due to its ability to analyze and learn complex relations within critical interconnected systems. However, the disparate impact that is amplified by the use of biased graph structures in these algorithms has raised significant concerns for their deployment in real-world decision systems. In addition, while synthetic graph generation has become pivotal for privacy and scalability considerations, the impact of generative learning algorithms on structural bias has not yet been investigated. Motivated by this, this work focuses on the analysis and mitigation of structural bias for both real and synthetic graphs. Specifically, we first theoretically analyze the sources of structural bias that result in disparity for the predictions of dyadic relations. To alleviate the identified bias factors, we design a novel fairness regularizer that offers a versatile use. Faced with the bias amplification in graph generation models brought to light in this work, we further propose a fair graph generation framework, FairWire, by leveraging our fair regularizer design in a generative model. Experimental results on real-world networks validate that the proposed tools herein deliver effective structural bias mitigation for both real and synthetic graphs.'}",https://openreview.net{'value': '/pdf/1f5eea2983c84e12175cd2b978aa11cb3f7ce158.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Uz804qLJT2,{'value': 'Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers'},Lorenzo Tiberi; Francesca Mignacco; Kazuki Irie; Haim Sompolinsky,~Lorenzo_Tiberi2; ~Francesca_Mignacco1; ~Kazuki_Irie1; ~Haim_Sompolinsky1,"{'value': ['Deep learning theory', 'Statistical mechanics', 'Transformers', 'Kernel Methods', 'Gaussian Processes', 'Finite Width Networks']}","{'value': 'Despite the remarkable empirical performance of Transformers, their theoretical understanding remains elusive. Here, we consider a deep multi-head self-attention network, that is closely related to Transformers yet analytically tractable. We develop a statistical mechanics theory of Bayesian learning in this model, deriving exact equations for the network\'s predictor statistics under the finite-width thermodynamic limit, i.e., $N,P\\rightarrow\\infty$, $P/N=\\mathcal{O}(1)$, where $N$ is the network width and $P$ is the number of training examples. Our theory shows that the predictor statistics are expressed as a sum of independent kernels, each one pairing different ""attention paths"", defined as information pathways through different attention heads across layers. The kernels are weighted according to a ""task-relevant kernel combination"" mechanism that aligns the total kernel with the task labels. As a consequence, this interplay between attention paths enhances generalization performance. Experiments confirm our findings on both synthetic and real-world sequence classification tasks. Finally, our theory explicitly relates the kernel combination mechanism to properties of the learned weights, allowing for a qualitative transfer of its insights to models trained via gradient descent. As an illustration, we demonstrate an efficient size reduction of the network, by pruning those attention heads that are deemed less relevant by our theory.'}",https://openreview.net{'value': '/pdf/82a9485b78dea1bba26fd25da466eb55c3f2205e.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Ur00BNk1v2,{'value': 'GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing'},Zhenyu Wang; Aoxue Li; Zhenguo Li; Xihui Liu,~Zhenyu_Wang3; ~Aoxue_Li2; ~Zhenguo_Li1; ~Xihui_Liu1,{'value': ['image generation']},"{'value': 'Despite the success achieved by existing image generation and editing methods, current models still struggle with complex problems including intricate text prompts, and the absence of  verification and self-correction mechanisms makes the generated images unreliable. Meanwhile, a single model tends to specialize in particular tasks and possess the corresponding capabilities, making it inadequate for fulfilling all user requirements. We propose GenArtist, a unified image generation and editing system, coordinated by a multimodal large language model (MLLM) agent. We integrate a comprehensive range of existing models into the tool library and utilize the agent for tool selection and execution. For a complex problem, the MLLM agent decomposes it into simpler sub-problems and constructs a tree structure to systematically plan the procedure of generation, editing, and self-correction with step-by-step verification. By automatically generating missing position-related inputs and incorporating position information, the appropriate tool can be effectively employed to address each sub-problem. Experiments demonstrate that GenArtist can perform various generation and editing tasks, achieving state-of-the-art performance and surpassing existing models such as SDXL and DALL-E 3, as can be seen in Fig. 1. We will open-source the code for future research and applications.'}",https://openreview.net{'value': '/pdf/f258668c755324b43c7684198fb87a097e4dd1b5.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Ul3lDYo3XQ,{'value': 'AGILE: A Novel Reinforcement Learning Framework of LLM Agents'},Peiyuan Feng; Yichen He; Guanhua Huang; Yuan Lin; Hanchong Zhang; Yuchen Zhang; Hang Li,~Peiyuan_Feng1; ~Yichen_He1; ~Guanhua_Huang1; ~Yuan_Lin3; ~Hanchong_Zhang1; ~Yuchen_Zhang1; ~Hang_Li4,"{'value': ['LLM agent', 'reinforcement learning', 'LLM-human interaction']}","{'value': ""We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments)  designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE.""}",https://openreview.net{'value': '/pdf/59dcbea256d8ee883de6d1ec9925a9dda78eb277.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=UahrHR5HQh,{'value': 'Variational Flow Matching for Graph Generation'},Floor Eijkelboom; Grigory Bartosh; Christian A. Naesseth; Max Welling; Jan-Willem van de Meent,~Floor_Eijkelboom1; ~Grigory_Bartosh1; ~Christian_A._Naesseth1; ~Max_Welling1; ~Jan-Willem_van_de_Meent1,"{'value': ['generative modeling', 'flow matching', 'variational inference', 'categorical', 'discrete', 'graph generation', 'molecular generation']}","{'value': 'We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM). We use this formulation to develop CatFlow, a flow matching method for categorical data that is easy to implement, computationally efficient, and achieves strong results on graph generation tasks. In VFM, the objective is to approximate the posterior probability path, which is a distribution over possible end points of a trajectory. VFM admits both the original flow matching objective and the CatFlow objective as special cases. We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective. We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks. In all cases, CatFlow exceeds or matches performance of the current state-of-the-art models.'}",https://openreview.net{'value': '/pdf/05079ddca048cf162523c0e9541b7cdb50127f8f.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=UGlDVc0GTU,{'value': 'LLM-based Skill Diffusion for Zero-shot Policy Adaptation'},Woo Kyung Kim; Youngseok Lee; Jooyoung Kim; Honguk Woo,~Woo_Kyung_Kim1; ~Youngseok_Lee2; ~Jooyoung_Kim1; ~Honguk_Woo1,"{'value': ['Imitation Learning', 'Planning', 'Diffusion Model', 'Large Language Model']}","{'value': 'Recent advances in data-driven imitation learning and offline reinforcement learning have highlighted the use of expert data for skill acquisition and the development of hierarchical policies based on these skills. However, these approaches have not significantly advanced in adapting these skills to unseen contexts, which may involve changing environmental conditions or different user requirements. In this paper, we present a novel LLM-based policy adaptation framework LDuS which leverages an LLM to guide the generation process of a skill diffusion model upon contexts specified in language, facilitating zero-shot skill-based policy adaptation to different contexts. To implement the skill diffusion model, we adapt the loss-guided diffusion with a sequential in-painting technique, where target trajectories are conditioned by masking them with past state-action sequences, thereby enabling the robust and controlled generation of skill trajectories in test-time. To have a loss function for a given context, we employ the LLM-based code generation with iterative refinement, by which the code and controlled trajectory are validated to align with the context in a closed-loop manner. Through experiments, we demonstrate the zero-shot adaptability of LDuS to various context types including different specification levels, multi-modality, and varied temporal conditions for several robotic manipulation tasks, outperforming other language-conditioned imitation and planning methods.'}",https://openreview.net{'value': '/pdf/b6f0b49a3aa38dc5aaa6b697129957b4d3fabfcb.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=U9MzoDOKZu,{'value': 'Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement'},Zhi Wang; Li Zhang; Wenhao Wu; Yuanheng Zhu; Dongbin Zhao; Chunlin Chen,~Zhi_Wang7; ~Li_Zhang43; ~Wenhao_Wu13; ~Yuanheng_Zhu1; ~Dongbin_Zhao1; ~Chunlin_Chen1,"{'value': ['decision transformer', 'offline meta reinforcement learning', 'world model']}","{'value': 'A longstanding goal of artificial general intelligence is highly capable generalists that can learn from diverse experiences and generalize to unseen tasks. The language and vision communities have seen remarkable progress toward this trend by scaling up transformer-based models trained on massive datasets, while reinforcement learning (RL) agents still suffer from poor generalization capacity under such paradigms. To tackle this challenge, we propose Meta Decision Transformer (Meta-DT), which leverages the sequential modeling ability of the transformer architecture and robust task representation learning via world model disentanglement to achieve efficient generalization in offline meta-RL. We pretrain a context-aware world model to learn a compact task representation, and inject it as a contextual condition to the causal transformer to guide task-oriented sequence generation. Then, we subtly utilize history trajectories generated by the meta-policy as a self-guided prompt to exploit the architectural inductive bias. We select the trajectory segment that yields the largest prediction error on the pretrained world model to construct the prompt, aiming to encode task-specific information complementary to the world model maximally. Notably, the proposed framework eliminates the requirement of any expert demonstration or domain knowledge at test time. Experimental results on MuJoCo and Meta-World benchmarks across various dataset types show that Meta-DT exhibits superior few and zero-shot generalization capacity compared to strong baselines while being more practical with fewer prerequisites. Our code is available at https://github.com/NJU-RL/Meta-DT.'}",https://openreview.net{'value': '/pdf/cc9d77765bb93053c4d1798a94ec9c8a23501afd.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=U3Rgdb4li9,{'value': 'Targeted Sequential Indirect Experiment Design'},Elisabeth Ailer; Niclas Dern; Jason Hartford; Niki Kilbertus,~Elisabeth_Ailer1; ~Niclas_Dern1; ~Jason_Hartford1; ~Niki_Kilbertus1,"{'value': ['causality', 'experiment design', 'instrumental variables', 'indirect experiments']}","{'value': 'Scientific hypotheses typically concern specific aspects of complex, imperfectly understood or entirely unknown mechanisms, such as the effect of gene expression levels on phenotypes or how microbial communities influence environmental health. Such queries are inherently causal (rather than purely associational), but in many settings, experiments can not be conducted directly on the target variables of interest, but are indirect. Therefore, they perturb the target variable, but do not remove potential confounding factors. If, additionally, the resulting experimental measurements are high-dimensional and the studied mechanisms nonlinear, the query of interest is generally not identified. We develop an adaptive strategy to design indirect experiments that optimally inform a targeted query about the ground truth mechanism in terms of sequentially narrowing the gap between an upper and lower bound on the query. While the general formulation consists of a bi-level optimization procedure, we derive an efficiently estimable analytical kernel-based estimator of the bounds for the causal effect, a query of key interest, and demonstrate the efficacy of our approach in confounded, multivariate, nonlinear synthetic settings.'}",https://openreview.net{'value': '/pdf/d6dbe3094fc9ef1881705acaf521213cc8dcb314.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Twqa0GFMGX,{'value': 'Idiographic Personality Gaussian Process for Psychological Assessment'},Yehu Chen; Muchen Xi; Joshua J. Jackson; Jacob Montgomery; Roman Garnett,~Yehu_Chen1; ~Muchen_Xi1; ~Joshua_J._Jackson1; ~Jacob_Montgomery1; ~Roman_Garnett1,"{'value': ['Applications -- Cognitive science', 'Gaussian process', 'Latent variable model']}","{'value': 'We develop a novel measurement framework based on Gaussian process coregionalization model to address a long-lasting debate in psychometrics: whether psychological features like personality share a common structure across the population or vary uniquely for individuals. We propose idiographic personality Gaussian process (IPGP), an intermediate model that accommodates both shared trait structure across individuals and ""idiographic"" deviations. IPGP leverages the Gaussian process coregionalization model to conceptualize responses of grouped survey batteries but adjusted to non-Gaussian ordinal data, and exploits stochastic variational inference for latent factor estimation. Using both synthetic data and a novel survey, we show that IPGP improves both prediction of actual responses and estimation of intrapersonal response patterns compared to existing benchmarks. In the survey study, IPGP also identifies unique clusters of personality taxonomies, displaying great potential in advancing individualized approaches to psychological diagnosis.'}",https://openreview.net{'value': '/pdf/3804612064e4761ae7d673a4cf4fa64a0e9b34ef.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Tw9nfNyOMy,{'value': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability'},Shenyuan Gao; Jiazhi Yang; Li Chen; Kashyap Chitta; Yihang Qiu; Andreas Geiger; Jun Zhang; Hongyang Li,~Shenyuan_Gao1; ~Jiazhi_Yang1; ~Li_Chen15; ~Kashyap_Chitta1; ~Yihang_Qiu1; ~Andreas_Geiger3; ~Jun_Zhang25; ~Hongyang_Li1,"{'value': ['World Model', 'Autonomous Driving', 'Video Prediction']}","{'value': 'World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving. Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application. In this paper, we present Vista, a generalizable driving world model with high fidelity and versatile controllability. Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations. To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information. We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts. For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy. After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios. Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over 70% of comparisons and surpasses the best-performing driving world model by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.'}",https://openreview.net{'value': '/pdf/692ed830a7f68a7b50cedc94e3ddc18cff8dd692.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=TusuJSbRxm,{'value': 'Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear $q^\\pi$-Realizability and Concentrability'},Volodymyr Tkachuk; Gellért Weisz; Csaba Szepesvari,~Volodymyr_Tkachuk2; ~Gellért_Weisz2; ~Csaba_Szepesvari1,"{'value': ['reinforcement learning', 'learning theory', 'offline RL', 'batch RL']}","{'value': 'We consider offline reinforcement learning (RL) in $H$-horizon Markov decision processes (MDPs) under the linear $q^\\pi$-realizability assumption, where the action-value function of every policy is linear with respect to a given $d$-dimensional feature function. The hope in this setting is that learning a good policy will be possible without requiring a sample size that scales with the number of states in the MDP. Foster et al. [2021] have shown this to be impossible even under $\\text{\\textit{concentrability}}$, a data coverage assumption where a coefficient $C_\\text{conc}$ bounds the extent to which the state-action distribution of any policy can veer off the data distribution. However, the data in this previous work was in the form of a sequence of individual transitions. This leaves open the question of whether the negative result mentioned could be overcome if the data was composed of sequences of full trajectories. In this work we answer this question positively by proving that with trajectory data, a dataset of size $\\text{poly}(d,H,C_\\text{conc})/\\epsilon^2$ is sufficient for deriving an $\\epsilon$-optimal policy, regardless of the size of the state space. The main tool that makes this result possible is due to Weisz et al. [2023], who demonstrate that linear MDPs can be used to approximate linearly $q^\\pi$-realizable MDPs. The connection to trajectory data is that the linear MDP approximation relies on ""skipping"" over certain states. The associated estimation problems are thus easy when working with trajectory data, while they remain nontrivial when working with individual transitions. The question of computational efficiency under our assumptions remains open.'}",https://openreview.net{'value': '/pdf/adc12f0636209deeead66591159ce417ba22fb06.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=TuspoNzIdB,{'value': 'Mixture of neural fields for heterogeneous reconstruction in cryo-EM'},Axel Levy; Rishwanth Raghu; David Shustin; Adele Rui-Yang Peng; Huan Li; Oliver Biggs Clarke; Gordon Wetzstein; Ellen D Zhong,~Axel_Levy1; ~Rishwanth_Raghu1; ~David_Shustin1; ~Adele_Rui-Yang_Peng1; ~Huan_Li8; ~Oliver_Biggs_Clarke1; ~Gordon_Wetzstein3; ~Ellen_D_Zhong1,"{'value': ['cryogenic electron microscopy', 'neural representations']}","{'value': 'Cryo-electron microscopy (cryo-EM) is an experimental technique for protein structure determination that images an ensemble of macromolecules in near-physiological contexts. While recent advances enable the reconstruction of dynamic conformations of a single biomolecular complex, current methods do not adequately model samples with mixed conformational and compositional heterogeneity. In particular, datasets containing mixtures of multiple proteins require the joint inference of structure, pose, compositional class, and conformational states for 3D reconstruction. Here, we present Hydra, an approach that models both conformational and compositional heterogeneity fully ab initio by parameterizing structures as arising from one of K neural fields. We employ a hybrid optimization strategy and demonstrate the effectiveness of our approach on synthetic datasets composed of mixtures of proteins with large degrees of conformational variability. We additionally demonstrate Hydra on an experimental dataset imaged of a cellular lysate containing a mixture of different protein complexes. Hydra expands the expressivity of heterogeneous reconstruction methods and thus broadens the scope of cryo-EM to increasingly complex samples.'}",https://openreview.net{'value': '/pdf/d07856c9c651fc7407707b28a0e50bcb3ba546a1.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Tt2xJaxDc4,{'value': 'Randomized Truthful Auctions with Learning Agents'},Gagan Aggarwal; Anupam Gupta; Andres Perlroth; Grigoris Velegkas,~Gagan_Aggarwal1; ~Anupam_Gupta2; ~Andres_Perlroth1; ~Grigoris_Velegkas1,"{'value': ['Auctions', 'No-Regret Learning', 'Revenue Maximization']}","{'value': ""We study a setting where agents use no-regret learning algorithms to participate in repeated auctions. Recently, Kolumbus and Nisan [2022a] showed, rather surprisingly, that when bidders participate in second-price auctions using no-regret bidding algorithms, no matter how large the number of interactions $T$ is, the runner-up bidder may not converge to bidding truthfully. Our first result shows that this holds forall deterministictruthful auctions. We also show that the ratio of the learning rates of different bidders can qualitatively affect the convergence of the bidders. Next, we consider the problem of revenue maximization in this environment. In the setting with fully rational bidders, the seminal result of Myerson [1981] showed that revenue can be maximized by using a second-price auction with reserves. We show that, in stark contrast, in our setting with learning bidders, randomized auctions can have strictly better revenue guarantees than second-price auctions with reserves, when $T$ is large enough. To do this, we provide a black-box transformation from any truthful auction $A$ to an auction $A'$ such that: i) all mean-based no-regret learners that participate in $A'$ converge to bidding truthfully, ii) the distance between the allocation rule and the payment rule between $A, A'$ is negligible. Finally, we study revenue maximization in the non-asymptotic regime. We define a notion of auctioneer regret that compares the revenue generated to the revenue of a second price auction with truthful bids. When the auctioneer has to use the same auction throughout the interaction, we show an (almost) tight regret bound of $\\tilde{\\Theta}(T^{3/4})$. Then, we consider the case where the auctioneer can use different auctions throughout the interaction, but in a way that is oblivious to the bids. For this setting, we show an (almost) tight bound of $\\tilde{\\Theta}(\\sqrt{T})$.""}",https://openreview.net{'value': '/pdf/7876efe7105e4ba1839383e697641e3c90f42f0a.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Tg2EVad7VF,{'value': 'DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation'},Weiting Tan; Jingyu Zhang; Lingfeng Shen; Daniel Khashabi; Philipp Koehn,~Weiting_Tan1; ~Jingyu_Zhang2; ~Lingfeng_Shen1; ~Daniel_Khashabi2; ~Philipp_Koehn2,"{'value': ['speech-to-speech translation', 'non-autoregressive modeling', 'diffusion models']}","{'value': 'Non-autoregressive Transformers (NATs) are recently applied in direct speech-to-speech translation systems, which convert speech across different languages without intermediate text data. Although NATs generate high-quality outputs and offer faster inference than autoregressive models, they tend to produce incoherent and repetitive results due to complex data distribution (e.g., acoustic and linguistic variations in speech). In this work, we introduce DiffNorm, a diffusion-based normalization strategy that simplifies data distributions for training NAT models. After training with a self-supervised noise estimation objective, DiffNorm constructs normalized target data by denoising synthetically corrupted speech features. Additionally, we propose to regularize NATs with classifier-free guidance, improving model robustness and translation quality by randomly dropping out source information during training. Our strategies result in a notable improvement of about $+7$ ASR-BLEU for English-Spanish (En-Es) translation and $+2$ ASR-BLEU for English-French (En-Fr) on the CVSS benchmark, while attaining over $14\\times$ speedup for En-Es and $5 \\times$ speedup for En-Fr translations compared to autoregressive baselines.'}",https://openreview.net{'value': '/pdf/dfbde3a23c3830820369f94e56cf9f26078b5e78.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=TeBKVfhP2M,{'value': 'Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models'},Alliot Nagle; Adway Girish; Marco Bondaschi; Michael Gastpar; Ashok Vardhan Makkuva; Hyeji Kim,~Alliot_Nagle1; ~Adway_Girish1; ~Marco_Bondaschi1; ~Michael_Gastpar1; ~Ashok_Vardhan_Makkuva1; ~Hyeji_Kim1,"{'value': ['information theory', 'prompt compression', 'LLMs', 'optimization']}","{'value': 'We formalize the problem of prompt compression for large language models (LLMs) and present a framework to unify token-level prompt compression methods which create hard prompts for black-box models. We derive the distortion-rate function for this setup as a linear program, and provide an efficient algorithm to compute this fundamental limit via the dual of the linear program. Using the distortion-rate function as the baseline, we study the performance of existing compression schemes on a synthetic dataset consisting of prompts generated from a Markov chain, natural language queries, and their respective answers. Our empirical analysis demonstrates the criticality of query-aware prompt compression, where the compressor has knowledge of the downstream task/query for the black-box LLM. We show that there is a large gap between the performance of current prompt compression methods and the optimal strategy, and propose Adaptive QuerySelect, a query-aware, variable-rate adaptation of a prior work to close the gap. We extend our experiments to a small natural language dataset to further confirm our findings on our synthetic dataset.'}",https://openreview.net{'value': '/pdf/11183e1b176d64417c44f261ee5b9f7fcc5b6dbf.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=TXsRGrzICz,{'value': 'What type of inference is planning?'},Miguel Lazaro-Gredilla; Li Yang Ku; Kevin Patrick Murphy; Dileep George,~Miguel_Lazaro-Gredilla1; ~Li_Yang_Ku1; ~Kevin_Patrick_Murphy1; ~Dileep_George1,"{'value': ['planning', 'variational inference', 'belief propagation', 'message passing']}","{'value': ""Multiple types of inference are available for probabilistic graphical models, e.g., marginal, maximum-a-posteriori, and even marginal maximum-a-posteriori. Which one do researchers mean when they talk about ``planning as inference''? There is no consistency in the literature, different types are used, and their ability to do planning is further entangled with specific approximations or additional constraints. In this work we use the variational framework to show that, just like all commonly used types of inference correspond to different weightings of the entropy terms in the variational problem, planning corresponds _exactly_ to a _different_ set of weights. This means that all the tricks of variational inference are readily applicable to planning. We develop an analogue of loopy belief propagation that allows us to perform approximate planning in factored-state Markov decisions processes without incurring intractability due to the exponentially large state space. The variational perspective shows that the previous types of inference for planning are only adequate in environments with low stochasticity, and allows us to characterize each type by its own merits, disentangling the type of inference from the additional approximations that its practical use requires. We validate these results empirically on synthetic MDPs and tasks posed in the International Planning Competition.""}",https://openreview.net{'value': '/pdf/8c8323aea284a7f25b2e6c2e49de2edd0b188afb.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=TUwWBLjFk9,{'value': 'On the Identifiability of Poisson Branching Structural Causal Model Using Probability Generating Function'},Yu Xiang; Jie Qiao; Zefeng Liang; Zihuai Zeng; Ruichu Cai; Zhifeng Hao,~Yu_Xiang9; ~Jie_Qiao1; ~Zefeng_Liang1; ~Zihuai_Zeng1; ~Ruichu_Cai1; ~Zhifeng_Hao4,{'value': ['Causal Discovery']},"{'value': 'Causal discovery from observational data, especially for count data, is essential across scientific and industrial contexts, such as biology, economics, and network operation maintenance. For this task, most approaches model count data using Bayesian networks or ordinal relations. However, they overlook the inherent branching structures that are frequently encountered, e.g., a browsing event might trigger an adding cart or purchasing event. This can be modeled by a binomial thinning operator (for branching) and an additive independent Poisson distribution (for noising), known as Poisson Branching Structure Causal Model (PB-SCM). There is a provably sound cumulant-based causal discovery method that allows the identification of the causal structure under a branching structure. However, we show that there still remains a gap in that there exist causal directions that are identifiable while the algorithm fails to identify them. In this work, we address this gap by exploring the identifiability of PB-SCM using the Probability Generating Function (PGF). By developing a compact and exact closed-form solution for the PGF of PB-SCM, we demonstrate that each component in this closed-form solution uniquely encodes a specific local structure, enabling the identification of the local structures by testing their corresponding component appearances in the PGF. Building on this, we propose a practical algorithm for learning causal skeletons and identifying causal directions of PB-SCM using PGF. The effectiveness of our method is demonstrated through experiments on both synthetic and real datasets.'}",https://openreview.net{'value': '/pdf/4d2cb1ec6dc22c5e9b9d2289e344d1bf27c1c875.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=T5UfIfmDbq,{'value': 'Monte Carlo Tree Search based Space Transfer for Black Box Optimization'},Shukuan Wang; Ke Xue; Lei Song; Xiaobin Huang; Chao Qian,~Shukuan_Wang1; ~Ke_Xue1; ~Lei_Song4; ~Xiaobin_Huang2; ~Chao_Qian1,"{'value': ['Bayesian optimization', 'Black-box optimization', 'Transfer optimization']}","{'value': 'Bayesian optimization (BO) is a popular method for computationally expensive black-box optimization. However, traditional BO methods need to solve new problems from scratch, leading to slow convergence. Recent studies try to extend BO to a transfer learning setup to speed up the optimization, where search space transfer is one of the most promising approaches and has shown impressive performance on many tasks. However, existing search space transfer methods either lack an adaptive mechanism or are not flexible enough, making it difficult to efficiently identify promising search space during the optimization process. In this paper, we propose a search space transfer learning method based on Monte Carlo tree search (MCTS), called MCTS-transfer, to iteratively divide, select, and optimize in a learned subspace. MCTS-transfer can not only provide a well-performing search space for warm-start but also adaptively identify and leverage the information of similar source tasks to reconstruct the search space during the optimization process. Experiments on synthetic functions, real-world problems, Design-Bench and hyper-parameter optimization show that MCTS-transfer can demonstrate superior performance compared to other search space transfer methods under different settings. Our code is available at \\url{https://github.com/lamda-bbo/mcts-transfer}.'}",https://openreview.net{'value': '/pdf/84c30c02e0108e7327b3caec9ed8ded6e4305f5d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=T07OHxcEYP,{'value': 'Differentially Private Reinforcement Learning with Self-Play'},Dan Qiao; Yu-Xiang Wang,~Dan_Qiao1; ~Yu-Xiang_Wang1,"{'value': ['differential privacy', 'multi-agent reinforcement learning', 'trajectory-wise privacy protection']}","{'value': ""We study the problem of multi-agent reinforcement learning (multi-agent RL) with differential privacy (DP) constraints. This is well-motivated by various real-world applications involving sensitive data, where it is critical to protect users' private information. We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where both definitions ensure trajectory-wise privacy protection. Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type bonuses.  The algorithm is able to satisfy JDP and LDP requirements when instantiated with appropriate privacy mechanisms. Furthermore, for both notions of DP, our regret bound generalizes the best known result under the single-agent RL case, while our regret could also reduce to the best known result for multi-agent RL without privacy constraints. To the best of our knowledge, these are the first results towards understanding trajectory-wise privacy protection in multi-agent RL.""}",https://openreview.net{'value': '/pdf/55d3d366b712716dfd05cc9436dc40d6a158fa4f.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=SxRblm9aMs,{'value': 'Are Graph Neural Networks Optimal Approximation Algorithms?'},Morris Yau; Nikolaos Karalias; Eric Hanqing Lu; Jessica Xu; Stefanie Jegelka,~Morris_Yau3; ~Nikolaos_Karalias1; ~Eric_Hanqing_Lu1; ~Jessica_Xu1; ~Stefanie_Jegelka3,"{'value': ['Combinatorial Optimization', 'Graph Neural Networks', 'Unsupervised Learning']}","{'value': 'In this work we design graph neural network architectures that capture optimal\napproximation algorithms for a large class of combinatorial optimization problems,\nusing powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message-passing algorithms can represent\nthe most powerful polynomial time algorithms for Max Constraint Satisfaction\nProblems assuming the Unique Games Conjecture. We leverage this result to\nconstruct efficient graph neural network architectures, OptGNN, that obtain high quality approximate solutions on landmark combinatorial optimization problems\nsuch as Max-Cut, Min-Vertex-Cover, and Max-3-SAT. Our approach achieves\nstrong empirical results across a wide range of real-world and synthetic datasets\nagainst solvers and neural baselines. Finally, we take advantage of OptGNN’s\nability to capture convex relaxations to design an algorithm for producing bounds\non the optimal solution from the learned embeddings of OptGNN.'}",https://openreview.net{'value': '/pdf/651568ff30d22d475ca6e3ed3d7fe34d455359df.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ShJWT0n7kX,"{'value': ""Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling""}",Yuanqi Du; Michael Plainer; Rob Brekelmans; Chenru Duan; Frank Noe; Carla P Gomes; Alan Aspuru-Guzik; Kirill Neklyudov,~Yuanqi_Du1; ~Michael_Plainer1; ~Rob_Brekelmans1; ~Chenru_Duan1; ~Frank_Noe1; ~Carla_P_Gomes1; ~Alan_Aspuru-Guzik2; ~Kirill_Neklyudov1,"{'value': ['Transition Path Sampling', 'Protein Folding', 'Schrödinger Bridge']}","{'value': ""Rare event sampling in dynamical systems is a fundamental problem arising in the natural sciences, which poses significant computational challenges due to an exponentially large space of trajectories. For settings where the dynamical system of interest follows a Brownian motion with known drift, the question of conditioning the process to reach a given endpoint or desired rare event is definitively answered by Doob's $h$-transform. However, the naive estimation of this transform is infeasible, as it requires simulating sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob's $h$-transform as an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimization, we propose a simulation-free training objective with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks.""}",https://openreview.net{'value': '/pdf/53bda8ac23be462ad0b7da366a8c85d8410b6aa4.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=SXy1nVGyO7,{'value': 'On the Identifiability of Hybrid Deep Generative Models: Meta-Learning as a Solution'},Yubo Ye; Maryam Toloubidokhti; Sumeet Vadhavkar; Xiajun Jiang; Huafeng Liu; Linwei Wang,~Yubo_Ye1; ~Maryam_Toloubidokhti1; ~Sumeet_Vadhavkar1; ~Xiajun_Jiang1; ~Huafeng_Liu1; ~Linwei_Wang1,{'value': ['hybrid modeling; identifiability; meta-learning']},"{'value': 'The interest in leveraging physics-based inductive bias in deep learning has resulted in recent development of _hybrid deep generative models (hybrid-DGMs)_  that integrates known physics-based mathematical expressions in neural generative models. To identify these hybrid-DGMs requires inferring parameters of the physics-based component along with their neural component. The identifiability of these hybrid-DGMs, however, has not yet been theoretically probed or established. How does the existing theory of the un-identifiability of general DGMs apply to hybrid-DGMs? What may be an effective approach to consutrct a hybrid-DGM with theoretically-proven identifiability? This paper provides the first theoretical probe into the identifiability of hybrid-DGMs, and present meta-learning as a novel solution to construct identifiable hybrid-DGMs. On synthetic and real-data benchmarks, we provide strong empirical evidence for the un-identifiability of existing hybrid-DGMs using unconditional priors, and strong identifiability results of the presented meta-formulations of hybrid-DGMs.'}",https://openreview.net{'value': '/pdf/f76c31d643671a10753028290052bccfbe7b85c1.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=SO1aRpwVLk,{'value': '4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models'},Heng Yu; Chaoyang Wang; Peiye Zhuang; Willi Menapace; Aliaksandr Siarohin; Junli Cao; Laszlo Attila Jeni; Sergey Tulyakov; Hsin-Ying Lee,~Heng_Yu3; ~Chaoyang_Wang1; ~Peiye_Zhuang2; ~Willi_Menapace1; ~Aliaksandr_Siarohin1; ~Junli_Cao2; ~Laszlo_Attila_Jeni1; ~Sergey_Tulyakov1; ~Hsin-Ying_Lee2,{'value': ['4D generation;  novel view synthesis; gaussian splatting; text-4D; 4D reconstruction']},"{'value': 'Existing dynamic scene generation methods mostly rely on distilling knowledge from pre-trained 3D generative models, which are typically fine-tuned on synthetic object datasets.\nAs a result, the generated scenes are often object-centric and lack photorealism. \nTo address these limitations, we introduce a novel pipeline designed for photorealistic text-to-4D scene generation, discarding the dependency on multi-view generative models and instead fully utilizing video generative models trained on diverse real-world datasets. \nOur method begins by generating a reference video using the video generation model.\nWe then learn the canonical 3D representation of the video using a freeze-time video, delicately generated from the reference video.\nTo handle inconsistencies in the freeze-time video, we jointly learn a per-frame deformation to model these imperfections.\nWe then learn the temporal deformation based on the canonical representation to capture dynamic interactions in the reference video. \nThe pipeline facilitates the generation of dynamic scenes with enhanced photorealism and structural integrity, viewable from multiple perspectives, thereby setting a new standard in 4D scene generation.'}",https://openreview.net{'value': '/pdf/4f642d74ab748d0b60939c5cc71a9904dbca0da0.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=SKCbZR8Pyd,{'value': 'SpeechAlign: Aligning Speech Generation to Human Preferences'},Dong Zhang; Zhaowei Li; Shimin Li; Xin Zhang; Pengyu Wang; Yaqian Zhou; Xipeng Qiu,~Dong_Zhang9; ~Zhaowei_Li4; ~Shimin_Li1; ~Xin_Zhang36; ~Pengyu_Wang2; ~Yaqian_Zhou1; ~Xipeng_Qiu1,"{'value': ['Speech generation', 'RLHF', 'self-improvement']}","{'value': 'Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of preference optimization to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging preference optimization to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences.  SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models.  Demos are available at https://0nutation.github.io/SpeechAlign.github.io/.'}",https://openreview.net{'value': '/pdf/5be49395a83725680ce034c52f056414051b592f.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=SGcnphYOeq,{'value': 'Parameter-free Clipped Gradient Descent Meets Polyak'},Yuki Takezawa; Han Bao; Ryoma Sato; Kenta Niwa; Makoto Yamada,~Yuki_Takezawa1; ~Han_Bao2; ~Ryoma_Sato1; ~Kenta_Niwa1; ~Makoto_Yamada3,"{'value': ['Polyak stepsize', 'clipped gradient descent', 'generalized smoothness']}","{'value': 'Gradient descent and its variants are de facto standard algorithms for training machine learning models. As gradient descent is sensitive to its hyperparameters, we need to tune the hyperparameters carefully using a grid search. However, the method is time-consuming, particularly when multiple hyperparameters exist. Therefore, recent studies have analyzed parameter-free methods that adjust the hyperparameters on the fly. However, the existing work is limited to investigations of parameter-free methods for the stepsize, and parameter-free methods for other hyperparameters have not been explored. For instance, although the gradient clipping threshold is a crucial hyperparameter in addition to the stepsize for preventing gradient explosion issues, none of the existing studies have investigated parameter-free methods for clipped gradient descent. Therefore, in this study, we investigate the parameter-free methods for clipped gradient descent. Specifically, we propose Inexact Polyak Stepsize, which converges to the optimal solution without any hyperparameters tuning, and its convergence rate is asymptotically independent of $L$ under $L$-smooth and $(L_0, L_1)$-smooth assumptions of the loss function, similar to that of clipped gradient descent with well-tuned hyperparameters. We numerically validated our convergence results using a synthetic function and demonstrated the effectiveness of our proposed methods using LSTM, Nano-GPT, and T5.'}",https://openreview.net{'value': '/pdf/f354672b35ef05ee13ef090fcd5e077407706173.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=S4ZqnMywcM,{'value': 'Spatio-Temporal Interactive Learning for Efficient Image Reconstruction of Spiking Cameras'},Bin Fan; Jiaoyang Yin; Yuchao Dai; Chao Xu; Tiejun Huang; Boxin Shi,~Bin_Fan3; ~Jiaoyang_Yin1; ~Yuchao_Dai1; ~Chao_Xu1; ~Tiejun_Huang1; ~Boxin_Shi3,"{'value': ['Spiking camera', 'Image reconstruction', 'High-speed motion', 'Spatio-temporal interaction', 'Coarse-to-fine']}","{'value': 'The spiking camera is an emerging neuromorphic vision sensor that records high-speed motion scenes by asynchronously firing continuous binary spike streams. Prevailing image reconstruction methods, generating intermediate frames from these spike streams, often rely on complex step-by-step network architectures that overlook the intrinsic collaboration of spatio-temporal complementary information. In this paper, we propose an efficient spatio-temporal interactive reconstruction network to jointly perform inter-frame feature alignment and intra-frame feature filtering in a coarse-to-fine manner. Specifically, it starts by extracting hierarchical features from a concise hybrid spike representation, then refines the motion fields and target frames scale-by-scale, ultimately obtaining a full-resolution output. Meanwhile, we introduce a symmetric interactive attention block and a multi-motion field estimation block to further enhance the interaction capability of the overall network. Experiments on synthetic and real-captured data show that our approach exhibits excellent performance while maintaining low model complexity.'}",https://openreview.net{'value': '/pdf/bb87f17700d0c54b5c58f6ab4c699c3530ce79bd.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=S3HvA808gk,{'value': 'A Closer Look at AUROC and AUPRC under Class Imbalance'},Matthew B.A. McDermott; Haoran Zhang; Lasse Hyldig Hansen; Giovanni Angelotti; Jack Gallifant,~Matthew_B.A._McDermott1; ~Haoran_Zhang4; ~Lasse_Hyldig_Hansen1; ~Giovanni_Angelotti1; ~Jack_Gallifant1,"{'value': ['AUROC', 'AUPRC', 'Area under the receiver operating characteristic', 'Area under the precision recall curve', 'evaluation', 'fairness', 'disparities', 'bias']}","{'value': 'In machine learning (ML), a widespread claim is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for tasks with class imbalance. This paper refutes this notion on two fronts. First, we theoretically characterize the behavior of AUROC and AUPRC in the presence of model mistakes, establishing clearly that AUPRC is not generally superior in cases of class imbalance. We further show that AUPRC can be a harmful metric as it can unduly favor model improvements in subpopulations with more frequent positive labels, heightening algorithmic disparities. Next, we empirically support our theory using experiments on both semi-synthetic and real-world fairness datasets. Prompted by these insights, we conduct a review of over 1.5 million scientific papers to understand the origin of this invalid claim, finding that it is often made without citation, misattributed to papers that do not argue this point, and aggressively over-generalized from source arguments. Our findings represent a dual contribution: a significant technical advancement in understanding the relationship between AUROC and AUPRC and a stark warning about unchecked assumptions in the ML community.'}",https://openreview.net{'value': '/pdf/9547a073dd9134a23afe6e8685bc31fa6bf9bfa4.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=RRRyQMn6dv,{'value': 'CoSW: Conditional Sample Weighting for Smoke Segmentation with Label Noise'},Lujian Yao; Haitao Zhao; Zhongze Wang; Kaijie Zhao; Jingchao Peng,~Lujian_Yao1; ~Haitao_Zhao1; ~Zhongze_Wang1; ~Kaijie_Zhao1; ~Jingchao_Peng1,{'value': ['Smoke Recognition; Smoke Segmentation; Industrial Applications']},"{'value': 'Smoke segmentation is of great importance in precisely identifying the smoke location, enabling timely fire rescue and gas leak detection. However, due to the visual diversity and blurry edges of the non-grid smoke, noisy labels are almost inevitable in large-scale pixel-level smoke datasets. Noisy labels significantly impact the robustness of the model and may lead to serious accidents. Nevertheless, currently, there are no specific methods for addressing noisy labels in smoke segmentation. Smoke differs from regular objects as its transparency varies, causing inconsistent features in the noisy labels. In this paper, we propose a conditional sample weighting (CoSW). CoSW utilizes a multi-prototype framework, where prototypes serve as prior information to apply different weighting criteria to the different feature clusters. A novel regularized within-prototype entropy (RWE) is introduced to achieve CoSW and stable prototype update. The experiments show that our approach achieves SOTA performance on both real-world and synthetic noisy smoke segmentation datasets.'}",https://openreview.net{'value': '/pdf/d4e62b7bcd956132110df1b4dbe4670633d16159.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=RERls4Opnm,{'value': 'Sample-efficient Bayesian Optimisation Using Known Invariances'},Theodore Brown; Alexandru Cioba; Ilija Bogunovic,~Theodore_Brown2; ~Alexandru_Cioba1; ~Ilija_Bogunovic2,"{'value': ['Bayesian optimisation', 'bandit optimisation', 'Gaussian processes', 'kernel methods', 'groups', 'invariance', 'transformations', 'sample efficiency']}","{'value': ""Bayesian optimisation (BO) is a powerful framework for global optimisation of costly functions, using predictions from Gaussian process models (GPs). In this work, we apply BO to functions that exhibit invariance to a known group of transformations.  We show that vanilla and constrained BO algorithms are inefficient when optimising such invariant objectives, and provide a method for incorporating group invariances into the kernel of the GP to produce invariance-aware algorithms that achieve significant improvements in sample efficiency. We derive a bound on the maximum information gain of these invariant kernels, and provide novel upper and lower bounds on the number of observations required for invariance-aware BO algorithms to achieve $\\epsilon$-optimality. We demonstrate our method's improved performance on a range of synthetic invariant and quasi-invariant functions. We also apply our method in the case where only some of the invariance is incorporated into the kernel, and find that these kernels achieve similar gains in sample efficiency at significantly reduced computational cost. Finally, we use invariant BO to design a current drive system for a nuclear fusion reactor, finding a high-performance solution where non-invariant methods failed.""}",https://openreview.net{'value': '/pdf/b730ed3cbc7119427a0cb045c36b15046919bee7.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=RE7wPI4vfT,{'value': 'Your Diffusion Model is Secretly a Noise Classifier and Benefits from Contrastive Training'},Yunshu Wu; Yingtao Luo; Xianghao Kong; Evangelos E. Papalexakis; Greg Ver Steeg,~Yunshu_Wu1; ~Yingtao_Luo1; ~Xianghao_Kong1; ~Evangelos_E._Papalexakis1; ~Greg_Ver_Steeg1,{'value': ['Diffusion']},"{'value': 'Diffusion models learn to denoise data and the trained denoiser is then used to generate new samples from the data distribution. \nIn this paper, we revisit the diffusion sampling process and identify a fundamental cause of sample quality degradation: the denoiser is poorly estimated in regions that are far Outside Of the training Distribution (OOD), and the sampling process inevitably evaluates in these OOD regions.\nThis can become problematic for all sampling methods, especially when we move to parallel sampling which requires us to initialize and update the entire sample trajectory of dynamics in parallel, leading to many OOD evaluations. \nTo address this problem, we introduce a new self-supervised training objective that differentiates the levels of noise added to a sample, leading to improved OOD denoising performance. The approach is based on our observation that diffusion models implicitly define a log-likelihood ratio that distinguishes distributions with different amounts of noise, and this expression depends on denoiser performance outside the standard training distribution.\nWe show by diverse experiments that the proposed contrastive diffusion training is effective for both sequential and parallel settings, and it improves the performance and speed of parallel samplers significantly. Code for our paper can be found at https://github.com/yunshuwu/ContrastiveDiffusionLoss'}",https://openreview.net{'value': '/pdf/e182e4879fd9de8cee8e9d15056a9d23f21d1002.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Qk3IBHyv6z,"{'value': 'Multi-Agent Imitation Learning: Value is Easy, Regret is Hard'}",Jingwu Tang; Gokul Swamy; Fei Fang; Steven Wu,~Jingwu_Tang1; ~Gokul_Swamy1; ~Fei_Fang1; ~Steven_Wu1,"{'value': ['imitation learning', 'multi-agent reinforcement learning']}","{'value': ""We study a multi-agent imitation learning (MAIL) problem where we take the perspective of a learner attempting to *coordinate* a group of agents based on demonstrations of an expert doing so. Most prior work in MAIL essentially reduces the problem to matching the behavior of the expert *within* the support of the demonstrations. While doing so is sufficient to drive the *value gap* between the learner and the expert to zero under the assumption that agents are non-strategic, it does not guarantee robustness to deviations by strategic agents. Intuitively, this is because strategic deviations can depend on a counterfactual quantity: the coordinator's recommendations outside of the state distribution their recommendations induce. In response, we initiate the study of an alternative objective for MAIL in Markov Games we term the *regret gap* that explicitly accounts for potential deviations by agents in the group. We first perform an in-depth exploration of the relationship between the value and regret gaps. First, we show that while the value gap can be efficiently minimized via a direct extension of single-agent IL algorithms, even *value equivalence* can lead to an arbitrarily large regret gap. This implies that achieving regret equivalence is harder than achieving value equivalence in MAIL. We then provide a pair of efficient reductions to no-regret online convex optimization that are capable of minimizing the regret gap *(a)* under a coverage assumption on the expert (MALICE) or *(b)* with access to a queryable expert (BLADES).""}",https://openreview.net{'value': '/pdf/85c274327fc673a249af4459e7b9afd62b81b79d.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=QiCJomIW3l,{'value': 'Toward Dynamic Non-Line-of-Sight Imaging with Mamba Enforced Temporal Consistency'},Yue Li; Yi Sun; Shida Sun; Juntian Ye; Yueyi Zhang; Feihu Xu; Zhiwei Xiong,~Yue_Li11; ~Yi_Sun20; ~Shida_Sun1; ~Juntian_Ye1; ~Yueyi_Zhang2; ~Feihu_Xu1; ~Zhiwei_Xiong1,"{'value': ['dynamic', 'non-line-of-sight imaging', 'spatial-temporal Mamba']}","{'value': 'Dynamic reconstruction in confocal non-line-of-sight imaging encounters great challenges since the dense raster-scanning manner limits the practical frame rate. A fewer pioneer works reconstruct high-resolution volumes from the under-scanning transient measurements but overlook temporal consistency among transient frames. To fully exploit multi-frame information, we propose the first spatial-temporal Mamba (ST-Mamba) based method tailored for dynamic reconstruction of transient videos. Our method capitalizes on neighbouring transient frames to aggregate the target 3D hidden volume. Specifically, the interleaved features extracted from the input transient frames are fed to the proposed ST-Mamba blocks, which leverage the time-resolving causality in transient measurement. The cross ST-Mamba blocks are then devised to integrate the adjacent transient features. The target high-resolution transient frame is subsequently recovered by the transient spreading module. After transient fusion and recovery, a physical-based network is employed to reconstruct the hidden volume. To tackle the substantial noise inherent in transient videos, we propose a wave-based loss function to impose constraints within the phasor field. Besides, we introduce a new dataset, comprising synthetic videos for training and real-world videos for evaluation. Extensive experiments showcase the superior performance of our method on both synthetic data and real world data captured by different imaging setups. The code and data are available at https://github.com/Depth2World/Dynamic_NLOS.'}",https://openreview.net{'value': '/pdf/8ceb8c4575a29d6db10203bc67a5f763ebcf0ef7.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=QgaGs7peYe,{'value': 'Predicting Future Actions of Reinforcement Learning Agents'},Stephen Chung; Scott Niekum; David Krueger,~Stephen_Chung1; ~Scott_Niekum1; ~David_Krueger1,"{'value': ['Safe reinforcement learning', 'deep reinforcement learning', 'agent predictability']}","{'value': 'As reinforcement learning agents become increasingly deployed in real-world scenarios, predicting future agent actions and events during deployment is important for facilitating better human-agent interaction and preventing catastrophic outcomes. This paper experimentally evaluates and compares the effectiveness of future action and event prediction for three types of RL agents: explicitly planning, implicitly planning, and non-planning. We employ two approaches: the inner state approach, which involves predicting based on the inner computations of the agents (e.g., plans or neuron activations), and a simulation-based approach, which involves unrolling the agent in a learned world model. Our results show that the plans of explicitly planning agents are significantly more informative for prediction than the neuron activations of the other types. Furthermore, using internal plans proves more robust to model quality compared to simulation-based approaches when predicting actions, while the results for event prediction are more mixed. These findings highlight the benefits of leveraging inner states and simulations to predict future agent actions and events, thereby improving interaction and safety in real-world deployments.'}",https://openreview.net{'value': '/pdf/73d9aec42fd261d8a81b78b0781a5e54f0b00ca3.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=QZ2d8E8Whu,{'value': 'LLMDFA: Analyzing Dataflow in Code with Large Language Models'},Chengpeng Wang; Wuqi Zhang; Zian Su; Xiangzhe Xu; Xiaoheng Xie; Xiangyu Zhang,~Chengpeng_Wang2; ~Wuqi_Zhang2; ~Zian_Su1; ~Xiangzhe_Xu1; ~Xiaoheng_Xie2; ~Xiangyu_Zhang3,"{'value': ['LLM for code', 'code reasoning', 'dataflow analysis']}","{'value': 'Dataflow analysis is a fundamental code analysis technique that identifies dependencies between program values. Traditional approaches typically necessitate successful compilation and expert customization, hindering their applicability and usability for analyzing uncompilable programs with evolving analysis needs in real-world scenarios. This paper presents LLMDFA, an LLM-powered compilation-free and customizable dataflow analysis framework. To address hallucinations for reliable results, we decompose the problem into several subtasks and introduce a series of novel strategies. Specifically, we leverage LLMs to synthesize code that outsources delicate reasoning to external expert tools, such as using a parsing library to extract program values of interest and invoking an automated theorem prover to validate path feasibility. Additionally, we adopt a few-shot chain-of-thought prompting to summarize dataflow facts in individual functions, aligning the LLMs with the program semantics of small code snippets to mitigate hallucinations. We evaluate LLMDFA on synthetic programs to detect three representative types of bugs and on real-world Android applications for customized bug detection. On average, LLMDFA achieves 87.10% precision and 80.77% recall, surpassing existing techniques with F1 score improvements of up to 0.35. We have open-sourced LLMDFA at https://github.com/chengpeng-wang/LLMDFA.'}",https://openreview.net{'value': '/pdf/2cfd2ed22ae0deee46b1fb6367fa370ba0fca4e6.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=QWsLks8LCO,{'value': 'Grounded Answers for Multi-agent Decision-making Problem through Generative World Model'},Zeyang Liu; Xinrui Yang; Shiguang Sun; Long Qian; Lipeng Wan; Xingyu Chen; Xuguang Lan,~Zeyang_Liu2; ~Xinrui_Yang1; ~Shiguang_Sun1; ~Long_Qian3; ~Lipeng_Wan1; ~Xingyu_Chen2; ~Xuguang_Lan2,"{'value': ['World Model', 'Multi-agent Reinforcement Learning', 'Model-based Reinforcement Learning']}","{'value': 'Recent progress in generative models has stimulated significant innovations in many fields, such as image generation and chatbots. Despite their success, these models often produce sketchy and misleading solutions for complex multi-agent decision-making problems because they miss the trial-and-error experience and reasoning as humans. To address this limitation, we explore a paradigm that integrates a language-guided simulator into the multi-agent reinforcement learning pipeline to enhance the generated answer. The simulator is a world model that separately learns dynamics and reward, where the dynamics model comprises an image tokenizer as well as a causal transformer to generate interaction transitions autoregressively, and the reward model is a bidirectional transformer learned by maximizing the likelihood of trajectories in the expert demonstrations under language guidance. Given an image of the current state and the task description, we use the world model to train the joint policy and produce the image sequence as the answer by running the converged policy on the dynamics model. The empirical results demonstrate that this framework can improve the answers for multi-agent decision-making problems by showing superior performance on the training and unseen tasks of the StarCraft Multi-Agent Challenge benchmark. In particular, it can generate consistent interaction sequences and explainable reward functions at interaction states, opening the path for training generative models of the future.'}",https://openreview.net{'value': '/pdf/f6adc813ef450f3b17fdc3c4422ae637c6df737f.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=QUYLbzwtTV,{'value': 'Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD Training'},Anchit Jain; Rozhin Nobahari; Aristide Baratin; Stefano Sarao Mannelli,~Anchit_Jain1; ~Rozhin_Nobahari1; ~Aristide_Baratin1; ~Stefano_Sarao_Mannelli1,"{'value': ['learning dynamics', 'online learning', 'stochastic gradient descent', 'analytical model', 'fairness', 'spurious correlation']}","{'value': 'Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations of the data. However, our current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics. To address this gap, this paper explores the evolution of bias in a teacher-student setup that models different data sub-populations with a Gaussian-mixture model. We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setup, which we prove to be exact in high dimension.\nNotably, our analysis identifies different properties of the sub-populations that drive bias at different timescales and hence shows a shifting preference of our classifier during training. By applying our general solution to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias. We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real data, i.e. using CIFAR10, MNIST, and CelebA datasets.'}",https://openreview.net{'value': '/pdf/2ed87c31602334ea03f38d06d97018af95930b88.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=QMaLS4VeY3,{'value': 'Aligning Audio-Visual Joint Representations with an Agentic Workflow'},Shentong Mo; Yibing Song,~Shentong_Mo1; ~Yibing_Song1,"{'value': ['Audio-visual Learning', 'LLM Agent']}","{'value': 'Visual content and accompanied audio signals naturally formulate a joint representation to improve audio-visual (AV) related applications. While studies develop various AV representation learning frameworks, the importance of AV data alignment is usually undermined for achieving high-quality representation. We observe that an audio signal may contain background noise interference. Also, non-synchronization may appear between audio and video streams. These non-strict data alignment limits representation quality and downgrade application performance. In this paper, we propose to improve AV joint representations from a data-centric perspective by aligning audio signals to visual data. Our alignment is conducted in an agentic workflow controlled by an LLM-based assistant named AVAgent. For each input AV data pair, our AVAgent uses a multi-modal LLM to convert audio and visual data into language descriptions separately (i.e., tool use). Then, AVAgent reasons whether this paired data is aligned well and plans to edit the audio signal if needed (i.e., planning). The audio editing is executed by predefined actions that filter noise or augment data. Moreover, we use a VLM to evaluate how modified audio signals match the visual content and provide feedback to AVAgent (i.e., reflection). The tool use, planning, and reflection steps operate cyclically to become an agentic workflow where audio signals are gradually aligned to visual content. To this end, existing methods can directly leverage the aligned AV data via our agentic workflow to improve AV joint representations. The experimental results comprehensively demonstrate the state-of-the-art performance of the proposed approach against previous baselines in diverse downstream tasks.'}",https://openreview.net{'value': '/pdf/56396006acd9ab133fbb46575dda3fbf9cc38b24.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=QGJSXMhVaL,"{'value': 'WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment'}",Hao Tang; Darren Yan Key; Kevin Ellis,~Hao_Tang5; ~Darren_Yan_Key1; ~Kevin_Ellis1,"{'value': ['program synthesis', 'LLM', 'reinforcement learning']}","{'value': 'We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We define this optimism as a logical constraint between a program and a planner. We study our agent on gridworlds, and on task planning, finding our approach is more sample-efficient compared to deep RL, more compute-efficient compared to ReAct-style agents, and that it can transfer its knowledge across environments by editing its code.'}",https://openreview.net{'value': '/pdf/29aa008f69c07f80f1be31b7e9909f05dcad7e2a.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Pwl9n4zlf5,{'value': 'AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning'},Minghao Chen; Yihang Li; Yanting Yang; Shiyu Yu; Binbin Lin; Xiaofei He,~Minghao_Chen2; ~Yihang_Li5; ~Yanting_Yang2; ~Shiyu_Yu1; ~Binbin_Lin3; ~Xiaofei_He2,"{'value': ['Large Language Models', 'AI Agents', 'planning', 'decision making', 'programming']}","{'value': 'Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce a *case-conditioned prompting* strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at https://github.com/minghchen/automanual.'}",https://openreview.net{'value': '/pdf/fa7bc0e7a61d8579a98e5eadb278e05289e2611c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=PvoxbjcRPT,{'value': 'MADiff: Offline Multi-agent Learning with Diffusion Models'},Zhengbang Zhu; Minghuan Liu; Liyuan Mao; Bingyi Kang; Minkai Xu; Yong Yu; Stefano Ermon; Weinan Zhang,~Zhengbang_Zhu1; ~Minghuan_Liu1; ~Liyuan_Mao2; ~Bingyi_Kang1; ~Minkai_Xu1; ~Yong_Yu1; ~Stefano_Ermon1; ~Weinan_Zhang1,"{'value': ['Multi-agent RL', 'Diffusion Models', 'Offline RL']}","{'value': 'Offline reinforcement learning (RL) aims to learn policies from pre-existing datasets without further interactions, making it a challenging task. Q-learning algorithms struggle with extrapolation errors in offline settings, while supervised learning methods are constrained by model expressiveness. Recently, diffusion models (DMs) have shown promise in overcoming these limitations in single-agent learning, but their application in multi-agent scenarios remains unclear. Generating trajectories for each agent with independent DMs may impede coordination, while concatenating all agents’ information can lead to low sample efficiency. Accordingly, we propose MADiff, which is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple agents. To our knowledge, MADiff is the first diffusion-based multi-agent learning framework, functioning as both a decentralized policy and a centralized controller. During decentralized executions, MADiff simultaneously performs teammate modeling, and the centralized controller can also be applied in multi-agent trajectory predictions. Our experiments demonstrate that MADiff outperforms baseline algorithms across various multi-agent learning tasks, highlighting its effectiveness in modeling complex multi-agent interactions.'}",https://openreview.net{'value': '/pdf/37477cb53f290215c287505636803a14d8c9a184.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Pojt9RWIjJ,{'value': 'From Transparent to Opaque: Rethinking Neural Implicit Surfaces with $\\alpha$-NeuS'},Haoran Zhang; Junkai Deng; Xuhui Chen; Fei Hou; Wencheng Wang; Hong Qin; Chen Qian; Ying He,~Haoran_Zhang20; ~Junkai_Deng1; ~Xuhui_Chen2; ~Fei_Hou1; ~Wencheng_Wang1; ~Hong_Qin1; ~Chen_Qian1; ~Ying_He1,"{'value': ['NeuS', 'Transparent Modeling']}","{'value': 'Traditional 3D shape reconstruction techniques from multi-view images, such as structure from motion and multi-view stereo, face challenges in reconstructing transparent objects. Recent advances in neural radiance fields and its variants primarily address opaque or transparent objects, encountering difficulties to reconstruct both transparent and opaque objects simultaneously. This paper introduces $\\alpha$-NeuS$\\textemdash$an extension of NeuS$\\textemdash$that proves NeuS is unbiased for materials from fully transparent to fully opaque. We find that transparent and opaque surfaces align with the non-negative local minima and the zero iso-surface, respectively, in the learned distance field of NeuS. Traditional iso-surfacing extraction algorithms, such as marching cubes, which rely on fixed iso-values, are ill-suited for such data. We develop a method to extract the transparent and opaque surface simultaneously based on DCUDF. To validate our approach, we construct a benchmark that includes both real-world and synthetic scenes, demonstrating its practical utility and effectiveness. Our data and code are publicly available at https://github.com/728388808/alpha-NeuS.'}",https://openreview.net{'value': '/pdf/1a0dea8bfbfc1f0f6aba9f8bffb5186239200ab3.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=PXGY9Fz8vC,{'value': 'Who’s Gaming the System? A Causally-Motivated Approach for Detecting Strategic Adaptation'},Trenton Chang; Lindsay Warrenburg; Sae-Hwan Park; Ravi B Parikh; Maggie Makar; Jenna Wiens,~Trenton_Chang1; ~Lindsay_Warrenburg1; ~Sae-Hwan_Park1; ~Ravi_B_Parikh1; ~Maggie_Makar1; ~Jenna_Wiens1,"{'value': ['causal inference', 'strategic classification', 'gaming', 'healthcare']}","{'value': 'In many settings, machine learning models may be used to inform decisions that impact individuals or entities who interact with the model. Such entities, or *agents,* may *game* model decisions by manipulating their inputs to the model to obtain better outcomes and maximize some utility. We consider a multi-agent setting where the goal is to identify the “worst offenders:” agents that are gaming most aggressively. However, identifying such agents is difficult without knowledge of their utility function. Thus, we introduce a framework in which each agent’s tendency to game is parameterized via a scalar. We show that this gaming parameter is only partially identifiable. By recasting the problem as a causal effect estimation problem where different agents represent different “treatments,” we prove that a ranking of all agents by their gaming parameters is identifiable. We present empirical results in a synthetic data study validating the usage of causal effect estimation for gaming detection and show in a case study of diagnosis coding behavior in the U.S. that our approach highlights features associated with gaming.'}",https://openreview.net{'value': '/pdf/7f31354db4587aad2bba879b475c0a1ac5a5c57e.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=PSPtj26Lbp,{'value': 'L4GM: Large 4D Gaussian Reconstruction Model'},Jiawei Ren; Kevin Xie; Ashkan Mirzaei; hanxue liang; Xiaohui Zeng; Karsten Kreis; Ziwei Liu; Antonio Torralba; Sanja Fidler; Seung Wook Kim; Huan Ling,~Jiawei_Ren1; ~Kevin_Xie1; ~Ashkan_Mirzaei1; ~hanxue_liang1; ~Xiaohui_Zeng2; ~Karsten_Kreis1; ~Ziwei_Liu1; ~Antonio_Torralba1; ~Sanja_Fidler1; ~Seung_Wook_Kim1; ~Huan_Ling1,{'value': ['4D Reconstruction; 4D Generation']},"{'value': 'We present L4GM, the first 4D Large Reconstruction Model that produces animated objects from a single-view video input -- in a single feed-forward pass that takes only a second.\nKey to our success is a novel dataset of multiview videos containing curated, rendered animated objects from Objaverse. This dataset depicts 44K diverse objects with 110K animations rendered in 48 viewpoints, resulting in 12M videos with a total of 300M frames. \nWe keep our L4GM simple for scalability and build directly on top of LGM, a pretrained 3D Large Reconstruction Model that outputs 3D Gaussian ellipsoids from multiview image input.\nL4GM outputs a per-frame 3D Gaussian splat representation from video frames sampled at a low fps and then upsamples the representation to a higher fps to achieve temporal smoothness. We add temporal self-attention layers to the base LGM to help it learn consistency across time, and utilize a per-timestep multiview rendering loss to train the model. The representation is upsampled to a higher framerate by training an interpolation model which produces intermediate 3D Gaussian representations. \nWe showcase that L4GM that is only trained on synthetic data generalizes well on in-the-wild videos, producing high quality animated 3D assets.'}",https://openreview.net{'value': '/pdf/3cf5c2c6b69733b025643bdcb785b92bd8e930b8.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=OtvNLTWYww,{'value': 'A Theoretical Understanding of Self-Correction through In-context Alignment'},Yifei Wang; Yuyang Wu; Zeming Wei; Stefanie Jegelka; Yisen Wang,~Yifei_Wang1; ~Yuyang_Wu1; ~Zeming_Wei1; ~Stefanie_Jegelka3; ~Yisen_Wang1,"{'value': ['Self-correction', 'Theory', 'In-context Learning', 'Transformer', 'Language Model', 'Alignment']}","{'value': 'Going beyond mimicking limited human experiences, recent studies show initial evidence that, like humans, large language models (LLMs) are capable of improving their abilities purely by self-correction, i.e., correcting previous responses through self-examination, as seen in models like OpenAI o1. Nevertheless, little is known about how such capabilities arise. In this work, based on a simplified setup akin to an alignment task, we theoretically analyze self-correction from an in-context learning perspective, showing that when LLMs give relatively accurate self-examinations as rewards, they are capable of refining responses in an in-context way. Notably, going beyond previous theories on over-simplified linear transformers, our theoretical construction underpins the roles of several key designs of realistic transformers for self-correction: softmax attention, multi-head attention, and the MLP block. We validate these findings extensively on synthetic datasets. Inspired by these findings, we propose a simple self-correction strategy, Checking as Context (CaC), which finds novel applications in alleviating social bias and defending against LLM jailbreaks. We believe that these findings will inspire further research on understanding, exploiting, and enhancing self-correction for building better foundation models. Code is at https://github.com/yifeiwang77/Self-Correction.'}",https://openreview.net{'value': '/pdf/61c3046eb708df4bdeec88d9611a7f8586a706cb.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=OoOCoZFVK3,{'value': 'Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning'},Hao Ma; Tianyi Hu; Zhiqiang Pu; Boyin Liu; Xiaolin Ai; Yanyan Liang; Min Chen,~Hao_Ma5; ~Tianyi_Hu1; ~Zhiqiang_Pu1; ~Boyin_Liu2; ~Xiaolin_Ai1; ~Yanyan_Liang1; ~Min_Chen7,"{'value': ['large language model', 'reinforcement learning with human feedback', 'multi-agent reinforcement learning']}","{'value': ""Reinforcement learning (RL) has emerged as a pivotal technique for fine-tuning large language models (LLMs) on specific tasks. However, prevailing RL fine-tuning methods predominantly rely on PPO and its variants. Though these algorithms are effective in general RL settings, they often exhibit suboptimal performance and vulnerability to distribution collapse when applied to the fine-tuning of LLMs. In this paper, we propose CORY, extending the RL fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement learning framework, to leverage the inherent coevolution and emergent capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is initially duplicated into two autonomous agents: a pioneer and an observer. The pioneer generates responses based on queries, while the observer generates responses using both the queries and the pioneer’s responses. The two agents are trained together. During training, the agents exchange roles periodically, fostering cooperation and coevolution between them. Experiments evaluate CORY's performance by fine-tuning GPT-2 and Llama-2 under subjective and objective reward functions on the IMDB Review and GSM8K datasets, respectively. Results show that CORY outperforms PPO in terms of policy optimality, resistance to distribution collapse, and training robustness, thereby underscoring its potential as a superior methodology for refining LLMs in real-world applications.""}",https://openreview.net{'value': '/pdf/577da7c3c3977be3b7b2899a2ac08885392ec6b4.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=OYmms5Mv9H,{'value': 'Geometric Trajectory Diffusion Models'},Jiaqi Han; Minkai Xu; Aaron Lou; Haotian Ye; Stefano Ermon,~Jiaqi_Han2; ~Minkai_Xu1; ~Aaron_Lou1; ~Haotian_Ye1; ~Stefano_Ermon1,"{'value': ['diffusion model', 'geometric trajectory', 'equivariant graph neural networks']}","{'value': 'Generative models have shown great promise in generating 3D geometric systems, which is a fundamental problem in many natural science domains such as molecule and protein design. However, existing approaches only operate on static structures, neglecting the fact that physical systems are always dynamic in nature. In this work, we propose geometric trajectory diffusion models (GeoTDM), the first diffusion model for modeling the temporal distribution of 3D geometric trajectories. Modeling such distribution is challenging as it requires capturing both the complex spatial interactions with physical symmetries and temporal correspondence encapsulated in the dynamics. We theoretically justify that diffusion models with equivariant temporal kernels can lead to density with desired symmetry, and  develop a novel transition kernel leveraging SE(3)-equivariant spatial convolution and temporal attention. Furthermore, to induce an expressive trajectory distribution for conditional generation, we introduce a generalized learnable geometric prior into the forward diffusion process to enhance temporal conditioning. We conduct extensive experiments on both unconditional and conditional generation in various scenarios, including physical simulation, molecular dynamics, and pedestrian motion. Empirical results on a wide suite of metrics demonstrate that GeoTDM can generate realistic geometric trajectories with significantly higher quality.'}",https://openreview.net{'value': '/pdf/aeef0c906f7c5b4a7f45bc03873108514d0fd60b.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=OQUg2T4qJB,{'value': 'Ordering-Based Causal Discovery for Linear and Nonlinear Relations'},Zhuopeng Xu; Yujie Li; Cheng Liu; Ning Gui,~Zhuopeng_Xu1; ~Yujie_Li8; ~Cheng_Liu10; ~Ning_Gui1,{'value': ['causal discovery; directed acyclic graph; additive noise model']},"{'value': 'Identifying causal relations from purely observational data typically requires additional assumptions on relations and/or noise. Most current methods restrict their analysis to datasets that are assumed to have pure linear or nonlinear relations, which is often not reflective of real-world datasets that contain a combination of both. This paper presents CaPS, an ordering-based causal discovery algorithm that effectively handles linear and nonlinear relations. CaPS introduces a novel identification criterion for topological ordering and incorporates the concept of ""parent score"" during the post-processing optimization stage. These scores quantify the strength of the average causal effect, helping to accelerate the pruning process and correct inaccurate predictions in the pruning step. Experimental results demonstrate that our proposed solutions outperform state-of-the-art baselines on synthetic data with varying ratios of linear and nonlinear relations. The results obtained from real-world data also support the competitiveness of CaPS. Code and datasets are available at https://github.com/E2real/CaPS.'}",https://openreview.net{'value': '/pdf/b1aebbce8b2e5fd3e0c0adc2bacb860b24dc9ac0.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=OP3sNTIE1O,{'value': 'Data Augmentation with Diffusion for Open-Set Semi-Supervised Learning'},Seonghyun Ban; Heesan Kong; Kee-Eung Kim,~Seonghyun_Ban1; ~Heesan_Kong1; ~Kee-Eung_Kim2,"{'value': ['Semi-supervised learning', 'Class distribution mismatch', 'Diffusion model.']}","{'value': 'Semi-supervised learning (SSL) seeks to utilize unlabeled data to overcome the limited amount of labeled data and improve model performance. However, many SSL methods typically struggle in real-world scenarios, particularly when there is a large number of irrelevant instances in the unlabeled data that do not belong to any class in the labeled data. Previous approaches often downweight instances from irrelevant classes to mitigate the negative impact of class distribution mismatch on model training. However, by discarding irrelevant instances, they may result in the loss of valuable information such as invariance, regularity, and diversity within the data. In this paper, we propose a data-centric generative augmentation approach that leverages a diffusion model to enrich labeled data using both labeled and unlabeled samples. A key challenge is extracting the diversity inherent in the unlabeled data while mitigating the generation of samples irrelevant to the labeled data. To tackle this issue, we combine diffusion model training with a discriminator that identifies and reduces the impact of irrelevant instances. We also demonstrate that such a trained diffusion model can even convert an irrelevant instance into a relevant one, yielding highly effective synthetic data for training. Through a comprehensive suite of experiments, we show that our data augmentation approach significantly enhances the performance of SSL methods, especially in the presence of class distribution mismatch.'}",https://openreview.net{'value': '/pdf/9ccac95da6d454260441b74551f011a2b4e7224c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=O5XbOoi0x3,{'value': 'Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis'},Yuxi Ren; Xin Xia; Yanzuo Lu; Jiacheng Zhang; Jie Wu; Pan Xie; XING WANG; Xuefeng Xiao,~Yuxi_Ren1; ~Xin_Xia1; ~Yanzuo_Lu1; ~Jiacheng_Zhang4; ~Jie_Wu8; ~Pan_Xie1; ~XING_WANG3; ~Xuefeng_Xiao1,"{'value': ['Diffusion Model', 'Consistency Distillation', 'Human-Feedback Learning']}","{'value': 'Recently, a series of diffusion-aware distillation algorithms have emerged to alleviate the computational overhead associated with the multi-step inference process of Diffusion Models (DMs). Current distillation techniques often dichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii) ODE Trajectory Reformulation. However, these approaches suffer from severe performance degradation or domain shifts. To address these limitations, we propose Hyper-SD, a novel framework that synergistically amalgamates the advantages of ODE Trajectory Preservation and Reformulation, while maintaining near-lossless performance during step compression. Firstly, we introduce Trajectory Segmented Consistency Distillation to progressively perform consistent distillation within pre-defined time-step segments, which facilitates the preservation of the original ODE trajectory from a higher-order perspective. Secondly, we incorporate human feedback learning to boost the performance of the model in a low-step regime and mitigate the performance loss incurred by the distillation process. Thirdly, we integrate score distillation to further improve the low-step generation capability of the model and offer the first attempt to leverage a unified LoRA to support the inference process at all steps. Extensive experiments and user studies demonstrate that Hyper-SD achieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5. For example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and +0.51 in Aes Score in the 1-step inference.'}",https://openreview.net{'value': '/pdf/19490734899d96e4bd026dfa37e4f50c32430a03.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=O2UwxfhY1P,{'value': 'On the Comparison between Multi-modal and Single-modal Contrastive Learning'},Wei Huang; Andi Han; Yongqiang Chen; Yuan Cao; zhiqiang xu; Taiji Suzuki,~Wei_Huang6; ~Andi_Han1; ~Yongqiang_Chen1; ~Yuan_Cao1; ~zhiqiang_xu1; ~Taiji_Suzuki1,"{'value': ['multi-modal contrastive learning', 'single-modal contrastive learning', 'optimization', 'learning theory']}","{'value': 'Multi-modal contrastive learning with language supervision has presented a paradigm shift in modern machine learning. By pre-training on a web-scale dataset, multi-modal contrastive learning can learn high-quality representations that exhibit impressive robustness and transferability. Despite its empirical success, the theoretical understanding is still in its infancy, especially regarding its comparison with single-modal contrastive learning. In this work, we introduce a feature learning theory framework that provides a theoretical foundation for understanding the differences between multi-modal and single-modal contrastive learning. Based on a data generation model consisting of signal and noise, our analysis is performed on a ReLU network trained with the InfoMax objective function. Through a trajectory-based optimization analysis and generalization characterization on downstream tasks, we identify the critical factor, which is the signal-to-noise ratio (SNR), that impacts the generalizability in downstream tasks of both multi-modal and single-modal contrastive learning. Through the cooperation between the two modalities, multi-modal learning can achieve better feature learning, leading to improvements in performance in downstream tasks compared to single-modal learning. Our analysis provides a unified framework that can characterize the optimization and generalization of both single-modal and multi-modal contrastive learning. Empirical experiments on both synthetic and real-world datasets further consolidate our theoretical findings.'}",https://openreview.net{'value': '/pdf/c9933b975dba8940756388b4decd66b29013d72e.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=O0nBMRlkc8,{'value': 'Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration'},Junyang Wang; Haiyang Xu; Haitao Jia; Xi Zhang; Ming Yan; Weizhou Shen; Ji Zhang; Fei Huang; Jitao Sang,~Junyang_Wang1; ~Haiyang_Xu1; ~Haitao_Jia1; ~Xi_Zhang11; ~Ming_Yan2; ~Weizhou_Shen1; ~Ji_Zhang3; ~Fei_Huang2; ~Jitao_Sang1,"{'value': ['multi-agent', 'multi-modal agent', 'multi-modal large language model', 'mobile operation', 'UI assistant']}","{'value': 'Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks — task progress navigation and focus content navigation — are difficult to effectively solve under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent condenses lengthy, interleaved image-text history operations and screens summaries into a pure-text task progress, which is then passed on to the decision agent. This reduction in context length makes it easier for decision agent to navigate the task progress. To retain focus content, we design a memory unit that updates with task progress by decision agent. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistake accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at https://github.com/X-PLUG/MobileAgent.'}",https://openreview.net{'value': '/pdf/1884d55b0eac95c14035e897dcbb1c8186bcd65e.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Nycj81Z692,{'value': 'UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction'},Yansong Ning; Hao Liu,~Yansong_Ning3; ~Hao_Liu17,"{'value': ['urban knowledge graph', 'knowledge graph construction', 'large language model agent']}","{'value': 'Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we obtain UrbanKGC agent family, consisting of UrbanKGent-7/8/13B version. We perform a comprehensive evaluation on two real-world datasets using both human and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent family can not only significantly outperform 31 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost. Compared with the existing benchmark, the UrbanKGent family could help construct an UrbanKG with hundreds of times richer relationships using only one-fifth of the data. Our data and code are available at https://github.com/usail-hkust/UrbanKGent.'}",https://openreview.net{'value': '/pdf/356b3e5ec7238657412c0d7e0e63e14721936737.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=NsqxN9iOJ7,{'value': 'Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation'},Yuanhao Zhai; Kevin Lin; Zhengyuan Yang; Linjie Li; Jianfeng Wang; Chung-Ching Lin; David Doermann; Junsong Yuan; Lijuan Wang,~Yuanhao_Zhai1; ~Kevin_Lin3; ~Zhengyuan_Yang1; ~Linjie_Li1; ~Jianfeng_Wang4; ~Chung-Ching_Lin2; ~David_Doermann2; ~Junsong_Yuan2; ~Lijuan_Wang1,"{'value': ['consistency distillation', 'video diffusion models', 'diffusion distillation', 'text-to-video generation']}","{'value': 'Image diffusion distillation achieves high-fidelity generation with very few sampling steps. However, directly applying these techniques to video models results in unsatisfied frame quality. This issue arises from the limited frame appearance quality in public video datasets, affecting the performance of both teacher and student video diffusion models. Our study aims to improve video diffusion distillation and meanwhile enabling the student model to improve frame appearance using the abundant high-quality image data. To this end, we propose motion consistency models (MCM), a single-stage video diffusion distillation method that disentangles motion and appearance learning. Specifically, MCM involves a video consistency model that distills motion from the video teacher model, and an image discriminator that boosts frame appearance to match high-quality image data. However, directly combining these components leads to two significant challenges: a conflict in frame learning objectives, where video distillation learns from low-quality video frames while the image discriminator targets high-quality images, and training-inference discrepancies due to the differing quality of video samples used during training and inference. To address these challenges, we introduce disentangled motion distillation and mixed trajectory distillation. The former applies the distillation objective solely to the motion representation, while the latter mitigates training-inference discrepancies by mixing distillation trajectories from both the low- and high-quality video domains. Extensive experiments show that our MCM achieves state-of-the-art video diffusion distillation performance. Additionally, our method can enhance frame quality in video diffusion models, producing frames with high aesthetic value or specific styles.'}",https://openreview.net{'value': '/pdf/38b3b43429f503f270b45ad9f0531f83298246b6.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=NmlnmLYMZ4,{'value': 'When does perceptual alignment benefit vision representations?'},Shobhita Sundaram; Stephanie Fu; Lukas Muttenthaler; Netanel Yakir Tamir; Lucy Chai; Simon Kornblith; Trevor Darrell; Phillip Isola,~Shobhita_Sundaram1; ~Stephanie_Fu1; ~Lukas_Muttenthaler1; ~Netanel_Yakir_Tamir1; ~Lucy_Chai1; ~Simon_Kornblith1; ~Trevor_Darrell2; ~Phillip_Isola1,"{'value': ['representation learning', 'alignment', 'perception', 'transfer learning', 'computer vision', 'foundation model']}","{'value': 'Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception. \nWhile vision representations have previously benefited from human preference alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability in standard computer vision tasks. We finetune state-of-the-art models on a dataset of human similarity judgments for synthetic image triplets and evaluate them across diverse computer vision tasks. We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, semantic segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can make them better representation learners.'}",https://openreview.net{'value': '/pdf/6eb7024452b45f4beef86c4a4c1a3e0f1e2a3494.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=NhyDfZXjQX,{'value': 'A Local Method for Satisfying Interventional Fairness with Partially Known Causal Graphs'},Haoxuan Li; Yue Liu; Zhi Geng; Kun Zhang,~Haoxuan_Li6; ~Yue_Liu9; ~Zhi_Geng1; ~Kun_Zhang1,"{'value': ['Interventional fairness', 'PDAG', 'MPDAG', 'Causal effect']}","{'value': 'Developing fair automated machine learning algorithms is critical in making safe and trustworthy decisions. Many causality-based fairness notions have been proposed to address the above issues by quantifying the causal connections between sensitive attributes and decisions, and when the true causal graph is fully known, certain algorithms that achieve interventional fairness have been proposed. However, when the true causal graph is unknown, it is still challenging to effectively and efficiently exploit partially directed acyclic graphs (PDAGs) to achieve interventional fairness. To exploit the PDAGs for achieving interventional fairness, previous methods have been built on variable selection or causal effect identification, but limited to reduced prediction accuracy or strong assumptions. In this paper, we propose a general min-max optimization framework that can achieve interventional fairness with promising prediction accuracy and can be extended to maximally oriented PDAGs (MPDAGs) with added background knowledge. Specifically, we first estimate all possible treatment effects of sensitive attributes on a given prediction model from all possible adjustment sets of sensitive attributes via an efficient local approach. Next, we propose to alternatively update the prediction model and possible estimated causal effects, where the prediction model is trained via a min-max loss to control the worst-case fairness violations. Extensive experiments on synthetic and real-world datasets verify the superiority of our methods. To benefit the research community, we have released our project at https://github.com/haoxuanli-pku/NeurIPS24-Interventional-Fairness-with-PDAGs.'}",https://openreview.net{'value': '/pdf/da6371fee0dbeae66de254e81762d318dc57b1ab.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Nf4MHF1pi5,{'value': 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents'},Wenkai Yang; Xiaohan Bi; Yankai Lin; Sishuo Chen; Jie Zhou; Xu Sun,~Wenkai_Yang1; ~Xiaohan_Bi1; ~Yankai_Lin1; ~Sishuo_Chen1; ~Jie_Zhou8; ~Xu_Sun1,"{'value': ['LLM-based Agents', 'Backdoor Attack']}","{'value': 'Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.'}",https://openreview.net{'value': '/pdf/14dda2e2e067d5c1fc5179293fd0d4072276f210.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=NbFOrcwqbR,{'value': 'Taming Generative Diffusion Prior for Universal Blind Image Restoration'},Siwei Tu; Weidong Yang; Ben Fei,~Siwei_Tu1; ~Weidong_Yang2; ~Ben_Fei2,"{'value': ['Blind image restoration', 'Diffusion model', 'Optimizable degradation model', 'Adaptive guidance scale']}","{'value': 'Diffusion models have been widely utilized for image restoration. However, previous blind image restoration methods still need to assume the type of degradation model while leaving the parameters to be optimized, limiting their real-world applications. Therefore, we aim to tame generative diffusion prior for universal blind image restoration dubbed BIR-D, which utilizes an optimizable convolutional kernel to simulate the degradation model and dynamically update the parameters of the kernel in the diffusion steps, enabling it to achieve blind image restoration results even in various complex situations. Besides, based on mathematical reasoning, we have provided an empirical formula for the chosen of adaptive guidance scale, eliminating the need for a grid search for the optimal parameter. Experimentally, Our BIR-D has demonstrated superior practicality and versatility than off-the-shelf unsupervised methods across various tasks both on real-world and synthetic datasets, qualitatively and quantitatively. BIR-D is able to fulfill multi-guidance blind image restoration. Moreover, BIR-D can also restore images that undergo multiple and complicated degradations, demonstrating the practical applications. The code is available at https://github.com/Tusiwei/BIR-D.'}",https://openreview.net{'value': '/pdf/7ad55192dc5f86009a3627d42e58f4316a42c1b7.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=NaCXcUKihH,{'value': 'Towards a theory of how the structure of language is acquired by deep neural networks'},Francesco Cagnetta; Matthieu Wyart,~Francesco_Cagnetta1; ~Matthieu_Wyart2,"{'value': ['Hierarchical Models', 'Language Models', 'Learning Theory', 'Representation Learning', 'Self-Supervised Learning', 'Statistical Physics of Learning']}","{'value': ""How much data is required to learn the structure of a language via next-token prediction? We study this question for synthetic datasets generated via a Probabilistic Context-Free Grammar (PCFG)---a hierarchical generative model that captures the tree-like structure of natural languages. We determine token-token correlations analytically in our model and show that they can be used to build a representation of the grammar's hidden variables, the longer the range the deeper the variable. In addition, a finite training set limits the resolution of correlations to an effective range, whose size grows with that of the training set. As a result, a Language Model trained with increasingly many examples can build a deeper representation of the grammar's structure, thus reaching good performance despite the high dimensionality of the problem. We conjecture that the relationship between training set size and effective range of correlations holds beyond our synthetic datasets, and we test it in a collection of lines from Shakespeare's plays. In particular, we show that reducing the input size leads to saturation of the test loss decay at a characteristic training set size that can be predicted in our framework.""}",https://openreview.net{'value': '/pdf/dba208c5d85bc48721957c9401e8430df5036e8c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=NQCkNM6TES,{'value': 'Harmonizing Stochasticity and Determinism: Scene-responsive Diverse Human Motion Prediction'},Zhenyu Lou; Qiongjie Cui; Tuo Wang; Zhenbo Song; Luoming Zhang; Cheng Cheng; Haofan Wang; Xu Tang; Huaxia Li; Hong Zhou,~Zhenyu_Lou1; ~Qiongjie_Cui1; ~Tuo_Wang3; ~Zhenbo_Song1; ~Luoming_Zhang1; ~Cheng_Cheng12; ~Haofan_Wang1; ~Xu_Tang1; ~Huaxia_Li1; ~Hong_Zhou5,"{'value': ['Human Motion Prediction', 'Diverse Motion Prediction', '3D Scene-aware', 'Multimodal']}","{'value': 'Diverse human motion prediction (HMP) is a fundamental application in computer vision that has recently attracted considerable interest. Prior methods primarily focus on the stochastic nature of human motion, while neglecting the specific impact of external environment, leading to the pronounced artifacts in prediction when applied to real-world scenarios. To fill this gap, this work introduces a novel task: predicting diverse human motion within real-world 3D scenes. In contrast to prior works, it requires harmonizing the deterministic constraints imposed by the surrounding 3D scenes with the stochastic aspect of human motion. For this purpose, we propose DiMoP3D, a diverse motion prediction framework with 3D scene awareness, which leverages the 3D point cloud and observed sequence to generate diverse and high-fidelity predictions. DiMoP3D is able to comprehend the 3D scene, and determines the probable target objects and their desired interactive pose based on the historical motion. Then, it plans the obstacle-free trajectory towards these interested objects, and generates diverse and physically-consistent future motions. On top of that, DiMoP3D identifies deterministic factors in the scene and integrates them into the stochastic modeling, making the diverse HMP in realistic scenes become a controllable stochastic generation process. On two real-captured benchmarks, DiMoP3D has demonstrated significant improvements over state-of-the-art methods, showcasing its effectiveness in generating diverse and physically-consistent motion predictions within real-world 3D environments.'}",https://openreview.net{'value': '/pdf/505af28d14be82438700ac8ba80d19a9d2598bb1.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=NLUYZ4ZqNq,{'value': 'SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain'},Pierre Colombo; Telmo Pires; Malik Boudiaf; Rui Melo; Gabriel Hautreux; Etienne Malaboeuf; Johanne Charpentier; Dominic Culver; Michael Desa,~Pierre_Colombo2; ~Telmo_Pires1; ~Malik_Boudiaf1; ~Rui_Melo2; ~Gabriel_Hautreux2; ~Etienne_Malaboeuf1; ~Johanne_Charpentier1; ~Dominic_Culver1; ~Michael_Desa1,{'value': ['NLP; Deep Learning; Law']},"{'value': ""In this paper, we introduce SaulLM-medium and SaulLM-large, two large language models (LLMs) families tailored for the legal sector. These models, which feature architectures of 54 billion and 140 billion parameters, respectively, are based on the Mixtral architecture. The development of SaulLM-54B and SaulLM-140B is guided by large-scale domain adaptation,  divided into strategies: (1) the exploitation of continued pretaining involving a legal corpus that includes over $400$ billion tokens, (2) the implementation of a specialized legal instruction-following protocol, and (3) the alignment of model outputs with human preferences in legal interpretations. The integration of synthetically generated data in the second and third steps enhances the models' capabilities in interpreting and processing legal texts, effectively reaching state-of-the-art performance and outperforming all previous open-source models on LegalBench Instruct. This research thoroughly explores the trade-offs involved in domain-specific adaptation at this scale, offering insights that may inform future studies on domain adaptation using strong decoder models. Building upon SaulLM-7B, this study refines the approach to produce an LLM better equipped for legal tasks and domains. Additionally, we release base, instruct and aligned versions on top of SaulLM-medium and SaulLM-large under the MIT License to facilitate reuse and collaborative research.""}",https://openreview.net{'value': '/pdf/f9a41c70e99ec8b08b09ed0818f4c13856d8f35c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=NJUClFbosX,{'value': 'Discrete Dictionary-based Decomposition Layer for Structured Representation Learning'},Taewon Park; Hyun-Chul Kim; Minho Lee,~Taewon_Park1; ~Hyun-Chul_Kim3; ~Minho_Lee2,"{'value': ['tensor product representation', 'neuro-symbolic neural network', 'systematic generalization', 'compositional generalization', 'decomposition problem', 'structured representation learning', 'discrete representation learning']}","{'value': 'Neuro-symbolic neural networks have been extensively studied to integrate symbolic operations with neural networks, thereby improving systematic generalization. Specifically, Tensor Product Representation (TPR) framework enables neural networks to perform differentiable symbolic operations by encoding the symbolic structure of data within vector spaces. However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations. To address this decomposition problem, we propose a Discrete Dictionary-based Decomposition (D3) layer designed to enhance the decomposition capabilities of TPR-based models. D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations. It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned symbolic features within these dictionaries. D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications. Our experimental results demonstrate that D3 significantly improves the systematic generalization of various TPR-based models while requiring fewer additional parameters. Notably, D3 outperforms baseline models on the synthetic task that demands the systematic decomposition of unseen combinatorial data.'}",https://openreview.net{'value': '/pdf/97de2f0f9a2268ca192d81a154085ed16a9d240b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=N4quRxE19p,{'value': 'AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning'},Shirley Wu; Shiyu Zhao; Qian Huang; Kexin Huang; Michihiro Yasunaga; Kaidi Cao; Vassilis N. Ioannidis; Karthik Subbian; Jure Leskovec; James Zou,~Shirley_Wu1; ~Shiyu_Zhao5; ~Qian_Huang2; ~Kexin_Huang1; ~Michihiro_Yasunaga1; ~Kaidi_Cao1; ~Vassilis_N._Ioannidis1; ~Karthik_Subbian1; ~Jure_Leskovec1; ~James_Zou1,"{'value': ['LLM agents', 'Tool utilization', 'Automatic prompt optimization', 'Complex retrieval', 'Question-Answering tasks']}","{'value': 'Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demon- strate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.'}",https://openreview.net{'value': '/pdf/85cc58f45bb9f518d73575f53151052a9f51d521.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=MzTdZhMjeC,{'value': 'MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-Object Demand-driven Navigation'},Hongcheng Wang; Peiqi Liu; Wenzhe Cai; Mingdong Wu; Zhengyu Qian; Hao Dong,~Hongcheng_Wang6; ~Peiqi_Liu2; ~Wenzhe_Cai1; ~Mingdong_Wu1; ~Zhengyu_Qian1; ~Hao_Dong3,"{'value': ['Mudolar Object Navigation', 'Demand-driven Navigation', 'Attribute Learning']}","{'value': 'The process of satisfying daily demands is a fundamental aspect of humans\' daily lives. With the advancement of embodied AI, robots are increasingly capable of satisfying human demands. Demand-driven navigation (DDN) is a task in which an agent must locate an object to satisfy a specified demand instruction, such as ""I am thirsty."" The previous study typically assumes that each demand instruction requires only one object to be fulfilled and does not consider individual preferences. However, the realistic human demand may involve multiple objects. In this paper, we introduce the Multi-object Demand-driven Navigation (MO-DDN) benchmark, which addresses these nuanced aspects, including multi-object search and personal preferences, thus making the MO-DDN task more reflective of real-life scenarios compared to DDN. Building upon previous work, we employ the concept of ``attribute\'\' to tackle this new task. However, instead of solely relying on attribute features in an end-to-end manner like DDN, we propose a modular method that involves constructing a coarse-to-fine attribute-based exploration agent (C2FAgent). Our experimental results illustrate that this coarse-to-fine exploration strategy capitalizes on the advantages of attributes at various decision-making levels, resulting in superior performance compared to baseline methods. Code and video can be found at https://sites.google.com/view/moddn.'}",https://openreview.net{'value': '/pdf/93905ccec59fb2772c8a3940329393bf93a88986.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=MzNjnbgcPN,{'value': 'OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations'},Yao Shu; Jiongfeng Fang; Ying Tiffany He; Fei Yu,~Yao_Shu1; ~Jiongfeng_Fang1; ~Ying_Tiffany_He1; ~Fei_Yu13,"{'value': ['Iteration Parallelization', 'First-Order Optimization', 'Iteration Complexity', 'Time Efficiency']}","{'value': 'First-order optimization (FOO) algorithms are pivotal in numerous computational domains, such as reinforcement learning and deep learning. However, their application to complex tasks often entails significant optimization inefficiency due to their need of many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first general framework that enhances the time efficiency of FOO by leveraging parallel computing to directly mitigate its requirement of many sequential iterations for convergence. To achieve this, OptEx utilizes a kernelized gradient estimation that is based on the history of evaluated gradients to predict the gradients required by the next few sequential iterations in FOO, which helps to break the inherent iterative dependency and hence enables the approximate parallelization of iterations in FOO. We further establish theoretical guarantees for the estimation error of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming that the estimation error diminishes to zero as the history of gradients accumulates and that our SGD-based OptEx enjoys an effective acceleration rate of Θ(√N ) over standard SGD given parallelism of N, in terms of the sequential iterations required for convergence. Finally, we provide extensive empirical studies, including synthetic functions, reinforcement learning tasks, and neural network training on various datasets, to underscore the substantial efficiency improvements achieved by our OptEx in practice.'}",https://openreview.net{'value': '/pdf/6355a6b19af5c8832921ce57986888808909ddc1.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=MzM99vV5Rx,{'value': 'IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering'},Ruosen Li; Ruochen Li; Barry Wang; Xinya Du,~Ruosen_Li1; ~Ruochen_Li3; ~Barry_Wang1; ~Xinya_Du1,"{'value': ['Interactive Question Answering', 'Automatic Evaluation', 'Question Answering', 'LLM Evaluation']}","{'value': 'To evaluate Large Language Models (LLMs) for question answering (QA), traditional methods typically focus on directly assessing the immediate responses generated by the models based on the given question and context. In the common use case of humans seeking AI assistant’s help in finding information, these non-interactive evaluations do not account for the dynamic nature of human-model conversations, and interaction-aware evaluations have shown that accurate models are not necessarily preferred by humans Lee et al. Recent works in human-computer interaction (HCI) have employed human evaluators to conduct interactions and evaluations, but they are often prohibitively expensive and time-consuming to scale. In this work, we introduce an automated evaluation framework IQA-EVAL to Interactive Question Answering Evaluations, more specifically, we introduce LLM-based Evaluation Agent (LEA) that can: (1) simulate human behaviors to generate interactions with IQA models; (2) automatically evaluate the generated interactions. Moreover, we propose assigning personas to LEAs to better simulate groups of real human evaluators. We show that: (1) our evaluation framework with GPT-4 (or Claude) as the backbone model achieves a high correlation with human evaluations on the IQA task; (2) assigning personas to LEA to better represent the crowd further significantly improves correlations. Finally, we use our automated metric to evaluate five recent LLMs with over 1000 questions from complex and ambiguous question answering tasks, which would cost $5k if evaluated by humans.'}",https://openreview.net{'value': '/pdf/2413a018973b3ca9681c42b02c40b36c57101666.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Mwj57TcHWX,{'value': 'DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning'},Weikang Wan; Ziyu Wang; Yufei Wang; Zackory Erickson; David Held,~Weikang_Wan1; ~Ziyu_Wang15; ~Yufei_Wang4; ~Zackory_Erickson1; ~David_Held1,"{'value': ['imitation learning', 'model-based reinforcement learning', 'differentiable trajectory optimization']}","{'value': 'This paper introduces DiffTORI, which utilizes $\\textbf{Diff}$erentiable $\\textbf{T}$rajectory $\\textbf{O}$ptimization as the policy representation to generate actions for deep $\\textbf{R}$einforcement and $\\textbf{I}$mitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization.  As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTORI addresses the “objective mismatch” issue of prior model-based RL algorithms, as the dynamics model in DiffTORI is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTORI  for imitation learning on standard robotic manipulation task suites with high-dimensional sensory observations and compare our method to feedforward policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15 model based RL tasks and 35 imitation learning tasks with high-dimensional image and point cloud inputs, DiffTORI outperforms prior state-of-the-art methods in both domains.'}",https://openreview.net{'value': '/pdf/797ea8f1f520e879eef0d48f51e53fa63d677421.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=MbZuh8L0Xg,{'value': 'DiffPhyCon: A Generative Approach to Control Complex Physical Systems'},Long Wei; Peiyan Hu; Ruiqi Feng; Haodong Feng; Yixuan Du; Tao Zhang; Rui Wang; Yue Wang; Zhi-Ming Ma; Tailin Wu,~Long_Wei1; ~Peiyan_Hu1; ~Ruiqi_Feng1; ~Haodong_Feng1; ~Yixuan_Du1; ~Tao_Zhang35; ~Rui_Wang56; ~Yue_Wang15; ~Zhi-Ming_Ma1; ~Tailin_Wu1,"{'value': ['Physical systems control', 'physical simulation', 'generative models', 'prior reweighting']}","{'value': ""Controlling the evolution of complex physical systems is a fundamental task across science and engineering. \nClassical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and plan near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method on three tasks: 1D Burgers' equation, 2D jellyfish movement control, and 2D high-dimensional smoke control, where our generated jellyfish dataset is released as a benchmark for complex physical system control research. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics. The project website, jellyfish dataset, and code can be found at https://github.com/AI4Science-WestlakeU/diffphycon.""}",https://openreview.net{'value': '/pdf/31de06d7d0540d105c2d105165d31dd63c4cfc26.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=MWHRxKz4mq,{'value': 'Marrying Causal Representation Learning with Dynamical Systems for Science'},Dingling Yao; Caroline Muller; Francesco Locatello,~Dingling_Yao1; ~Caroline_Muller1; ~Francesco_Locatello1,{'value': ['Causal representation learning']},"{'value': 'Causal representation learning promises to extend causal models to hidden causal variables from raw entangled measurements. However, most progress has focused on proving identifiability results in different settings, and we are not aware of any successful real-world application. At the same time, the field of dynamical systems benefited from deep learning and scaled to countless applications but does not allow parameter identification. In this paper, we draw a clear connection between the two and their key assumptions, allowing us to apply identifiable methods developed in causal representation learning to dynamical systems. At the same time, we can leverage scalable differentiable solvers developed for differential equations to build models that are both identifiable and practical. Overall, we learn explicitly controllable models that isolate the trajectory-specific parameters for further downstream tasks such as out-of-distribution classification or treatment effect estimation. We experiment with a wind simulator with partially known factors of variation. We also apply the resulting model to real-world climate data and successfully answer downstream causal questions in line with existing literature on climate change.'}",https://openreview.net{'value': '/pdf/4d262327bb4f673e796dc8cfec9687d6eb06301b.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=MQIET1VfoV,{'value': 'Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance'},Joshua McClellan; Naveed Haghani; John Winder; Furong Huang; Pratap Tokekar,~Joshua_McClellan1; ~Naveed_Haghani1; ~John_Winder2; ~Furong_Huang1; ~Pratap_Tokekar1,"{'value': ['Equivariant Graph Neural Networks', 'Reinforcement Learning', 'Multi-agent Reinforcement Learning', 'Symmetry', 'generalization', 'sample efficiency', 'MARL']}","{'value': 'Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1]. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error [ 2 ]. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.'}",https://openreview.net{'value': '/pdf/10ad8896ad4558b07189368d1a583d7030833d2c.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=MPJ3oXtTZl,{'value': 'G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering'},Xiaoxin He; Yijun Tian; Yifei Sun; Nitesh V Chawla; Thomas Laurent; Yann LeCun; Xavier Bresson; Bryan Hooi,~Xiaoxin_He1; ~Yijun_Tian1; ~Yifei_Sun1; ~Nitesh_V_Chawla1; ~Thomas_Laurent1; ~Yann_LeCun1; ~Xavier_Bresson6; ~Bryan_Hooi1,"{'value': ['Retrieval Augmented Generation', 'Graph Question Answering', 'Graph Neural Network', 'Large Language Model', 'Textual Graphs']}","{'value': ""Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our \\textit{G-Retriever} method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, \\textit{G-Retriever} performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/G-Retriever}}""}",https://openreview.net{'value': '/pdf/dd5411c5713d592bdad14949720408bf8cf0071f.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=MNg331t8Tj,{'value': 'Advancing Fine-Grained Classification by Structure and Subject Preserving Augmentation'},Eyal Michaeli; Ohad Fried,~Eyal_Michaeli1; ~Ohad_Fried1,"{'value': ['Fine-grained Visual Classification', 'Data Augmentation', 'Synthetic Data', 'Diffusion Models', 'Image Classification']}","{'value': ""Fine-grained visual classification (FGVC) involves classifying closely related subcategories. This task is inherently difficult due to the subtle differences between classes and the high intra-class variance. Moreover, FGVC datasets are typically small and challenging to gather, thus highlighting a significant need for effective data augmentation.\nRecent advancements in text-to-image diffusion models have introduced new possibilities for data augmentation in image classification. While these models have been used to generate training data for classification tasks, their effectiveness in full-dataset training of FGVC models remains under-explored. Recent techniques that rely on text-to-image generation or Img2Img methods, such as SDEdit, often struggle to generate images that accurately represent the class while modifying them to a degree that significantly increases the dataset's diversity. To address these challenges, we present SaSPA: Structure and Subject Preserving Augmentation. Contrary to recent methods, our method does not use real images as guidance, thereby increasing generation flexibility and promoting greater diversity. To ensure accurate class representation, we employ conditioning mechanisms, specifically by conditioning on image edges and subject representation.\nWe conduct extensive experiments and benchmark SaSPA against both traditional and generative data augmentation techniques. SaSPA consistently outperforms all established baselines across multiple settings, including full dataset training and contextual bias. Additionally, our results reveal interesting patterns in using synthetic data for FGVC models; for instance, we find a relationship between the amount of real data used and the optimal proportion of synthetic data.""}",https://openreview.net{'value': '/pdf/a68333e357015c0da34498829380c0df7fac727c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=MDpIQ9hQ7H,{'value': 'From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning'},Pusen Dong; Tianchen Zhu; Yue Qiu; Haoyi Zhou; Jianxin Li,~Pusen_Dong1; ~Tianchen_Zhu1; ~Yue_Qiu7; ~Haoyi_Zhou1; ~Jianxin_Li3,"{'value': ['Safe RL', 'Multimodal Learning', 'Temporal Credit Assignment', 'Conditioned Reinforcement Learning']}","{'value': 'Safe reinforcement learning (RL) requires the agent to finish a given task while obeying specific constraints. Giving constraints in natural language form has great potential for practical scenarios due to its flexible transfer capability and accessibility. Previous safe RL methods with natural language constraints typically need to design cost functions manually for each constraint, which requires domain expertise and lacks flexibility. In this paper, we harness the dual role of text in this task, using it not only to provide constraint but also as a training signal. We introduce the Trajectory-level Textual Constraints Translator (TTCT) to replace the manually designed cost function. Our empirical results demonstrate that TTCT effectively comprehends textual constraint and trajectory, and the policies trained by TTCT can achieve a lower violation rate than the standard cost function. Extra studies are conducted to demonstrate that the TTCT has zero-shot transfer capability to adapt to constraint-shift environments.'}",https://openreview.net{'value': '/pdf/3e9d50d24b1a21619ddb6d7315431df9d2ade121.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Lzl8qJYXv5,{'value': 'Estimating the Hallucination Rate of Generative AI'},Andrew Jesson; Nicolas Beltran-Velez; Quentin Chu; Sweta Karlekar; Jannik Kossen; Yarin Gal; John Patrick Cunningham; David Blei,~Andrew_Jesson1; ~Nicolas_Beltran-Velez1; ~Quentin_Chu1; ~Sweta_Karlekar2; ~Jannik_Kossen2; ~Yarin_Gal1; ~John_Patrick_Cunningham1; ~David_Blei2,"{'value': ['Uncertainty Quantification', 'Large Language Models', 'Conditional Generative Models', 'Hallucination Prediction']}","{'value': 'This paper presents a method for estimating the hallucination rate for in-context learning (ICL) with generative AI. In ICL, a conditional generative model (CGM) is prompted with a dataset and a prediction question and asked to generate a response. One interpretation of ICL assumes that the CGM computes the posterior predictive of an unknown Bayesian model, which implicitly defines a joint distribution over observable datasets and latent mechanisms. This joint distribution factorizes into two components: the model prior over mechanisms and the model likelihood of datasets given a mechanism. With this perspective, we define a \\textit{hallucination} as a generated response to the prediction question with low model likelihood given the mechanism. We develop a new method that takes an ICL problem and estimates the probability that a CGM will generate a hallucination. Our method only requires generating prediction questions and responses from the CGM and evaluating its response log probability. We empirically evaluate our method using large language models for synthetic regression and natural language ICL tasks.'}",https://openreview.net{'value': '/pdf/f36674f8aefd4b68ce1465513240bb51b254521e.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=LuCLf4BJsr,{'value': 'Chain of Agents: Large Language Models Collaborating on Long-Context Tasks'},Yusen Zhang; Ruoxi Sun; Yanfei Chen; Tomas Pfister; Rui Zhang; Sercan O Arik,~Yusen_Zhang1; ~Ruoxi_Sun2; ~Yanfei_Chen1; ~Tomas_Pfister1; ~Rui_Zhang7; ~Sercan_O_Arik1,"{'value': ['Large Language Models', 'Long Context Tasks', 'Multi-agent Collaboration', 'LLM Agents']}","{'value': 'Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output. CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context. We perform a comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs.'}",https://openreview.net{'value': '/pdf/8bdf384cc5f563b5777f68d1215b600d11eea738.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Lc8gemv97Y,{'value': 'Dealing with Synthetic Data Contamination in Online Continual Learning'},Maorong Wang; Nicolas Michel; Jiafeng Mao; Toshihiko Yamasaki,~Maorong_Wang1; ~Nicolas_Michel1; ~Jiafeng_Mao1; ~Toshihiko_Yamasaki1,"{'value': ['Online Continual Learning', 'Image Generation', 'Replay-based method', 'Entropy Selection']}","{'value': 'Image generation has shown remarkable results in generating high-fidelity realistic images, in particular with the advancement of diffusion-based models. However, the prevalence of AI-generated images may have side effects for the machine learning community that are not clearly identified. Meanwhile, the success of deep learning in computer vision is driven by the massive dataset collected on the Internet. The extensive quantity of synthetic data being added to the Internet would become an obstacle for future researchers to collect ""clean"" datasets without AI-generated content. Prior research has shown that using datasets contaminated by synthetic images may result in performance degradation when used for training. In this paper, we investigate the potential impact of contaminated datasets on Online Continual Learning (CL) research. We experimentally show that contaminated datasets might hinder the training of existing online CL methods. Also, we propose Entropy Selection with Real-synthetic similarity Maximization (ESRM), a method to alleviate the performance deterioration caused by synthetic images when training online CL models. Experiments show that our method can significantly alleviate performance deterioration, especially when the contamination is severe. For reproducibility, the source code of our work is available at https://github.com/maorong-wang/ESRM.'}",https://openreview.net{'value': '/pdf/eeeaa1a535b3be8d4d63b515ebc76e0021839555.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=LX1lwP90kt,{'value': 'Modeling Latent Neural Dynamics with Gaussian Process Switching Linear Dynamical Systems'},Amber Hu; David M. Zoltowski; Aditya Nair; David Anderson; Lea Duncker; Scott Linderman,~Amber_Hu1; ~David_M._Zoltowski1; ~Aditya_Nair1; ~David_Anderson10; ~Lea_Duncker1; ~Scott_Linderman1,"{'value': ['gaussian process', 'switching', 'slds', 'neural', 'neuroscience', 'dynamics', 'probabilistic', 'time series']}","{'value': 'Understanding how the collective activity of neural populations relates to computation and ultimately behavior is a key goal in neuroscience. To this end, statistical methods which describe high-dimensional neural time series in terms of low-dimensional latent dynamics have played a fundamental role in characterizing neural systems. Yet, what constitutes a successful method involves two opposing criteria: (1) methods should be expressive enough to capture complex nonlinear dynamics, and (2) they should maintain a notion of interpretability often only warranted by simpler linear models. In this paper, we develop an approach that balances these two objectives: the Gaussian Process Switching Linear Dynamical System (gpSLDS). Our method builds on previous work modeling the latent state evolution via a stochastic differential equation whose nonlinear dynamics are described by a Gaussian process (GP-SDEs). We propose a novel kernel function which enforces smoothly interpolated locally linear dynamics, and therefore expresses flexible -- yet interpretable -- dynamics akin to those of recurrent switching linear dynamical systems (rSLDS). Our approach resolves key limitations of the rSLDS such as artifactual oscillations in dynamics near discrete state boundaries, while also providing posterior uncertainty estimates of the dynamics. To fit our models, we leverage a modified learning objective which improves the estimation accuracy of kernel hyperparameters compared to previous GP-SDE fitting approaches. We apply our method to synthetic data and data recorded in two neuroscience experiments and demonstrate favorable performance in comparison to the rSLDS.'}",https://openreview.net{'value': '/pdf/e2849e5bc0cb30e56023ec7263bfd2001aff5d83.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=LUsx0chTsL,{'value': 'Talking Heads: Understanding Inter-Layer Communication in Transformer Language Models'},Jack Merullo; Carsten Eickhoff; Ellie Pavlick,~Jack_Merullo2; ~Carsten_Eickhoff1; ~Ellie_Pavlick1,"{'value': ['mechanistic interpretability', 'interpretability', 'attention', 'subspaces', 'circuit', 'LLM', 'LM', 'language model', 'transformer']}","{'value': 'Although it is known that transformer language models (LMs) pass features from early layers to later layers, it is not well understood how this information is represented and routed by the model. We analyze a mechanism used in two LMs to selectively inhibit items in a context in one task, and find that it underlies a commonly used abstraction across many context-retrieval behaviors. Specifically, we find that models write into low-rank subspaces of the residual stream to represent features which are then read out by later layers, forming low-rank *communication channels* (Elhage et al., 2021) between layers. A particular 3D subspace in model activations in GPT-2 can be traversed to positionally index items in lists, and we show that this mechanism can explain an otherwise arbitrary-seeming sensitivity of the model to the order of items in the prompt. That is, the model has trouble copying the correct information from context when many items ``crowd"" this limited space. By decomposing attention heads with the Singular Value Decomposition (SVD), we find that previously described interactions between heads separated by one or more layers can be predicted via analysis of their weight matrices alone. We show that it is possible to manipulate the internal model representations as well as edit model weights based on the mechanism we discover in order to significantly improve performance on our synthetic Laundry List task, which requires recall from a list, often improving task accuracy by over 20\\%. Our analysis reveals a surprisingly intricate interpretable structure learned from language model pretraining, and helps us understand why sophisticated LMs sometimes fail in simple domains, facilitating future analysis of more complex behaviors.'}",https://openreview.net{'value': '/pdf/36ee0432499ff1e59c6cc78051ffaa3256ee19bc.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=LJNqVIKSCr,{'value': 'Double-Ended Synthesis Planning with Goal-Constrained Bidirectional Search'},Kevin Yu; Jihye Roh; Ziang Li; Wenhao Gao; Runzhong Wang; Connor W. Coley,~Kevin_Yu1; ~Jihye_Roh1; ~Ziang_Li2; ~Wenhao_Gao1; ~Runzhong_Wang1; ~Connor_W._Coley1,"{'value': ['Retrosynthesis', 'synthesis planning', 'chemistry', 'bidirectional search']}","{'value': 'Computer-aided synthesis planning (CASP) algorithms have demonstrated expert-level abilities in planning retrosynthetic routes to molecules of low to moderate complexity. However, current search methods assume the sufficiency of reaching arbitrary building blocks, failing to address the common real-world constraint where using specific molecules is desired. To this end, we present a formulation of synthesis planning with starting material constraints. Under this formulation, we propose Double-Ended Synthesis Planning ($\\texttt{DESP}$), a novel CASP algorithm under a _bidirectional graph search_ scheme that interleaves expansions from the target and from the goal starting materials to ensure constraint satisfiability. The search algorithm is guided by a goal-conditioned cost network learned offline from a partially observed hypergraph of valid chemical reactions. We demonstrate the utility of $\\texttt{DESP}$ in improving solve rates and reducing the number of search expansions by biasing synthesis planning towards expert goals on multiple new benchmarks. $\\texttt{DESP}$ can make use of existing one-step retrosynthesis models, and we anticipate its performance to scale as these one-step model capabilities improve.'}",https://openreview.net{'value': '/pdf/28a84c7beddeea0844e294d5a0ccea7ea6314dc5.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=LEed5Is4oi,{'value': 'Robot Policy Learning with Temporal Optimal Transport Reward'},Yuwei Fu; Haichao Zhang; Di Wu; Wei Xu; Benoit Boulet,~Yuwei_Fu1; ~Haichao_Zhang4; ~Di_Wu11; ~Wei_Xu13; ~Benoit_Boulet1,"{'value': ['Reinforcement Learning', 'Imitation Learning', 'Optimal Transport']}","{'value': 'Reward specification is one of the most tricky problems in Reinforcement Learning, which usually requires tedious hand engineering in practice. One promising approach to tackle this challenge is to adopt existing expert video demonstrations for policy learning. Some recent work investigates how to learn robot policies from only a single/few expert video demonstrations. For example, reward labeling via Optimal Transport (OT) has been shown to be an effective strategy to generate a proxy reward by measuring the alignment between the robot trajectory and the expert demonstrations. However, previous work mostly overlooks that the OT reward is invariant to temporal order information, which could bring extra noise to the reward signal. To address this issue, in this paper, we introduce the Temporal Optimal Transport (TemporalOT) reward to incorporate temporal order information for learning a more accurate OT-based proxy reward. Extensive experiments on the Meta-world benchmark tasks validate the efficacy of the proposed method. Our code is available at: https://github.com/fuyw/TemporalOT.'}",https://openreview.net{'value': '/pdf/546d5a3bfcb9e2fdc8b68c1bf6c486d493da366e.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=L8Q21Qrjmd,{'value': 'Pessimistic Backward Policy for GFlowNets'},Hyosoon Jang; Yunhui Jang; Minsu Kim; Jinkyoo Park; Sungsoo Ahn,~Hyosoon_Jang3; ~Yunhui_Jang1; ~Minsu_Kim2; ~Jinkyoo_Park1; ~Sungsoo_Ahn1,"{'value': ['Generative flow networks', 'generative models', 'reinforcement learning']}","{'value': 'This paper studies Generative Flow Networks (GFlowNets), which learn to sample objects proportionally to a given reward function through the trajectory of state transitions. In this work, we observe that GFlowNets tend to under-exploit the high-reward objects due to training on insufficient number of trajectories, which may lead to a large gap between the estimated flow and the (known) reward value. In response to this challenge, we propose a pessimistic backward policy for GFlowNets (PBP-GFN), which maximizes the observed flow to align closely with the true reward for the object. We extensively evaluate PBP-GFN across eight benchmarks, including hyper-grid environment, bag generation, structured set generation, molecular generation, and four RNA sequence generation tasks. In particular, PBP-GFN enhances the discovery of high-reward objects, maintains the diversity of the objects, and consistently outperforms existing methods.'}",https://openreview.net{'value': '/pdf/92d38b9b6ae76d6649a9aa2b01e160078cd9dbad.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=L4RwA0qyUd,{'value': 'Proximal Causal Inference With Text Data'},Jacob M. Chen; Rohit Bhattacharya; Katherine A. Keith,~Jacob_M._Chen1; ~Rohit_Bhattacharya1; ~Katherine_A._Keith1,"{'value': ['proximal causal inference', 'directed acyclic graphs', 'electronic health records', 'causal inference']}","{'value': 'Recent text-based causal methods attempt to mitigate confounding bias by estimating proxies of confounding variables that are partially or imperfectly measured from unstructured text data. These approaches, however, assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is sometimes infeasible due to data privacy or annotation costs. In this work, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that uses two instances of pre-treatment text data, infers two proxies using two zero-shot models on the separate instances, and applies these proxies in the proximal g-formula. We prove, under certain assumptions about the instances of text and accuracy of the zero-shot predictions, that our method of inferring text-based proxies satisfies identification conditions of the proximal g-formula while other seemingly reasonable proposals do not. To address untestable assumptions associated with our method and the proximal g-formula, we further propose an odds ratio falsification heuristic that flags when to proceed with downstream effect estimation using the inferred proxies. We evaluate our method in synthetic and semi-synthetic settings---the latter with real-world clinical notes from MIMIC-III and open large language models for zero-shot prediction---and find that our method produces estimates with low bias. We believe that this text-based design of proxies allows for the use of proximal causal inference in a wider range of scenarios, particularly those for which obtaining suitable proxies from structured data is difficult.'}",https://openreview.net{'value': '/pdf/0b66e866067ce02be0fd5a73f12ff0ac335a4cdf.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=L1jajNWON5,{'value': 'CondTSF: One-line Plugin of Dataset Condensation for Time Series Forecasting'},Jianrong Ding; Zhanyu Liu; Guanjie Zheng; Haiming Jin; Linghe Kong,~Jianrong_Ding1; ~Zhanyu_Liu1; ~Guanjie_Zheng1; ~Haiming_Jin2; ~Linghe_Kong1,"{'value': ['Dataset condensation', 'Time series forecasting']}","{'value': '\\textit{Dataset condensation} is a newborn technique that generates a small dataset that can be used in training deep neural networks (DNNs) to lower storage and training costs. The objective of dataset condensation is to ensure that the model trained with the synthetic dataset can perform comparably to the model trained with full datasets. However, existing methods predominantly concentrate on classification tasks, posing challenges in their adaptation to time series forecasting (TS-forecasting). This challenge arises from disparities in the evaluation of synthetic data. In classification, the synthetic data is considered well-distilled if the model trained with the full dataset and the model trained with the synthetic dataset yield identical labels for the same input, regardless of variations in output logits distribution. Conversely, in TS-forecasting, the effectiveness of synthetic data distillation is determined by the distance between predictions of the two models. The synthetic data is deemed well-distilled only when all data points within the predictions are similar. Consequently, TS-forecasting has a more rigorous evaluation methodology compared to classification. To mitigate this gap, we theoretically analyze the optimization objective of dataset condensation for TS-forecasting and propose a new one-line plugin of dataset condensation for TS-forecasting designated as Dataset \\textbf{Cond}ensation for \\textbf{T}ime \\textbf{S}eries \\textbf{F}orecasting (CondTSF) based on our analysis. Plugging CondTSF into previous dataset condensation methods facilitates a reduction in the distance between the predictions of the model trained with the full dataset and the model trained with the synthetic dataset, thereby enhancing performance. We conduct extensive experiments on eight commonly used time series datasets. CondTSF consistently improves the performance of all previous dataset condensation methods across all datasets, particularly at low condensing ratios.'}",https://openreview.net{'value': '/pdf/28cc2b016db0f17c24001943fa709b13d11a3c95.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=KxjGi1krBi,{'value': 'Bayesian Optimization of Functions over Node Subsets in Graphs'},Huidong Liang; Xingchen Wan; Xiaowen Dong,~Huidong_Liang2; ~Xingchen_Wan1; ~Xiaowen_Dong1,"{'value': ['Bayesian Optimization', 'Graphs']}","{'value': 'We address the problem of optimizing over functions defined on node subsets in a graph.  The optimization of such functions is often a non-trivial task given their combinatorial, black-box and expensive-to-evaluate nature. Although various algorithms have been introduced in the literature, most are either task-specific or computationally inefficient and only utilize information about the graph structure without considering the characteristics of the function. To address these limitations, we utilize Bayesian Optimization (BO), a sample-efficient black-box solver, and propose a novel framework for combinatorial optimization on graphs. More specifically, we map each $k$-node subset in the original graph to a node in a new combinatorial graph and adopt a local modeling approach to efficiently traverse the latter graph by progressively sampling its subgraphs using a recursive algorithm. Extensive experiments under both synthetic and real-world setups demonstrate the effectiveness of the proposed BO framework on various types of graphs and optimization tasks, where its behavior is analyzed in detail with ablation studies.'}",https://openreview.net{'value': '/pdf/4100c73ea0bf991eed715d01781fb5fd152ebb75.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=KrHFICMPjm,{'value': 'GUIDE: Real-Time Human-Shaped Agents'},Lingyu Zhang; Zhengran Ji; Nicholas R Waytowich; Boyuan Chen,~Lingyu_Zhang3; ~Zhengran_Ji1; ~Nicholas_R_Waytowich1; ~Boyuan_Chen1,"{'value': ['Human-guided Reinforcement Learning', 'Real-time decision making']}","{'value': 'The recent rapid advancement of machine learning has been driven by increasingly powerful models with the growing availability of training data and computational resources. However, real-time decision-making tasks with limited time and sparse learning signals remain challenging. One way of improving the learning speed and performance of these agents is to leverage human guidance. In this work, we introduce GUIDE, a framework for real-time human-guided reinforcement learning by enabling continuous human feedback and grounding such feedback into dense rewards to accelerate policy learning. Additionally, our method features a simulated feedback module that learns and replicates human feedback patterns in an online fashion, effectively reducing the need for human input while allowing continual training. We demonstrate the performance of our framework on challenging tasks with sparse rewards and visual observations. Our human study involving 50 subjects offers strong quantitative and qualitative evidence of the effectiveness of our approach. With only 10 minutes of human feedback, our algorithm achieves up to 30\\% increase in success rate compared to its RL baseline.'}",https://openreview.net{'value': '/pdf/4c5b77e64e14afd376d04a6edb956072c602816b.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=KjNEzWRIqn,{'value': 'Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale'},Tianyue Ou; Frank F. Xu; Aman Madaan; Jiarui Liu; Robert Lo; Abishek Sridhar; Sudipta Sengupta; Dan Roth; Graham Neubig; Shuyan Zhou,~Tianyue_Ou1; ~Frank_F._Xu1; ~Aman_Madaan1; ~Jiarui_Liu1; ~Robert_Lo1; ~Abishek_Sridhar1; ~Sudipta_Sengupta1; ~Dan_Roth3; ~Graham_Neubig1; ~Shuyan_Zhou1,"{'value': ['AI agents', 'sythetic data', 'web navigation']}","{'value': 'LLMs can now act as autonomous agents that interact with digital environments and complete specific objectives (e.g., arranging an online meeting). However, accuracy is still far from satisfactory, partly due to a lack of large-scale, direct demonstrations for digital tasks. Obtaining supervised data from humans is costly, and automatic data collection through exploration or reinforcement learning relies on complex environmental and content setup, resulting in datasets that lack comprehensive coverage of various scenarios. On the other hand, there is abundant knowledge that may indirectly assist task completion, such as online tutorials that were created for human consumption. In this work, we present Synatra, an approach that effectively transforms this indirect knowledge into direct supervision at scale. We define different types of indirect knowledge, and carefully study the available sources to obtain it, methods to encode the structure of direct demonstrations, and finally methods to transform indirect knowledge into direct demonstrations. We use 100k such synthetically-created demonstrations to finetune a 7B CodeLlama, and demonstrate that the resulting agent surpasses all comparably sized models on three web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as surpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic demonstrations prove to be only 3% the cost of human demonstrations (at $0.031 each), we show that the synthetic demonstrations can be more effective than an identical number of human demonstrations collected from limited domains.'}",https://openreview.net{'value': '/pdf/e7cd1a1be77aba5525dc023f829368207511d686.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=KcDcaVOW1S,{'value': 'Conformalized Time Series with Semantic Features'},Baiting Chen; Zhimei Ren; Lu Cheng,~Baiting_Chen1; ~Zhimei_Ren1; ~Lu_Cheng2,"{'value': ['conformal prediction', 'time series', 'neural network', 'uncertainty qualification']}","{'value': 'Conformal prediction is a powerful tool for uncertainty quantification, but its application to time-series data is constrained by the violation of the exchangeability assumption. Current solutions for time-series prediction typically operate in the output space and rely on manually selected weights to address distribution drift, leading to overly conservative predictions. To enable dynamic weight learning in the semantically rich latent space, we introduce a novel approach called Conformalized Time Series with Semantic Features (CT-SSF). CT-SSF utilizes the inductive bias in deep representation learning to dynamically adjust weights, prioritizing semantic features relevant to the current prediction. Theoretically, we show that CT-SSF surpasses previous methods defined in the output space. Experiments on synthetic and benchmark datasets demonstrate that CT-SSF significantly outperforms existing state-of-the-art (SOTA) conformal prediction techniques in terms of prediction efficiency while maintaining a valid coverage guarantee.'}",https://openreview.net{'value': '/pdf/fb45f1496d66aa4e0aee58dd0920f37bd0c31695.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=KZrfBTrPey,"{'value': ""ALI-Agent: Assessing LLMs'  Alignment with Human Values via Agent-based Evaluation""}",Jingnan Zheng; Han Wang; An Zhang; Tai D. Nguyen; Jun Sun; Tat-Seng Chua,~Jingnan_Zheng1; ~Han_Wang19; ~An_Zhang2; ~Tai_D._Nguyen1; ~Jun_Sun12; ~Tat-Seng_Chua2,"{'value': ['Large language models', 'Alignment', 'Agent', 'Evaluation']}","{'value': 'Large Language Models (LLMs) can elicit unintended and even harmful content when misaligned with human values, posing severe risks to users and society. To mitigate these risks, current evaluation benchmarks predominantly employ expert-designed contextual scenarios to assess how well LLMs align with human values. However, the labor-intensive nature of these benchmarks limits their test scope, hindering their ability to generalize to the extensive variety of open-world use cases and identify rare but crucial long-tail risks. Additionally, these static tests fail to adapt to the rapid evolution of LLMs, making it hard to evaluate timely alignment issues. To address these challenges, we propose ALI-Agent, an evaluation framework that leverages the autonomous abilities of LLM-powered agents to conduct in-depth and adaptive alignment assessments. ALI-Agent operates through two principal stages: Emulation and Refinement. During the Emulation stage, ALI-Agent automates the generation of realistic test scenarios. In the Refinement stage, it iteratively refines the scenarios to probe long-tail risks. Specifically, ALI-Agent incorporates a memory module to guide test scenario generation, a tool-using module to reduce human labor in tasks such as evaluating feedback from target LLMs, and an action module to refine tests. Extensive experiments across three aspects of human values--stereotypes, morality, and legality--demonstrate that ALI-Agent, as a general evaluation framework, effectively identifies model misalignment. Systematic analysis also validates that the generated test scenarios represent meaningful use cases, as well as integrate enhanced measures to probe long-tail risks.'}",https://openreview.net{'value': '/pdf/48717ce572116e740ccd3f83f3344d95ffee435c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=KYHma7hzjr,{'value': 'Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?'},Sonia Laguna; Ričards Marcinkevičs; Moritz Vandenhirtz; Julia E Vogt,~Sonia_Laguna1; ~Ričards_Marcinkevičs1; ~Moritz_Vandenhirtz1; ~Julia_E_Vogt1,"{'value': ['interpretability', 'explainability', 'concepts', 'concept bottleneck models', 'model interventions', 'healthcare']}","{'value': ""Recently, interpretable machine learning has re-explored concept bottleneck models (CBM). An advantage of this model class is the user's ability to intervene on predicted concept values, affecting the downstream output. In this work, we introduce a method to perform such concept-based interventions on *pretrained* neural networks, which are not interpretable by design, only given a small validation set with concept labels. Furthermore, we formalise the notion of *intervenability* as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black boxes. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We focus on backbone architectures of varying complexity, from simple, fully connected neural nets to Stable Diffusion. We demonstrate that the proposed fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the practical utility of our techniques, we apply them to deep chest X-ray classifiers and show that fine-tuned black boxes are more intervenable than CBMs. Lastly, we establish that our methods are still effective under vision-language-model-based concept annotations, alleviating the need for a human-annotated validation set.""}",https://openreview.net{'value': '/pdf/1caac2e59a9c47f2b1cc88e5c752928530d40b96.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=KKrj1vCQaG,{'value': 'RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance'},Zhicheng Sun; Zhenhao Yang; Yang Jin; Haozhe Chi; Kun Xu; Kun Xu; Liwei Chen; Hao Jiang; Yang Song; Kun Gai; Yadong MU,~Zhicheng_Sun1; ~Zhenhao_Yang1; ~Yang_Jin1; ~Haozhe_Chi1; ~Kun_Xu4; ~Kun_Xu6; ~Liwei_Chen3; ~Hao_Jiang10; ~Yang_Song6; ~Kun_Gai1; ~Yadong_MU1,"{'value': ['Personalized Image Generation', 'Rectified Flow', 'Classifier Guidance']}","{'value': 'Customizing diffusion models to generate identity-preserving images from user-provided reference images is an intriguing new problem. The prevalent approaches typically require training on extensive domain-specific images to achieve identity preservation, which lacks flexibility across different use cases. To address this issue, we exploit classifier guidance, a training-free technique that steers diffusion models using an existing classifier, for personalized image generation. Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators. Moreover, its solving procedure proves to be stable when anchored to a reference flow trajectory, with a convergence guarantee. The derived method is implemented on rectified flow with different off-the-shelf image discriminators, delivering advantageous personalization results for human faces, live subjects, and certain objects. Code is available at https://github.com/feifeiobama/RectifID.'}",https://openreview.net{'value': '/pdf/c8f2d67b397ef8a8eee9fddcf495d00135c935b4.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=K9IGlMQpif,{'value': 'SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models'},Yu Yang; Siddhartha Mishra; Jeffrey N Chiang; Baharan Mirzasoleiman,~Yu_Yang4; ~Siddhartha_Mishra3; ~Jeffrey_N_Chiang1; ~Baharan_Mirzasoleiman1,{'value': ['data selection']},"{'value': 'Despite the effectiveness of data selection for pretraining and instruction fine-tuning\nlarge language models (LLMs), improving data efficiency in supervised fine-tuning\n(SFT) for specialized domains poses significant challenges due to the complexity\nof fine-tuning data. To bridge this gap, we introduce an effective and scalable\ndata selection method for SFT, SmallToLarge (S2L), which trains a small\nmodel, clusters loss trajectories of the examples, and samples from these clusters to\nguide data selection for larger models. We prove that during fine-tuning, samples\nwithin the same loss trajectory cluster exhibit similar gradients. Then, we show\nthat S2L subsets have a bounded gradient error w.r.t. the full data, hence guarantee\nconvergence to the neighborhood of the optimal solution. We demonstrate through\nextensive experiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data requirement to just $11$%\nof the original MathInstruct dataset to match full dataset performance while\noutperforming state-of-the-art data selection algorithms by an average of $4.7$%\nacross $6$ in- and out-domain evaluation datasets. Remarkably, selecting only 50K\ndata for SFT, S2L achieves a $32.7$% accuracy on the challenging MATH\nbenchmark, improving Phi-2 by $16.6$%. In clinical text summarization on the\nMIMIC-III dataset, S2L again outperforms training on the full dataset using\nonly $50$% of the data. Notably, S2L can perform scalable data selection using a\nreference model $100\\times$ smaller than the target model, proportionally reducing the\ncomputational cost.'}",https://openreview.net{'value': '/pdf/0a2da6d64cd7f9e62b8aa4f9c56311ab881fcbfa.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=JM0IQSliol,{'value': 'Shape analysis for time series'},Thibaut Germain; Samuel Gruffaz; Charles Truong; Alain Oliviero Durmus; Laurent Oudre,~Thibaut_Germain1; ~Samuel_Gruffaz1; ~Charles_Truong1; ~Alain_Oliviero_Durmus1; ~Laurent_Oudre2,"{'value': ['Machine learning for sciences', 'Machine learning for healthcare', 'Representation learning for time series', 'Shape analysis', 'LDDMM', 'Kernel methods']}","{'value': 'Analyzing inter-individual variability of physiological functions is particularly appealing in medical and biological contexts to describe or quantify health conditions. Such analysis can be done by comparing individuals to a reference one with time series as biomedical data.\nThis paper introduces an unsupervised representation learning (URL) algorithm for time series tailored to inter-individual studies. The idea is to represent time series as deformations of a reference time series. The deformations are diffeomorphisms parameterized and learned by our method called TS-LDDMM. Once the deformations and the reference time series are learned, the vector representations of individual time series are given by the parametrization of their corresponding deformation. At the crossroads between URL for time series and shape analysis, the proposed algorithm handles irregularly sampled multivariate time series of variable lengths and provides shape-based representations of temporal data.\nIn this work, we establish a representation theorem for the graph of a time series and derive its consequences on the LDDMM framework. We showcase the advantages of our representation compared to existing methods using synthetic data and real-world examples motivated by biomedical applications.'}",https://openreview.net{'value': '/pdf/f78274931fd6e1b135d50eb16edc4e3742ec3c56.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=JEflV4nRlH,{'value': 'What Makes and Breaks Safety Fine-tuning? A Mechanistic Study'},Samyak Jain; Ekdeep Singh Lubana; Kemal Oksuz; Tom Joy; Philip Torr; Amartya Sanyal; Puneet K. Dokania,~Samyak_Jain1; ~Ekdeep_Singh_Lubana1; ~Kemal_Oksuz1; ~Tom_Joy1; ~Philip_Torr1; ~Amartya_Sanyal1; ~Puneet_K._Dokania1,"{'value': ['Mechanistic Interpretability', 'AI Safety', 'Safety fine tuning', 'Large Language Models']}","{'value': 'Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g., “design”) versus the specific concepts the task is asked to be performed upon (e.g., a “cycle” vs. a “bomb”). Using this, we investigate three well-known safety fine-tuning methods—supervised safety fine-tuning, direct preference optimization, and unlearning—and provide significant evidence demonstrating that these methods minimally transform MLP weights to specifically align unsafe inputs into its weights’ null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. Code is available at https://github.com/fiveai/understanding_safety_finetuning.'}",https://openreview.net{'value': '/pdf/81116601d33b2f8062d8349ffea18be421ec92bf.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=JCyBN5syv3,{'value': 'SimGen: Simulator-conditioned Driving Scene Generation'},Yunsong Zhou; Michael Simon; Zhenghao Peng; Sicheng Mo; Hongzi Zhu; Minyi Guo; Bolei Zhou,~Yunsong_Zhou1; ~Michael_Simon1; ~Zhenghao_Peng1; ~Sicheng_Mo2; ~Hongzi_Zhu1; ~Minyi_Guo1; ~Bolei_Zhou5,"{'value': ['Autonomous Driving', 'Generative Models', 'Simulators']}","{'value': 'Controllable synthetic data generation can substantially lower the annotation cost of training data. Prior works use diffusion models to generate driving images conditioned on the 3D object layout. However, those models are trained on small-scale datasets like nuScenes, which lack appearance and layout diversity. Moreover, overfitting often happens, where the trained models can only generate images based on the layout data from the validation set of the same dataset. In this work, we introduce a simulator-conditioned scene generation framework called SimGen that can learn to generate diverse driving scenes by mixing data from the simulator and the real world. It uses a novel cascade diffusion pipeline to address challenging sim-to-real gaps and multi-condition conflicts. A driving video dataset DIVA is collected to enhance the generative diversity of SimGen, which contains over 147.5 hours of real-world driving videos from 73 locations worldwide and simulated driving data from the MetaDrive simulator. SimGen achieves superior generation quality and diversity while preserving controllability based on the text prompt and the layout pulled from a simulator. We further demonstrate the improvements brought by SimGen for synthetic data augmentation on the BEV detection and segmentation task and showcase its capability in safety-critical data generation.'}",https://openreview.net{'value': '/pdf/767991e0d8bf535e0c7ec95c3f3222ed05c30813.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=J709rtAUD1,{'value': 'Causal Temporal Representation Learning with Nonstationary Sparse Transition'},Xiangchen Song; Zijian Li; Guangyi Chen; Yujia Zheng; Yewen Fan; Xinshuai Dong; Kun Zhang,~Xiangchen_Song1; ~Zijian_Li1; ~Guangyi_Chen1; ~Yujia_Zheng1; ~Yewen_Fan1; ~Xinshuai_Dong1; ~Kun_Zhang1,"{'value': ['Causal Representation Learning', 'Identifiability', 'Nonlinear ICA', 'Temporal Distribution Shift']}","{'value': 'Causal Temporal Representation Learning (Ctrl) methods aim to identify the temporal causal dynamics of complex nonstationary temporal sequences. Despite the success of existing Ctrl methods, they require either directly observing the domain variables or assuming a Markov prior on them. Such requirements limit the application of these methods in real-world scenarios when we do not have such prior knowledge of the domain variables. To address this problem, this work adopts a sparse transition assumption, aligned with intuitive human understanding, and presents identifiability results from a theoretical perspective. In particular, we explore under what conditions on the significance of the variability of the transitions we can build a model to identify the distribution shifts. Based on the theoretical result, we introduce a novel framework, *Causal Temporal Representation Learning with Nonstationary Sparse Transition* (CtrlNS), designed to leverage the constraints on transition sparsity and conditional independence to reliably identify both distribution shifts and latent factors. Our experimental evaluations on synthetic and real-world datasets demonstrate significant improvements over existing baselines, highlighting the effectiveness of our approach.'}",https://openreview.net{'value': '/pdf/186b67ba23a623ccd652075782ed79fd03e35ba7.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=J1Y70keorq,{'value': 'Multi-model Ensemble Conformal Prediction in Dynamic Environments'},Erfan Hajihashemi; Yanning Shen,~Erfan_Hajihashemi1; ~Yanning_Shen1,"{'value': ['uncertainty quantification', 'conformal prediction', 'dynamic environments']}","{'value': 'Conformal prediction is an uncertainty quantification method that constructs a prediction set for a previously unseen datum, ensuring the true label is included with a predetermined coverage probability. Adaptive conformal prediction has been developed to address data distribution shifts in dynamic environments. However, the efficiency of prediction sets varies depending on the learning model used. Employing a single fixed model may not consistently offer the best performance in dynamic environments with unknown data distribution shifts. To address this issue, we introduce a novel adaptive conformal prediction framework, where the model used for creating prediction sets is selected ‘on the fly’ from multiple candidate models. The proposed algorithm is proven to achieve strongly adaptive regret over all intervals while maintaining valid coverage. Experiments on both real and synthetic datasets corroborate that the proposed approach consistently yields more efficient prediction sets while maintaining valid coverage, outperforming alternative methods.'}",https://openreview.net{'value': '/pdf/e8099609fd67117212f5fdae1d419adf13be51f9.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=J0Itri0UiN,{'value': 'Counterfactual Fairness by Combining Factual and Counterfactual Predictions'},Zeyu Zhou; Tianci Liu; Ruqi Bai; Jing Gao; Murat Kocaoglu; David I. Inouye,~Zeyu_Zhou1; ~Tianci_Liu1; ~Ruqi_Bai1; ~Jing_Gao1; ~Murat_Kocaoglu1; ~David_I._Inouye1,"{'value': ['Counterfactual Fairness', 'Fairness', 'Trustworthy ML']}","{'value': ""In high-stakes domains such as healthcare and hiring, the role of machine learning (ML) in decision-making raises significant fairness concerns. \nThis work focuses on Counterfactual Fairness (CF), which posits that an ML model's outcome on any individual should remain unchanged if they had belonged to a different demographic group.\nPrevious works have proposed methods that guarantee CF. \nNotwithstanding, their effects on the model's predictive performance remain largely unclear.\nTo fill this gap, we provide a theoretical study on the inherent trade-off between CF and predictive performance in a model-agnostic manner. \nWe first propose a simple but effective method to cast an optimal but potentially unfair predictor into a fair one with a minimal loss of performance.\nBy analyzing the excess risk incurred by perfect CF, we quantify this inherent trade-off. \nFurther analysis on our method's performance with access to only incomplete causal knowledge is also conducted. \nBuilt upon this, we propose a practical algorithm that can be applied in such scenarios. \nExperiments on both synthetic and semi-synthetic datasets demonstrate the validity of our analysis and methods.""}",https://openreview.net{'value': '/pdf/f4f77f1b31ce0d2755f2ba534318ec70d62f6cb3.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=IxazPgGF8h,{'value': 'ChatCam: Empowering Camera Control through Conversational AI'},Xinhang Liu; Yu-Wing Tai; Chi-Keung Tang,~Xinhang_Liu1; ~Yu-Wing_Tai2; ~Chi-Keung_Tang1,"{'value': ['camera operation', 'LLM']}","{'value': ""Cinematographers adeptly capture the essence of the world, crafting compelling visual narratives through intricate camera movements. Witnessing the strides made by large language models in perceiving and interacting with the 3D world, this study explores their capability to control cameras with human language guidance. We introduce ChatCam, a system that navigates camera movements through conversations with users, mimicking a professional cinematographer's workflow. To achieve this, we propose CineGPT, a GPT-based autoregressive model for text-conditioned camera trajectory generation. We also develop an Anchor Determinator to ensure precise camera trajectory placement. ChatCam understands user requests and employs our proposed tools to generate trajectories, which can be used to render high-quality video footage on radiance field representations. Our experiments, including comparisons to state-of-the-art approaches and user studies, demonstrate our approach's ability to interpret and execute complex instructions for camera operation, showing promising applications in real-world production settings. Project page: https://xinhangliu.com/chatcam.""}",https://openreview.net{'value': '/pdf/c3de0c5e1ef391957274ddbe6d9b8ccd92b4743f.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Iq2IAWozNr,{'value': 'Smoke and Mirrors in Causal Downstream Tasks'},Riccardo Cadei; Lukas Lindorfer; Sylvia Cremer; Cordelia Schmid; Francesco Locatello,~Riccardo_Cadei1; ~Lukas_Lindorfer1; ~Sylvia_Cremer1; ~Cordelia_Schmid1; ~Francesco_Locatello1,"{'value': ['AI for Science', 'Randomized Controlled Trial', 'Representation Learning']}","{'value': 'Machine Learning and AI have the potential to transform data-driven scientific discovery, enabling accurate predictions for several scientific phenomena. As many scientific questions are inherently causal, this paper looks at the causal inference task of treatment effect estimation, where the outcome of interest is recorded in high-dimensional observations in a Randomized Controlled Trial (RCT). Despite being the simplest possible causal setting and a perfect fit for deep learning, we theoretically find that many common choices in the literature may lead to biased estimates. To test the practical impact of these considerations, we recorded ISTAnt, the first real-world benchmark for causal inference downstream tasks on high-dimensional observations as an RCT studying how garden ants (Lasius neglectus) respond to microparticles applied onto their colony members by hygienic grooming. Comparing 6 480 models fine-tuned from state-of-the-art visual backbones, we find that the sampling and modeling choices significantly affect the accuracy of the causal estimate, and that classification accuracy is not a proxy thereof. We further validated the analysis, repeating it on a synthetically generated visual data set controlling the causal model. Our results suggest that future benchmarks should carefully consider real downstream scientific questions, especially causal ones. Further, we highlight guidelines for representation learning methods to help answer causal questions in the sciences.'}",https://openreview.net{'value': '/pdf/0da02ead31eaa563152d1c602f5227264b7e0596.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=IoRT7EhFap,{'value': 'Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning'},Ronglong Fang; Yuesheng Xu,~Ronglong_Fang1; ~Yuesheng_Xu1,"{'value': ['deep neural network', 'spectral bias', 'multi-grade deep learning']}","{'value': 'Deep neural networks (DNNs) have showcased their remarkable precision in approximating smooth functions. However, they suffer from the {\\it spectral bias}, wherein DNNs typically exhibit a tendency to prioritize the learning of lower-frequency components of a function, struggling to effectively capture its high-frequency features. This paper is to address this issue. Notice that a function having only low frequency components may be well-represented by a shallow neural network (SNN), a network having only a few layers. By observing that composition of low frequency functions can effectively approximate a high-frequency function, we propose to learn a function containing high-frequency components by composing several SNNs, each of which learns certain low-frequency information from the given data. We implement the proposed idea by exploiting the multi-grade deep learning (MGDL) model, a recently introduced model that trains a DNN incrementally, grade by grade, a current grade learning from the residue of the previous grade only an SNN (with trainable parameters) composed with the SNNs (with fixed parameters) trained in the preceding grades as features. We apply MGDL to synthetic, manifold, colored images, and MNIST datasets, all characterized by presence of high-frequency features. Our study reveals that MGDL excels at representing functions containing high-frequency information. Specifically, the neural networks learned in each grade adeptly capture some low-frequency information, allowing their compositions with SNNs learned in the previous grades effectively representing the high-frequency features. Our experimental results underscore the efficacy of MGDL in addressing the spectral bias inherent in DNNs. By leveraging MGDL, we offer insights into overcoming spectral bias limitation of DNNs, thereby enhancing the performance and applicability of deep learning models in tasks requiring the representation of high-frequency information. This study confirms that the proposed method offers a promising solution to address the spectral bias of DNNs. The code is available on GitHub: \\href{https://github.com/Ronglong-Fang/AddressingSpectralBiasviaMGDL}{\\texttt{Addressing Spectral Bias via MGDL}}.'}",https://openreview.net{'value': '/pdf/7c1832b55ff98718216586022306db75081c62a3.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=IRXyPm9IPW,{'value': 'Multimodal Large Language Models Make Text-to-Image Generative Models Align Better'},Xun Wu; Shaohan Huang; Guolong Wang; Jing Xiong; Furu Wei,~Xun_Wu1; ~Shaohan_Huang1; ~Guolong_Wang1; ~Jing_Xiong4; ~Furu_Wei1,{'value': ['Text-to-Image Generation;Reinforcement Learning from AI Feedback;Multimodal Large Language Model']},"{'value': 'Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration. To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects. We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer. To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators. Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions. Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models.'}",https://openreview.net{'value': '/pdf/b1997ae3b608c9cb6c81e9d59197e5a4b371fe4a.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=I2gVmVRgNk,{'value': 'Towards Understanding Evolving Patterns in Sequential Data'},QIUHAO Zeng; Long-Kai Huang; Qi CHEN; Charles Ling; Boyu Wang,~QIUHAO_Zeng1; ~Long-Kai_Huang1; ~Qi_CHEN6; ~Charles_Ling1; ~Boyu_Wang3,"{'value': ['Time series', 'sequential data', 'autoregressive tasks', 'evolving domain generalization', 'temporal domain generalization']}","{'value': 'In many machine learning tasks, data is inherently sequential. Most existing algorithms learn from sequential data in an auto-regressive manner, which predicts the next unseen data point based on the observed sequence, implicitly assuming the presence of an \\emph{evolving pattern} embedded in the data that can be leveraged. However, identifying and assessing evolving patterns in learning tasks often relies on subjective judgments rooted in the prior knowledge of human experts, lacking a standardized quantitative measure. Furthermore, such measures enable us to determine the suitability of employing sequential models effectively and make informed decisions on the temporal order of time series data, and feature/data selection processes. To address this issue, we introduce the Evolving Rate (EvoRate), which quantitatively approximates the intensity of evolving patterns in the data with Mutual Information. Furthermore, in some temporal data with neural mutual information estimations, we only have snapshots at different timestamps, lacking correspondence, which hinders EvoRate estimation. To tackle this challenge, we propose EvoRate$_\\mathcal{W}$, aiming to establish correspondence with optimal transport for estimating the first-order EvoRate. Experiments on synthetic and real-world datasets including images and tabular data validate the efficacy of our EvoRate.'}",https://openreview.net{'value': '/pdf/1bad237b38f022e552a2d25e8441f9509ba655a2.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Hz6cSigMyU,{'value': 'Reinforcing LLM Agents via Policy Optimization with Action Decomposition'},Muning Wen; Ziyu Wan; Jun Wang; Weinan Zhang; Ying Wen,~Muning_Wen2; ~Ziyu_Wan2; ~Jun_Wang2; ~Weinan_Zhang1; ~Ying_Wen1,"{'value': ['Reinforcement Learning', 'Language Agent', 'LLM agent', 'Large Language Models']}","{'value': ""Language models as intelligent agents push the boundaries of sequential decision-making agents but struggle with limited knowledge of environmental dynamics and exponentially huge action space. Recent efforts like GLAM and TWOSOME manually constrain the action space to a restricted subset and employ reinforcement learning to align agents' knowledge with specific environments. However, they overlook fine-grained credit assignments for intra-action tokens, which is essential for efficient language agent optimization, and rely on human's prior knowledge to restrict action space. This paper proposes decomposing language agent optimization from the action level to the token level, offering finer supervision for each intra-action token and manageable optimization complexity in environments with unrestricted action spaces. Beginning with the simplification of flattening all actions, we theoretically explore the discrepancies between action-level optimization and this naive token-level optimization. We then derive the Bellman backup with Action Decomposition (BAD) to integrate credit assignments for both intra-action and inter-action tokens, effectively eliminating the discrepancies. Implementing BAD within the PPO algorithm, we introduce Policy Optimization with Action Decomposition (POAD). POAD benefits from a finer-grained credit assignment process and lower optimization complexity, leading to enhanced learning efficiency and generalization abilities in aligning language agents with interactive environments. We validate POAD across diverse testbeds, with results affirming the advantages of our approach and the correctness of our theoretical analysis. The source code can be accessed directly with this link: https://github.com/morning9393/ADRL.""}",https://openreview.net{'value': '/pdf/fd07ad5fd7b998602681408100ee3e0c5bbd23c8.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=HkC4OYee3Q,{'value': 'SleeperNets: Universal Backdoor Poisoning Attacks Against  Reinforcement Learning Agents'},Ethan Rathbun; Christopher Amato; Alina Oprea,~Ethan_Rathbun1; ~Christopher_Amato1; ~Alina_Oprea1,"{'value': ['Reinforcement Learning', 'Backdoor Attacks', 'Adversarial Machine Learning', 'Security', 'Poisoning Attacks', 'Reinforcement Learning Theory']}","{'value': 'Reinforcement learning (RL) is an actively growing field that is seeing increased usage in real-world, safety-critical applications -- making it paramount to ensure the robustness of RL algorithms against adversarial attacks. In this work we explore a particularly stealthy form of training-time attacks against RL -- backdoor poisoning. Here the adversary intercepts the training of an RL agent with the goal of reliably inducing a particular action when the agent observes a pre-determined trigger at inference time. We uncover theoretical limitations of prior work by proving their inability to generalize across domains and MDPs. Motivated by this, we formulate a novel poisoning attack framework which interlinks the adversary\'s objectives with those of finding an optimal policy -- guaranteeing attack success in the limit. Using insights from our theoretical analysis we develop ""SleeperNets"" as a universal backdoor attack which exploits a newly proposed threat model and leverages dynamic reward poisoning techniques. We evaluate our attack in 6 environments spanning multiple domains and demonstrate significant improvements in attack success over existing methods, while preserving benign episodic return.'}",https://openreview.net{'value': '/pdf/ee7d10957a89cd861d00b9a5517286275742c5b1.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=HfztZgwpxI,{'value': 'Transferring disentangled representations: bridging the gap between synthetic and real images'},Jacopo Dapueto; Nicoletta Noceti; Francesca Odone,~Jacopo_Dapueto1; ~Nicoletta_Noceti1; ~Francesca_Odone1,"{'value': ['Disentangled Representations', 'Transfer Learning', 'Syn2Real']}","{'value': 'Developing meaningful and efficient representations that separate the fundamental structure of the data generation mechanism is crucial in representation learning. However, Disentangled Representation Learning has not fully shown its potential on real images, because of correlated generative factors, their resolution and limited access to ground truth labels. Specifically on the latter, we investigate the possibility of leveraging synthetic data to learn general-purpose disentangled representations applicable to real data, discussing the effect of fine-tuning and what properties of disentanglement are preserved after the transfer. We provide an extensive empirical study to address these issues. In addition, we propose a new interpretable intervention-based metric, to measure the quality of factors encoding in the representation. Our results indicate that some level of disentanglement, transferring a representation from synthetic to real data, is possible and effective.'}",https://openreview.net{'value': '/pdf/0055fe64df12a35787aac784381943547daadc76.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=HcqV2bPFKz,{'value': 'Hierarchical Object-Aware Dual-Level Contrastive Learning for Domain Generalized Stereo Matching'},Yikun Miao; Meiqing Wu; Siew Kei Lam; Changsheng Li; Thambipillai Srikanthan,~Yikun_Miao1; ~Meiqing_Wu1; ~Siew_Kei_Lam1; ~Changsheng_Li4; ~Thambipillai_Srikanthan1,"{'value': ['Stereo Matching', 'Domain Generalization', 'Object-Aware', 'Contrastive Learning']}","{'value': 'Stereo matching algorithms that leverage end-to-end convolutional neural networks have recently demonstrated notable advancements in performance. However, a common issue is their susceptibility to domain shifts, hindering their ability in generalizing to diverse, unseen realistic domains. We argue that existing stereo matching networks overlook the importance of extracting semantically and structurally meaningful features. To address this gap, we propose an effective hierarchical object-aware dual-level contrastive learning (HODC) framework for domain generalized stereo matching. Our framework guides the model in extracting features that support semantically and structurally driven matching by segmenting objects at different scales and enhances correspondence between intra- and inter-scale regions from the left feature map to the right using dual-level contrastive loss. HODC can be integrated with existing stereo matching models in the training stage, requiring no modifications to the architecture. Remarkably, using only synthetic datasets for training, HODC achieves state-of-the-art generalization performance with various existing stereo matching network architectures, across multiple realistic datasets.'}",https://openreview.net{'value': '/pdf/71cf4cbf386c597dfe5d407198c4f456ba22e474.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=HU2uyDjAcy,{'value': 'Local and Adaptive Mirror Descents in Extensive-Form Games'},Côme Fiegel; Pierre Menard; Tadashi Kozuno; Remi Munos; Vianney Perchet; Michal Valko,~Côme_Fiegel1; ~Pierre_Menard2; ~Tadashi_Kozuno1; ~Remi_Munos1; ~Vianney_Perchet3; ~Michal_Valko1,"{'value': ['online learning', 'game theory', 'extensive-form games', 'mirror descent']}","{'value': 'We study how to learn $\\epsilon$-optimal strategies in zero-sum imperfect information games (IIG) with *trajectory feedback*. In this setting, players update their policies sequentially, based on their observations over a fixed number of episodes denoted by $T$. Most existing procedures suffer from high variance due to the use of importance sampling over sequences of actions. To reduce this variance, we consider a *fixed sampling* approach, where players still update their policies over time, but with observations obtained through a given fixed sampling policy. Our approach is based on an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set, using individually decreasing learning rates and a *regularized loss*. We show that this approach guarantees a convergence rate of $\\tilde{\\mathcal{O}}(T^{-1/2})$ with high probability and has a near-optimal dependence on the game parameters when applied with the best theoretical choices of learning rates and sampling policies. To achieve these results, we generalize the notion of OMD stabilization, allowing for time-varying regularization with convex increments.'}",https://openreview.net{'value': '/pdf/d611ef23f6215c52bc55a606306c5aa1359e045c.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=HSRs6yyuUK,{'value': 'Preventing Model Collapse in Deep Canonical Correlation Analysis by Noise Regularization'},Junlin He; Jinxiao Du; Susu Xu; Wei Ma,~Junlin_He2; ~Jinxiao_Du1; ~Susu_Xu2; ~Wei_Ma3,{'value': ['Multi-view representation learning; Canonical Correlation Analysis; Deep Canonical Correlation Analysis; Noise regularization; Model collapse']},"{'value': 'Multi-View Representation Learning (MVRL) aims to learn a unified representation of an object from multi-view data.\nDeep Canonical Correlation Analysis (DCCA) and its variants share simple formulations and demonstrate state-of-the-art performance. However, with extensive experiments, we observe the issue of model collapse, i.e., the performance of DCCA-based methods will drop drastically when training proceeds. The model collapse issue could significantly hinder the wide adoption of DCCA-based methods because it is challenging to decide when to early stop. To this end, we develop NR-DCCA, which is equipped with a novel noise regularization approach to prevent model collapse. Theoretical analysis shows that the Correlation Invariant Property is the key to preventing model collapse, and our noise regularization forces the neural network to possess such a property. A framework to construct synthetic data with different common and complementary information is also developed to compare MVRL methods comprehensively. The developed NR-DCCA outperforms baselines stably and consistently in both synthetic and real-world datasets, and the proposed noise regularization approach can also be generalized to other DCCA-based methods such as DGCCA.'}",https://openreview.net{'value': '/pdf/899fd75b3fc319a346b3e5d8053573a679fa9953.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=HSJOt2hyDf,{'value': 'Initializing Services in Interactive ML Systems for Diverse Users'},Avinandan Bose; Mihaela Curmei; Daniel L. Jiang; Jamie Heather Morgenstern; Sarah Dean; Lillian J. Ratliff; Maryam Fazel,~Avinandan_Bose1; ~Mihaela_Curmei2; ~Daniel_L._Jiang2; ~Jamie_Heather_Morgenstern1; ~Sarah_Dean2; ~Lillian_J._Ratliff1; ~Maryam_Fazel1,"{'value': ['Algorithm design for multi-service ML systems', 'initialization', 'clustering', 'approximation ratio', 'preference learning']}","{'value': ""This paper investigates ML systems serving a group of users, with multiple models/services, each aimed at specializing to a sub-group of users. We consider settings where upon deploying a set of services, users choose the one minimizing their personal losses and the learner iteratively learns by interacting with diverse users. Prior research shows that the outcomes of learning dynamics, which comprise both the services' adjustments and users' service selections, hinge significantly on the initial conditions. However, finding good initial conditions faces two main challenges: (i) \\emph{Bandit feedback:} Typically, data on user preferences are not available before deploying services \nand observing user behavior; (ii) \\emph{Suboptimal local solutions:} The total loss landscape (i.e., the sum of loss functions across all users and services) is not convex and gradient-based algorithms can get stuck in poor local minima.\n\nWe address these challenges with a randomized algorithm to adaptively select a minimal set of users for data collection in order to initialize a set of services. Under mild assumptions on the loss functions, we prove that our initialization leads to a total loss within a factor of the \\textit{globally optimal total loss,with complete user preference data}, and this factor scales logarithmically in the number of services. This result is a generalization of the well-known $k$-means++ guarantee to a broad problem class which is also of independent interest.\nThe theory is complemented by experiments on real as well as semi-synthetic datasets.""}",https://openreview.net{'value': '/pdf/8248557cf19e225459989218fe140e3a5e05036e.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=HQgHCVZiHw,{'value': 'Is Score Matching Suitable for Estimating Point Processes?'},Haoqun Cao; Zizhuo Meng; Tianjun Ke; Feng Zhou,~Haoqun_Cao1; ~Zizhuo_Meng1; ~Tianjun_Ke1; ~Feng_Zhou9,"{'value': ['point processes', 'score matching', 'parameter estimation']}","{'value': 'Score matching estimators for point processes have gained widespread attention in recent years because they do not require the calculation of intensity integrals, thereby effectively addressing the computational challenges in maximum likelihood estimation (MLE). Some existing works have proposed score matching estimators for point processes. However, this work demonstrates that the incompleteness of the estimators proposed in those works renders them applicable only to specific problems, and they fail for more general point processes. To address this issue, this work introduces the weighted score matching estimator to point processes. Theoretically, we prove the consistency of the estimator we propose. Experimental results indicate that our estimator accurately estimates model parameters on synthetic data and yields results consistent with MLE on real data. In contrast, existing score matching estimators fail to perform effectively. Codes are publicly available at \\url{https://github.com/KenCao2007/WSM_TPP}.'}",https://openreview.net{'value': '/pdf/5d086f5651da668c868278266d3a7cc9a716a273.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=HFS800reZK,{'value': 'Learning Representations for Hierarchies with Minimal Support'},Benjamin Rozonoyer; Michael Boratko; Dhruvesh Patel; Wenlong Zhao; Shib Sankar Dasgupta; Hung Le; Andrew McCallum,~Benjamin_Rozonoyer1; ~Michael_Boratko1; ~Dhruvesh_Patel1; ~Wenlong_Zhao1; ~Shib_Sankar_Dasgupta2; ~Hung_Le4; ~Andrew_McCallum1,"{'value': ['graph embeddings', 'representation learning']}","{'value': 'When training node embedding models to represent large directed graphs (digraphs), it is impossible to observe all entries of the adjacency matrix during training. As a consequence most methods employ sampling. For very large digraphs, however, this means many (most) entries may be unobserved during training. In general, observing every entry would be necessary to uniquely identify a graph, however if we know the graph has a certain property some entries can be omitted - for example, only half the entries would be required for a symmetric graph. \nIn this work, we develop a novel framework to identify a subset of entries required to uniquely distinguish a graph among all transitively-closed DAGs. We give an explicit algorithm to compute the provably minimal set of entries, and demonstrate empirically that one can train node embedding models with greater efficiency and performance, provided the energy function has an appropriate inductive bias. We achieve robust performance on synthetic hierarchies and a larger real-world taxonomy, observing improved convergence rates in a resource-constrained setting while reducing the set of training examples by as much as 99%.'}",https://openreview.net{'value': '/pdf/98c7ccf6ef86019ffc994aba434e5c6603739459.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=H7qVZ0Zu8E,{'value': 'Achieving Linear Convergence with Parameter-Free Algorithms in Decentralized Optimization'},Ilya Kuruzov; Gesualdo Scutari; Alexander Gasnikov,~Ilya_Kuruzov1; ~Gesualdo_Scutari1; ~Alexander_Gasnikov1,"{'value': ['decentralized optimization', 'linear convergence', 'parameter free']}","{'value': 'This paper addresses the minimization of the sum of strongly convex, smooth\nfunctions over a network of agents without a centralized server. Existing decentralized algorithms require knowledge of functions and network parameters, such as the Lipschitz constant of the global gradient and/or network connectivity, for\nhyperparameter tuning. Agents usually cannot access this information, leading\nto conservative selections and slow convergence or divergence. This paper introduces a decentralized algorithm that eliminates the need for specific parameter\ntuning. Our approach employs an operator splitting technique with a novel variable\nmetric, enabling a local backtracking line-search to adaptively select the stepsize\nwithout global information or extensive communications. This results in favorable\nconvergence guarantees and dependence on optimization and network parameters\ncompared to existing nonadaptive methods. Notably, our method is the first adaptive decentralized algorithm that achieves linear convergence for strongly convex,\nsmooth objectives. Preliminary numerical experiments support our theoretical\nfindings, demonstrating superior performance in convergence speed and scalability.'}",https://openreview.net{'value': '/pdf/b81c0c22820ab25025a5d7c893e8927677325f5d.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Gug7wc0BSs,{'value': 'Value-Based Deep Multi-Agent Reinforcement Learning with Dynamic Sparse Training'},Pihe Hu; Shaolong Li; Zhuoran Li; Ling Pan; Longbo Huang,~Pihe_Hu1; ~Shaolong_Li2; ~Zhuoran_Li1; ~Ling_Pan1; ~Longbo_Huang2,"{'value': ['Multi-Agent Reinforcement Learning', 'Dynamic Sparse Training', 'Value Learning']}","{'value': 'Deep Multi-agent Reinforcement Learning (MARL) relies on neural networks with numerous parameters in multi-agent scenarios, often incurring substantial computational overhead. Consequently, there is an urgent need to expedite training and enable model compression in MARL. This paper proposes the utilization of dynamic sparse training (DST), a technique proven effective in deep supervised learning tasks, to alleviate the computational burdens in MARL training. However, a direct adoption of DST fails to yield satisfactory MARL agents, leading to breakdowns in value learning within deep sparse value-based MARL models. Motivated by this challenge, we introduce an innovative Multi-Agent Sparse Training (MAST) framework aimed at simultaneously enhancing the reliability of learning targets and the rationality of sample distribution to improve value learning in sparse models. Specifically, MAST incorporates the Soft Mellowmax Operator with a hybrid TD-($\\lambda$) schema to establish dependable learning targets. Additionally, it employs a dual replay buffer mechanism to enhance the distribution of training samples. Building upon these aspects, MAST utilizes gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks. Our comprehensive experimental investigation across various value-based MARL algorithms on multiple benchmarks demonstrates, for the first time, significant reductions in redundancy of up to $20\\times$ in Floating Point Operations (FLOPs) for both training and inference, with less than 3% performance degradation.'}",https://openreview.net{'value': '/pdf/bf8bb00ab8e48a246aea7bd4371261f2f92f54dd.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=GnaFrZRHPf,{'value': 'Adaptive Preference Scaling for Reinforcement Learning with Human Feedback'},Ilgee Hong; Zichong Li; Alexander Bukharin; Yixiao Li; Haoming Jiang; Tianbao Yang; Tuo Zhao,~Ilgee_Hong1; ~Zichong_Li2; ~Alexander_Bukharin1; ~Yixiao_Li2; ~Haoming_Jiang1; ~Tianbao_Yang1; ~Tuo_Zhao2,"{'value': ['Reinforcement Learning from Human Feedback', 'Large Language Models', 'Alignment']}","{'value': 'Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.'}",https://openreview.net{'value': '/pdf/eaa267a47dd2c488be1aad5e49a25710d060880a.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=GmdGEF8xxU,{'value': 'What Is Missing For Graph Homophily? Disentangling Graph Homophily For Graph Neural Networks'},Yilun Zheng; Sitao Luan; Lihui Chen,~Yilun_Zheng1; ~Sitao_Luan1; ~Lihui_Chen1,"{'value': ['Graph Neural Networks', 'Graph Homophily']}","{'value': 'Graph homophily refers to the phenomenon that connected nodes tend to share similar characteristics. Understanding this concept and its related metrics is crucial for designing effective Graph Neural Networks (GNNs). The most widely used homophily metrics, such as edge or node homophily, quantify such ""similarity"" as label consistency across the graph topology. These metrics are believed to be able to reflect the performance of GNNs, especially on node-level tasks. However, many recent studies have empirically demonstrated that the performance of GNNs does not always align with homophily metrics, and how homophily influences GNNs still remains unclear and controversial. Then, a crucial question arises: What is missing in our current understanding of homophily? To figure out the missing part, in this paper, we disentangle the graph homophily into three aspects: label, structural, and feature homophily, which are derived from the three basic elements of graph data. We argue that the synergy of the three homophily can provide a more comprehensive understanding of GNN performance. Our new proposed structural and feature homophily consider the neighborhood consistency and feature dependencies among nodes, addressing the previously overlooked structural and feature aspects in graph homophily. To investigate their synergy, we propose a Contextual Stochastic Block Model with three types of Homophily (CSBM-3H), where the topology and feature generation are controlled by the three metrics. Based on the theoretical analysis of CSBM-3H, we derive a new composite metric, named Tri-Hom, that considers all three aspects and overcomes the limitations of conventional homophily metrics. The theoretical conclusions and the effectiveness of Tri-Hom have been verified through synthetic experiments on CSBM-3H. In addition, we conduct experiments on $31$ real-world benchmark datasets and calculate the correlations between homophily metrics and model performance. Tri-Hom has significantly higher correlation values than $17$ existing metrics that only focus on a single homophily aspect, demonstrating its superiority and the importance of homophily synergy. Our code is available at https://github.com/zylMozart/Disentangle_GraphHom.'}",https://openreview.net{'value': '/pdf/b8ee864ad9e12045d02d7acac167f412ef23c62e.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Glt37xoU7e,{'value': 'Omnigrasp: Grasping Diverse Objects with Simulated Humanoids'},Zhengyi Luo; Jinkun Cao; Sammy Christen; Alexander Winkler; Kris M. Kitani; Weipeng Xu,~Zhengyi_Luo1; ~Jinkun_Cao1; ~Sammy_Christen1; ~Alexander_Winkler1; ~Kris_M._Kitani1; ~Weipeng_Xu1,"{'value': ['Physics Simulation', 'Humanoid Control', 'Dexterous Manipulation']}","{'value': ""We present a method for controlling a simulated humanoid to grasp an object and move it to follow an object's trajectory. Due to the challenges in controlling a humanoid with dexterous hands, prior methods often use a disembodied hand and only consider vertical lifts or short trajectories. This limited scope hampers their applicability for object manipulation required for animation and simulation. To close this gap, we learn a controller that can pick up a large number (>1200) of objects and carry them to follow randomly generated trajectories. Our key insight is to leverage a humanoid motion representation that provides human-like motor skills and significantly speeds up training. Using only simplistic reward, state, and object representations, our method shows favorable scalability on diverse objects and trajectories. For training, we do not need a dataset of paired full-body motion and object trajectories. At test time, we only require the object mesh and desired trajectories for grasping and transporting. To demonstrate the capabilities of our method, we show state-of-the-art success rates in following object trajectories and generalizing to unseen objects. Code and models will be released.""}",https://openreview.net{'value': '/pdf/08974bf790f4c1e69d0de5e867da7b9a5b9e0e44.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=GkJbXpd3wM,{'value': 'Active Set Ordering'},Quoc Phong Nguyen; Sunil Gupta; Svetha Venkatesh; Bryan Kian Hsiang Low; Patrick Jaillet,~Quoc_Phong_Nguyen2; ~Sunil_Gupta2; ~Svetha_Venkatesh1; ~Bryan_Kian_Hsiang_Low1; ~Patrick_Jaillet1,"{'value': ['active learning', 'Bayesian optimization', 'top-k set', 'contour line']}","{'value': 'In this paper, we formalize the active set ordering problem, which involves actively discovering a set of inputs based on their orderings determined by expensive evaluations of a blackbox function. We then propose the mean prediction (MP) algorithm and theoretically analyze it  in terms of the regret of predicted pairwise orderings between inputs. Notably, as a special case of this framework, we can cast Bayesian optimization as an active set ordering problem by recognizing that maximizers can be identified solely by comparison rather than by precisely estimating the function evaluations. As a result, we are able to construct the popular Gaussian process upper confidence bound (GP-UCB) algorithm through the lens of ordering with several nuanced insights. We empirically validate the performance of our proposed solution using various synthetic functions and real-world datasets.'}",https://openreview.net{'value': '/pdf/8b25c658a7b28be435809600893ab5503ba1508d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=GRmQjLzaPM,{'value': 'BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction'},Zikang Zhou; Haibo HU; Xinhong Chen; Jianping Wang; Nan Guan; Kui Wu; Yung-Hui Li; Yu-Kai Huang; Chun Jason Xue,~Zikang_Zhou1; ~Haibo_HU3; ~Xinhong_Chen3; ~Jianping_Wang1; ~Nan_Guan1; ~Kui_Wu1; ~Yung-Hui_Li3; ~Yu-Kai_Huang3; ~Chun_Jason_Xue1,"{'value': ['Multi-Agent Systems', 'Transformers', 'Generative Models', 'Autonomous Driving']}","{'value': 'Simulating realistic behaviors of traffic agents is pivotal for efficiently validating the safety of autonomous driving systems. Existing data-driven simulators primarily use an encoder-decoder architecture to encode the historical trajectories before decoding the future. However, the heterogeneity between encoders and decoders complicates the models, and the manual separation of historical and future trajectories leads to low data utilization. Given these limitations, we propose BehaviorGPT, a homogeneous and fully autoregressive Transformer designed to simulate the sequential behavior of multiple agents. Crucially, our approach discards the traditional separation between ""history"" and ""future"" by modeling each time step as the ""current"" one for motion generation, leading to a simpler, more parameter- and data-efficient agent simulator. We further introduce the Next-Patch Prediction Paradigm (NP3) to mitigate the negative effects of autoregressive modeling, in which models are trained to reason at the patch level of trajectories and capture long-range spatial-temporal interactions. Despite having merely 3M model parameters, BehaviorGPT won first place in the 2024 Waymo Open Sim Agents Challenge with a realism score of 0.7473 and a minADE score of 1.4147, demonstrating its exceptional performance in traffic agent simulation.'}",https://openreview.net{'value': '/pdf/8a93c63c8997805fca102b403710312eae5c929f.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=FmNoFIImZG,{'value': 'TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models'},Andrei Margeloiu; Xiangjian Jiang; Nikola Simidjievski; Mateja Jamnik,~Andrei_Margeloiu1; ~Xiangjian_Jiang1; ~Nikola_Simidjievski1; ~Mateja_Jamnik1,"{'value': ['tabular data', 'data augmentation', 'synthetic data generation', 'energy based model']}","{'value': 'Data collection is often difficult in critical fields such as medicine, physics, and chemistry, yielding typically only small tabular datasets. However, classification methods tend to struggle with these small datasets, leading to poor predictive performance. Increasing the training set with additional synthetic data, similar to data augmentation in images, is commonly believed to improve downstream tabular classification performance. However, current tabular generative methods that learn either the joint distribution $ p(\\mathbf{x}, y) $ or the class-conditional distribution $ p(\\mathbf{x} \\mid y) $ often overfit on small datasets, resulting in poor-quality synthetic data, usually worsening classification performance compared to using real data alone. To solve these challenges, we introduce TabEBM, a novel class-conditional generative method using Energy-Based Models (EBMs). Unlike existing tabular methods that use a shared model to approximate all class-conditional densities, our key innovation is to create distinct EBM generative models for each class, each modelling its class-specific data distribution individually. This approach creates robust energy landscapes, even in ambiguous class distributions. Our experiments show that TabEBM generates synthetic data with higher quality and better statistical fidelity than existing methods. When used for data augmentation, our synthetic data consistently leads to improved classification performance across diverse datasets of various sizes, especially small ones. Code is available at https://github.com/andreimargeloiu/TabEBM.'}",https://openreview.net{'value': '/pdf/25a6e595799705e000238621240dbbb44f3cf7d7.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=FTpKGuxEfy,{'value': 'Vision Foundation Model Enables Generalizable Object Pose Estimation'},Kai Chen; Yiyao Ma; Xingyu Lin; Stephen James; Jianshu Zhou; Yun-Hui Liu; Pieter Abbeel; Qi Dou,~Kai_Chen9; ~Yiyao_Ma1; ~Xingyu_Lin1; ~Stephen_James1; ~Jianshu_Zhou1; ~Yun-Hui_Liu1; ~Pieter_Abbeel2; ~Qi_Dou2,"{'value': ['Object Pose Estimation', 'Vision Foundation Model']}","{'value': 'Object pose estimation plays a crucial role in robotic manipulation, however, its practical applicability still suffers from limited generalizability. This paper addresses the challenge of generalizable object pose estimation, particularly focusing on category-level object pose estimation for unseen object categories. Current methods either require impractical instance-level training or are confined to predefined categories, limiting their applicability. We propose VFM-6D, a novel framework that explores harnessing existing vision and language models, to elaborate object pose estimation into two stages: category-level object viewpoint estimation and object coordinate map estimation. Based on the two-stage framework, we introduce a 2D-to-3D feature lifting module and a shape-matching module, both of which leverage pre-trained vision foundation models to improve object representation and matching accuracy. VFM-6D is trained on cost-effective synthetic data and exhibits superior generalization capabilities. It can be applied to both instance-level unseen object pose estimation and category-level object pose estimation for novel categories. Evaluations on benchmark datasets demonstrate the effectiveness and versatility of VFM-6D in various real-world scenarios.'}",https://openreview.net{'value': '/pdf/361ff94a1b419acf5dbfca68f2a1f1362ffc8472.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=FSgwgQXTxo,{'value': 'Reasoning Multi-Agent Behavioral Topology for Interactive Autonomous Driving'},Haochen Liu; Li Chen; Yu Qiao; Chen Lv; Hongyang Li,~Haochen_Liu2; ~Li_Chen15; ~Yu_Qiao1; ~Chen_Lv1; ~Hongyang_Li1,"{'value': ['Trajectory Prediction', 'Autonomous Driving']}","{'value': 'Autonomous driving system aims for safe and social-consistent driving through the behavioral integration among interactive agents. However, challenges remain due to multi-agent scene uncertainty and heterogeneous interaction. Current dense and sparse behavioral representations struggle with inefficiency and inconsistency in multi-agent modeling, leading to instability of collective behavioral patterns when integrating prediction and planning (IPP). To address this, we initiate a topological formation that serves as a compliant behavioral foreground to guide downstream trajectory generations. Specifically, we introduce Behavioral Topology (BeTop), a pivotal topological formulation that explicitly represents the consensual behavioral pattern among multi-agent future. BeTop is derived from braid theory to distill compliant interactive topology from multi-agent future trajectories. A synergistic learning framework (BeTopNet) supervised by BeTop facilitates the consistency of behavior prediction and planning within the predicted topology priors. Through imitative contingency learning, BeTop also effectively manages behavioral uncertainty for prediction and planning. Extensive verification on large-scale real-world datasets, including nuPlan and WOMD, demonstrates that BeTop achieves state-of-the-art performance in both prediction and planning tasks. Further validations on the proposed interactive scenario benchmark showcase planning compliance in interactive cases. Code and model is available at https://github.com/OpenDriveLab/BeTop.'}",https://openreview.net{'value': '/pdf/9e8beb8037d5b6dd0ca253feefa7fad799da0544.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=FOTMgW8w5t,{'value': 'Using Surrogates in Covariate-adjusted Response-adaptive Randomization Experiments with Delayed Outcomes'},Lei Shi; Waverly Wei; Jingshen Wang,~Lei_Shi12; ~Waverly_Wei1; ~Jingshen_Wang1,"{'value': ['Covariate-adjusted response-adaptive randomization design', 'Response-adaptive randomization design', 'Surrogate biomarker', 'Causal inference']}","{'value': 'Covariate-adjusted response-adaptive randomization (CARA) designs are gaining increasing attention. These designs combine the advantages of randomized experiments with the ability to adaptively revise treatment allocations based on data collected across multiple stages, enhancing estimation efficiency. Yet, CARA designs often assume that primary outcomes are immediately observable, which is not the case in many clinical scenarios where there is a delay in observing primary outcomes. This assumption can lead to significant missingness and inefficient estimation of treatment effects. To tackle this practical challenge, we propose a CARA experimental strategy integrating delayed primary outcomes with immediately observed surrogate outcomes. Surrogate outcomes are intermediate clinical outcomes that are predictive or correlated with the primary outcome of interest. Our design goal is to improve the estimation efficiency of the average treatment effect (ATE) of the primary outcome utilizing surrogate outcomes. From a methodological perspective, our approach offers two benefits: First, we accommodate arm and covariates-dependent delay mechanisms without imposing any parametric modeling assumptions on the distribution of outcomes. Second, when primary outcomes are not fully observed, surrogate outcomes can guide the adaptive treatment allocation rule. From a theoretical standpoint, we prove the semiparametric efficiency bound of estimating ATE under delayed primary outcomes while incorporating surrogate outcomes. We show that the ATE estimator under our proposed design strategy attains this semiparametric efficiency bound and achieves asymptotic normality. Through theoretical investigations and a synthetic HIV study, we show that our design is more efficient than the design without incorporating any surrogate information.'}",https://openreview.net{'value': '/pdf/1ebaf2d1a8eb26b80a2bff8e1c376ed8cda84456.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=FNtsZLwkGr,{'value': 'Pruning neural network models for gene regulatory dynamics using data and domain knowledge'},Intekhab Hossain; Jonas Fischer; Rebekka Burkholz; John Quackenbush,~Intekhab_Hossain1; ~Jonas_Fischer1; ~Rebekka_Burkholz1; ~John_Quackenbush1,"{'value': ['neural network pruning', 'sparsification', 'domain knowledge', 'gene regulation']}","{'value': ""The practical utility of machine learning models in the sciences often hinges on their interpretability. It is common to assess a model's merit for scientific discovery, and thus novel insights, by how well it aligns with already available domain knowledge - a dimension that is currently largely disregarded in the comparison of neural network models. While pruning can simplify deep neural network architectures and excels in identifying sparse models, as we show in the context of gene regulatory network inference, state-of-the-art techniques struggle with biologically meaningful structure learning. To address this issue, we propose DASH, a generalizable framework that guides network pruning by using domain-specific structural information in model fitting and leads to sparser, better interpretable models that are more robust to noise. Using both synthetic data with ground truth information, as well as real-world gene expression data, we show that DASH, using knowledge about gene interaction partners within the putative regulatory network, outperforms general pruning methods by a large margin and yields deeper insights into the biological systems being studied.""}",https://openreview.net{'value': '/pdf/ddc15b67f3287ea82df3c04444a5546a9a0b4af5.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=FFW6rPz48Z,{'value': 'Analysing Multi-Task Regression via Random Matrix Theory with Application to Time Series Forecasting'},Romain Ilbert; Malik Tiomoko; Cosme Louart; Ambroise Odonnat; Vasilii Feofanov; Themis Palpanas; Ievgen Redko,~Romain_Ilbert1; ~Malik_Tiomoko1; ~Cosme_Louart1; ~Ambroise_Odonnat1; ~Vasilii_Feofanov1; ~Themis_Palpanas1; ~Ievgen_Redko2,{'value': ['Random Matrix Theory ; Optimization ; Regularization ; Multi-task regression ; Multi-task learning ; Multivariate Time Series Forecasting']},"{'value': 'In this paper, we introduce a novel theoretical framework for multi-task regression, applying random matrix theory to provide precise performance estimations, under high-dimensional, non-Gaussian data distributions. We formulate a multi-task optimization problem as a regularization technique to enable single-task models to leverage multi-task learning information. We derive a closed-form solution for multi-task optimization in the context of linear models. Our analysis provides valuable insights by linking the multi-task learning performance to various model statistics such as raw data covariances, signal-generating hyperplanes, noise levels, as well as the size and number of datasets. We finally propose a consistent estimation of training and testing errors, thereby offering a robust foundation for hyperparameter optimization in multi-task regression scenarios. Experimental validations on both synthetic and real-world datasets in regression and multivariate time series forecasting demonstrate improvements on univariate models, incorporating our method into the training loss and thus leveraging multivariate information.'}",https://openreview.net{'value': '/pdf/ff5c8afaf7b48ba26b5ac93b76f334af498c83a9.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=FFJFGx78OK,{'value': 'Consistency Diffusion Bridge Models'},Guande He; Kaiwen Zheng; Jianfei Chen; Fan Bao; Jun Zhu,~Guande_He1; ~Kaiwen_Zheng2; ~Jianfei_Chen1; ~Fan_Bao1; ~Jun_Zhu2,"{'value': ['Diffusion Bridges', 'Consistency Models', 'Image Translation']}","{'value': ""Diffusion models (DMs) have become the dominant paradigm of generative modeling in a variety of domains by learning stochastic processes from noise to data. Recently, diffusion denoising bridge models (DDBMs), a new formulation of generative modeling that builds stochastic processes between fixed data endpoints based on a reference diffusion process, have achieved empirical success across tasks with coupled data distribution, such as image-to-image translation. However, DDBM's sampling process typically requires hundreds of network evaluations to achieve decent performance, which may impede their practical deployment due to high computational demands. In this work, inspired by the recent advance of consistency models in DMs, we tackle this problem by learning the consistency function of the probability-flow ordinary differential equation (PF-ODE) of DDBMs, which directly predicts the solution at a starting step given any point on the ODE trajectory. Based on a dedicated general-form ODE solver, we propose two paradigms: consistency bridge distillation and consistency bridge training, which is flexible to apply on DDBMs with broad design choices. Experimental results show that our proposed method could sample $4\\times$ to $50\\times$ faster than the base DDBM and produce better visual quality given the same step in various tasks with pixel resolution ranging from $64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such as semantic interpolation in the data space.""}",https://openreview.net{'value': '/pdf/41f74c0c05c65351ac913cf79d5ad58db96b1d6c.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=FCsEvaMorw,{'value': 'Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts'},Mikayel Samvelyan; Sharath Chandra Raparthy; Andrei Lupu; Eric Hambro; Aram H. Markosyan; Manish Bhatt; Yuning Mao; Minqi Jiang; Jack Parker-Holder; Jakob Nicolaus Foerster; Tim Rocktäschel; Roberta Raileanu,~Mikayel_Samvelyan1; ~Sharath_Chandra_Raparthy3; ~Andrei_Lupu1; ~Eric_Hambro1; ~Aram_H._Markosyan1; ~Manish_Bhatt1; ~Yuning_Mao1; ~Minqi_Jiang1; ~Jack_Parker-Holder1; ~Jakob_Nicolaus_Foerster1; ~Tim_Rocktäschel1; ~Roberta_Raileanu2,"{'value': ['open-endedness', 'adversarial robustness', 'safety']}","{'value': 'As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.'}",https://openreview.net{'value': '/pdf/850cb40885235200144ad98b8e922426d0f06b88.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=FBLJIfW64D,{'value': 'Dimension-free deterministic equivalents and scaling laws for random feature regression'},Leonardo Defilippis; Bruno Loureiro; Theodor Misiakiewicz,~Leonardo_Defilippis1; ~Bruno_Loureiro1; ~Theodor_Misiakiewicz1,"{'value': ['random features', 'deterministic equivalents', 'error rates', 'random matrix theory', 'scaling laws']}","{'value': 'In this work we investigate the generalization performance of random feature ridge regression (RFRR). Our main contribution is a general deterministic equivalent for the test error of RFRR. Specifically, under a certain concentration property, we show that the test error is well approximated by a closed-form expression that only depends on the feature map eigenvalues. Notably, our approximation guarantee is non-asymptotic, multiplicative, and independent of the feature map dimension---allowing for infinite-dimensional features. We expect this deterministic equivalent to hold broadly beyond our theoretical analysis, and we empirically validate its predictions on various real and synthetic datasets. As an application, we derive sharp excess error rates under standard power-law assumptions of the spectrum and target decay. In particular, we provide a tight result for the smallest number of features achieving optimal minimax error rate.'}",https://openreview.net{'value': '/pdf/d05b8aff77c1d04cb4e154ef105afb96c9204104.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=EQZlEfjrkV,{'value': 'On the Parameter Identifiability of Partially Observed Linear Causal Models'},Xinshuai Dong; Ignavier Ng; Biwei Huang; Yuewen Sun; Songyao Jin; Roberto Legaspi; Peter Spirtes; Kun Zhang,~Xinshuai_Dong1; ~Ignavier_Ng1; ~Biwei_Huang1; ~Yuewen_Sun1; ~Songyao_Jin1; ~Roberto_Legaspi1; ~Peter_Spirtes1; ~Kun_Zhang1,"{'value': ['Linear Causal Model', 'Parameter Identification']}","{'value': 'Linear causal models are important tools for modeling causal dependencies and yet in practice, only a subset of the variables can be observed. In this paper,  we examine the parameter identifiability of these models by investigating whether the edge coefficients can be recovered given the causal structure and partially observed data. Our setting is more general than that of prior research—we allow all variables, including both observed and latent ones, to be flexibly related, and we consider the coefficients of all edges, whereas most existing works focus only on the edges between observed variables. Theoretically, we identify three types of indeterminacy for the parameters in partially observed linear causal models. We then provide graphical conditions that are sufficient for all parameters to be identifiable and show that some of them are provably necessary. Methodologically, we propose a novel likelihood-based parameter estimation method that addresses the variance indeterminacy of latent variables in a specific way and can asymptotically recover the underlying parameters up to trivial indeterminacy. Empirical studies on both synthetic and real-world datasets validate our identifiability theory and the effectiveness of the proposed method in the finite-sample regime.'}",https://openreview.net{'value': '/pdf/1772a55f72797ee372077168d075d59ba3350c7a.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=EKdk4vxKO4,{'value': 'MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making'},Yubin Kim; Chanwoo Park; Hyewon Jeong; Yik Siu Chan; Xuhai Xu; Daniel McDuff; Hyeonhoon Lee; Marzyeh Ghassemi; Cynthia Breazeal; Hae Won Park,~Yubin_Kim2; ~Chanwoo_Park2; ~Hyewon_Jeong1; ~Yik_Siu_Chan1; ~Xuhai_Xu1; ~Daniel_McDuff1; ~Hyeonhoon_Lee1; ~Marzyeh_Ghassemi2; ~Cynthia_Breazeal1; ~Hae_Won_Park1,"{'value': ['Medical Decision Making', 'Multi-Agent Collaboration']}","{'value': ""Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named **M**edical **D**ecision-making **Agents** (**MDAgents**) that helps to address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, a simple emulation inspired by the way real-world medical decision-making processes are adapted to tasks of different complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and clinical diagnosis benchmarks, including a comparison of\nLLMs’ medical complexity classification against human physicians. MDAgents achieved the **best performance in seven out of ten** benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant **improvement of up to 4.2\\%** ($p$ < 0.05) compared to previous methods' best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy **improvement of 11.8\\%**. Our code can be found at https://github.com/mitmedialab/MDAgents.""}",https://openreview.net{'value': '/pdf/9993edbaf6679577c07aeae6b39fe0a546abaca1.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=E4ILjwzdEA,{'value': 'Length Optimization in Conformal Prediction'},Shayan Kiyani; George J. Pappas; Hamed Hassani,~Shayan_Kiyani2; ~George_J._Pappas1; ~Hamed_Hassani2,"{'value': ['Conformal prediction', 'Conditional coverage', 'Prediction set size', 'Uncertainty quantification']}","{'value': 'Conditional validity and length efficiency are two crucial aspects of conformal prediction (CP). Conditional validity ensures accurate uncertainty quantification for data subpopulations, while proper length efficiency ensures that the prediction sets remain informative. Despite significant efforts to address each of these issues individually, a principled framework that reconciles these two objectives has been missing in the CP literature. In this paper, we develop Conformal Prediction with Length-Optimization (CPL) - a novel and practical framework that constructs prediction sets with (near-) optimal length while ensuring conditional validity under various classes of covariate shifts, including the key cases of marginal and group-conditional coverage. In the infinite sample regime, we provide strong duality results which indicate that CPL achieves conditional validity and length optimality. In the finite sample regime, we show that CPL constructs conditionally valid prediction sets. Our extensive empirical evaluations demonstrate the superior prediction set size performance of CPL compared to state-of-the-art methods across diverse real-world and synthetic datasets in classification, regression, and large language model-based multiple choice question answering. An Implementation of our algorithm can be accessed at the following link: https://github.com/shayankiyani98/CP.'}",https://openreview.net{'value': '/pdf/5427bae8d8296fbab1a0dd10970cd5980f9baa0b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=E2JCQyYu0E,{'value': 'Multi-hypotheses Conditioned Point Cloud Diffusion for 3D Human Reconstruction from Occluded Images'},Donghwan Kim; Tae-Kyun Kim,~Donghwan_Kim6; ~Tae-Kyun_Kim2,"{'value': ['human reconstruction', 'point cloud diffusion', 'multi-hypothesis']}","{'value': '3D human shape reconstruction under severe occlusion due to human-object or human-human interaction is a challenging problem. While implicit function methods capture detailed clothed shapes, they require aligned shape priors and or are weak at inpainting occluded regions given an image input. Parametric models i.e. SMPL, instead offer whole body shapes, however, are often misaligned with images. In this work, we propose a novel pipeline composed of a probabilistic SMPL model and point cloud diffusion for pixel-aligned detailed 3D human reconstruction under occlusion. Multiple hypotheses generated by the probabilistic SMPL method are conditioned via continuous 3D shape representations. Point cloud diffusion refines the distribution of 3D points fitted to both the multi-hypothesis shape condition and pixel-aligned image features, offering detailed clothed shapes and inpainting occluded parts of human bodies. In the experiments using the CAPE, MultiHuman and Hi4D datasets, the proposed method outperforms various SOTA methods based on SMPL, implicit functions, point cloud diffusion, and their combined, under synthetic and real occlusions. Our code is publicly available at https://donghwankim0101.github.io/projects/mhcdiff.'}",https://openreview.net{'value': '/pdf/2b5178c90c8265ce4f68b5284777766f7bf6fddf.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=DupvYqqlAG,{'value': 'Spectral Learning of Shared Dynamics Between Generalized-Linear Processes'},Lucine L Oganesian; Omid G. Sani; Maryam Shanechi,~Lucine_L_Oganesian1; ~Omid_G._Sani1; ~Maryam_Shanechi1,"{'value': ['state space models', 'subspace identification', 'dynamical systems', 'neural coding', 'generalized-linear models']}","{'value': ""Generalized-linear dynamical models (GLDMs) remain a widely-used framework within neuroscience for modeling time-series data, such as neural spiking activity or categorical decision outcomes. Whereas the standard usage of GLDMs is to model a single data source, certain applications require jointly modeling two generalized-linear time-series sources while also dissociating their shared and private dynamics. Most existing GLDM variants and their associated learning algorithms do not support this capability. Here we address this challenge by developing a multi-step analytical subspace identification algorithm for learning a GLDM that explicitly models shared vs. private dynamics within two generalized-linear time-series. In simulations, we demonstrate our algorithm's ability to dissociate and model the dynamics within two time-series sources while being agnostic to their respective observation distributions. In neural data, we consider two specific applications of our algorithm for modeling discrete population spiking activity with respect to a secondary time-series. In both synthetic and real data, GLDMs learned with our algorithm more accurately decoded one time-series from the other using lower-dimensional latent states, as compared to models identified using existing GLDM learning algorithms.""}",https://openreview.net{'value': '/pdf/b3d55867c578a5270a152b2f0d3b6a4728fbfb10.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=DlYNGpCuwa,{'value': 'Aligning LLM Agents by Learning Latent Preference from User Edits'},Ge Gao; Alexey Taymanov; Eduardo Salinas; Paul Mineiro; Dipendra Misra,~Ge_Gao1; ~Alexey_Taymanov1; ~Eduardo_Salinas1; ~Paul_Mineiro1; ~Dipendra_Misra1,"{'value': ['NLP', 'LLM', 'preference learning', 'user feedback', 'user edits']}","{'value': ""We study interactive learning of language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks, CIPHER outperforms baselines by achieving the lowest edit distance cost. Meanwhile, CIPHER has a lower computational expense, as using learned preference results in a shorter prompt than directly using user edits. Our further analysis reports that the user preference learned by CIPHER shows significant similarity to the ground truth latent preference.""}",https://openreview.net{'value': '/pdf/55a265ef475e9a6b90a137936e222ac4be7a4a0c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=DdKdr4kqxh,{'value': 'Identifying Spatio-Temporal Drivers of Extreme Events'},Mohamad Hakam Shams Eddin; Juergen Gall,~Mohamad_Hakam_Shams_Eddin1; ~Juergen_Gall1,"{'value': ['anomaly detection', 'weakly supervised learning', 'Earth science', 'climate science', 'remote sensing', 'deep learning']}","{'value': 'The spatio-temporal relations of impacts of extreme events and their drivers in climate data are not fully understood and there is a need of machine learning approaches to identify such spatio-temporal relations from data. The task, however, is very challenging since there are time delays between extremes and their drivers, and the spatial response of such drivers is inhomogeneous. In this work, we propose a first approach and benchmarks to tackle this challenge. Our approach is trained end-to-end to predict spatio-temporally extremes and spatio-temporally drivers in the physical input variables jointly. By enforcing the network to predict extremes from spatio-temporal binary masks of identified drivers, the network successfully identifies drivers that are correlated with extremes. We evaluate our approach on three newly created synthetic benchmarks, where two of them are based on remote sensing or reanalysis climate data, and on two real-world reanalysis datasets. The source code and datasets are publicly available at the project page https://hakamshams.github.io/IDE.'}",https://openreview.net{'value': '/pdf/de07db4e16c50434ebcb77a7bc5885fc4bb45cdc.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=DUHX779C5q,{'value': 'Language Grounded Multi-agent Reinforcement Learning with Human-interpretable Communication'},Huao Li; Hossein Nourkhiz Mahjoub; Behdad Chalaki; Vaishnav Tadiparthi; Kwonjoon Lee; Ehsan Moradi Pari; Charles Michael Lewis; Katia P. Sycara,~Huao_Li1; ~Hossein_Nourkhiz_Mahjoub1; ~Behdad_Chalaki1; ~Vaishnav_Tadiparthi1; ~Kwonjoon_Lee1; ~Ehsan_Moradi_Pari1; ~Charles_Michael_Lewis1; ~Katia_P._Sycara1,"{'value': ['Multi-Agent Reinforcement Learning', 'Emergent Communication', 'Ad-hoc Teamwork', 'Large Language Models']}","{'value': 'Multi-Agent Reinforcement Learning (MARL) methods have shown promise in enabling agents to learn a shared communication protocol from scratch and accomplish challenging team tasks. However, the learned language is usually not interpretable to humans or other agents not co-trained together, limiting its applicability in ad-hoc teamwork scenarios. In this work, we propose a novel computational pipeline that aligns the communication space between MARL agents with an embedding space of human natural language by grounding agent communications on synthetic data generated by embodied Large Language Models (LLMs) in interactive teamwork scenarios. Our results demonstrate that introducing language grounding not only maintains task performance but also accelerates the emergence of communication. Furthermore, the learned communication protocols exhibit zero-shot generalization capabilities in ad-hoc teamwork scenarios with unseen teammates and novel task states. This work presents a significant step toward enabling effective communication and collaboration between artificial agents and humans in real-world teamwork settings.'}",https://openreview.net{'value': '/pdf/6664b584d62b376a0ad3f0a353ad58f8ef896b2e.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=DRC9pZwBwR,{'value': 'Recursive Introspection: Teaching Language Model Agents How to Self-Improve'},Yuxiao Qu; Tianjun Zhang; Naman Garg; Aviral Kumar,~Yuxiao_Qu1; ~Tianjun_Zhang1; ~Naman_Garg1; ~Aviral_Kumar2,"{'value': ['Large Language Model', 'Reinforcement Learning', 'Self-Improvement']}","{'value': 'A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially. In this paper, we develop $\\textbf{RISE:}$ $\\textbf{R}$ecursive $\\textbf{I}$ntro$\\textbf{S}$p$\\textbf{E}$ction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation and offline reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models, without disrupting one-turn abilities as a result of expressing more complex distributions.'}",https://openreview.net{'value': '/pdf/f50ad94a939b176eb3bdf712e863034fc3076193.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=D6nlm2AYHi,{'value': 'Learning Distinguishable Trajectory Representation with Contrastive Loss'},Tianxu Li; Kun Zhu; Juan Li; Yang Zhang,~Tianxu_Li1; ~Kun_Zhu1; ~Juan_Li4; ~Yang_Zhang29,"{'value': ['Multi-Agent Reinforcement Learning', 'multi-agent exploration', 'trajectory representation', 'contrastive learning', 'multi-agent diversity']}","{'value': 'Policy network parameter sharing is a commonly used technique in advanced deep multi-agent reinforcement learning (MARL) algorithms to improve learning efficiency by reducing the number of policy parameters and sharing experiences among agents. Nevertheless, agents that share the policy parameters tend to learn similar behaviors. To encourage multi-agent diversity, prior works typically maximize the mutual information between trajectories and agent identities using variational inference. However, this category of methods easily leads to inefficient exploration due to limited trajectory visitations. To resolve this limitation, inspired by the learning of pre-trained models, in this paper, we propose a novel Contrastive Trajectory Representation (CTR) method based on learning distinguishable trajectory representations to encourage multi-agent diversity. Specifically, CTR maps the trajectory of an agent into a latent trajectory representation space by an encoder and an autoregressive model. To achieve the distinguishability among trajectory representations of different agents, we introduce contrastive learning to maximize the mutual information between the trajectory representations and learnable identity representations of different agents. We implement CTR on top of QMIX and evaluate its performance in various cooperative multi-agent tasks. The empirical results demonstrate that our proposed CTR yields significant performance improvement over the state-of-the-art methods.'}",https://openreview.net{'value': '/pdf/489ebd7164382aaf171257ff74f66af7b45da364.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=CeOwahuQic,{'value': 'Can Large Language Model Agents Simulate Human Trust Behavior?'},Chengxing Xie; Canyu Chen; Feiran Jia; Ziyu Ye; Shiyang Lai; Kai Shu; Jindong Gu; Adel Bibi; Ziniu Hu; David Jurgens; James Evans; Philip Torr; Bernard Ghanem; Guohao Li,~Chengxing_Xie2; ~Canyu_Chen1; ~Feiran_Jia1; ~Ziyu_Ye1; ~Shiyang_Lai1; ~Kai_Shu1; ~Jindong_Gu1; ~Adel_Bibi1; ~Ziniu_Hu1; ~David_Jurgens1; ~James_Evans1; ~Philip_Torr1; ~Bernard_Ghanem1; ~Guohao_Li1,"{'value': ['LLM Agent', 'Human Simulation', 'Behavioral Alignment', 'Trust Games']}","{'value': 'Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition,  we probe the biases of agent trust and  differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.'}",https://openreview.net{'value': '/pdf/83af3ec1753188a2e392e049fc2d839ffd53ab72.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=CcmHlE6N6u,{'value': 'LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes'},Zefan Qu; Ke Xu; Gerhard Petrus Hancke; Rynson W. H. Lau,~Zefan_Qu1; ~Ke_Xu5; ~Gerhard_Petrus_Hancke1; ~Rynson_W._H._Lau1,"{'value': ['Deep learning', 'Neural Radiance Field', 'Low light']}","{'value': 'Neural Radiance Fields (NeRFs) have shown remarkable performances in producing novel-view images from high-quality scene images. However, hand-held low-light photography challenges NeRFs as the captured images may simultaneously suffer from low visibility, noise, and camera shakes.\nWhile existing NeRF methods may handle either low light or motion, directly combining them or incorporating additional image-based enhancement methods does not work as these degradation factors are highly coupled.\nWe observe that noise in low-light images is always sharp regardless of camera shakes, which implies an implicit order of these degradation factors within the image formation process.\nThis inspires us to explore such an order to decouple and remove these degradation factors while training the NeRF.\nTo this end, we propose in this paper a novel model, named LuSh-NeRF, which can reconstruct a clean and sharp NeRF from a group of hand-held low-light images.\nThe key idea of LuSh-NeRF is to sequentially model noise and blur in the images via multi-view feature consistency and frequency information of NeRF, respectively.\nSpecifically, LuSh-NeRF includes a novel Scene-Noise Decomposition (SND) module for decoupling the noise from the scene representation and a novel Camera Trajectory Prediction (CTP) module for the estimation of camera motions based on low-frequency scene information.\nTo facilitate training and evaluations, we construct a new dataset containing both synthetic and real images.\nExperiments show that LuSh-NeRF outperforms existing approaches. Our code and dataset can be found here: https://github.com/quzefan/LuSh-NeRF.'}",https://openreview.net{'value': '/pdf/06fe10fb3958bb2d5f2550de529a77924b664df2.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=C62d2nS3KO,{'value': 'Multistep Distillation of Diffusion Models via Moment Matching'},Tim Salimans; Thomas Mensink; Jonathan Heek; Emiel Hoogeboom,~Tim_Salimans1; ~Thomas_Mensink1; ~Jonathan_Heek1; ~Emiel_Hoogeboom1,"{'value': ['generative modeling', 'diffusion', 'distillation']}","{'value': 'We present a new method for making diffusion models faster to sample. The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory. Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching. By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset. We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers.'}",https://openreview.net{'value': '/pdf/f5964a3a798db199e7614693b91434cfaca4f46c.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=Bh0LLUp8OA,{'value': 'Contracting with a Learning Agent'},Guru Guruganesh; Yoav Kolumbus; Jon Schneider; Inbal Talgam-Cohen; Emmanouil-Vasileios Vlatakis-Gkaragkounis; Joshua Ruizhi Wang; S. Matthew Weinberg,~Guru_Guruganesh1; ~Yoav_Kolumbus1; ~Jon_Schneider1; ~Inbal_Talgam-Cohen2; ~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1; ~Joshua_Ruizhi_Wang1; ~S._Matthew_Weinberg1,"{'value': ['Contract Theory', 'Learning', 'No-Regret Learning', 'Mean-Based Learners']}","{'value': 'Real-life contractual relations typically involve repeated interactions between the principal and agent, where, despite theoretical appeal, players rarely use complex dynamic strategies and instead manage uncertainty through learning algorithms.\n\nIn this paper, we initiate the study of repeated contracts with learning agents, focusing on those achieving no-regret outcomes. For the canonical setting where the agent’s actions result in success or failure, we present a simple, optimal solution for the principal: Initially provide a linear contract with scalar $\\alpha > 0$, then switch to a zero-scalar contract. This shift causes the agent to “free-fall” through their action space, yielding non-zero rewards for the principal at zero cost. Interestingly, despite the apparent exploitation, there are instances where our dynamic contract can make \\emph{both} players better off compared to the best static contract. \n\nWe then broaden the scope of our results to general linearly-scaled contracts, and, finally, to the best of our knowledge, we provide the first analysis of optimization against learning agents with uncertainty about the time horizon.'}",https://openreview.net{'value': '/pdf/0d2c03c338f2857c5b56ed45544ca467fed6dfaa.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=BNnZwbZGpm,{'value': 'Provably Faster Algorithms for Bilevel Optimization via Without-Replacement Sampling'},Junyi Li; Heng Huang,~Junyi_Li1; ~Heng_Huang1,"{'value': ['sampling', 'bilevel optimization']}","{'value': 'Bilevel Optimization has experienced significant advancements recently with the introduction of new efficient algorithms. Mirroring the success in single-level optimization, stochastic gradient-based algorithms are widely used in bilevel optimization. However, a common limitation in these algorithms is the presumption of independent sampling, which can lead to increased computational costs due to the unique hyper-gradient structure in bilevel problems. To address this challenge, we study the example-selection strategy for bilevel optimization in this work. More specifically, we introduce a without-replacement sampling based algorithm which achieves a faster convergence rate compared to its counterparts that rely on independent sampling. Beyond the standard bilevel optimization formulation, we extend our discussion to conditional bilevel optimization and also two special cases: minimax and compositional optimization. Finally, we validate our algorithms over both synthetic and real-world applications. Numerical results clearly showcase the superiority of our algorithms.'}",https://openreview.net{'value': '/pdf/02a1d8edd1255179e52012fdd5a11e5b2a4e5acc.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=BJndYScO6o,{'value': 'Model-based Diffusion for Trajectory Optimization'},Chaoyi Pan; Zeji Yi; Guanya Shi; Guannan Qu,~Chaoyi_Pan1; ~Zeji_Yi1; ~Guanya_Shi1; ~Guannan_Qu1,"{'value': ['Diffusion', 'Trajectory Optimization', 'Motion Planning', 'Robotics', 'Sampling-based Control']}","{'value': 'Recent advances in diffusion models have demonstrated their strong capabilities in generating high-fidelity samples from complex distributions through an iterative refinement process. Despite the empirical success of diffusion models in motion planning and control, the model-free nature of these methods does not leverage readily available model information and limits their generalization to new scenarios beyond the training data (e.g., new robots with different dynamics). In this work, we introduce Model-Based Diffusion (MBD), an optimization approach using the diffusion process to solve trajectory optimization (TO) problems without data. The key idea is to explicitly compute the score function by leveraging the model information in TO problems, which is why we refer to our approach as model-based diffusion. Moreover, although MBD does not require external data, it can be naturally integrated with data of diverse qualities to steer the diffusion process. We also reveal that MBD has interesting connections to sampling-based optimization. Empirical evaluations show that MBD outperforms state-of-the-art reinforcement learning and sampling-based TO methods in challenging contact-rich tasks. Additionally, MBD’s ability to integrate with data enhances its versatility and practical applicability, even with imperfect and infeasible data (e.g., partial-state demonstrations for high-dimensional humanoids), beyond the scope of standard diffusion models. Videos and codes are available in the supplementary materials.'}",https://openreview.net{'value': '/pdf/de9314b0cb13ef6263570ff2f4dcc2e0c40b3c12.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=BEiqNQZIky,{'value': 'Efficiently Learning Significant Fourier Feature Pairs for Statistical Independence Testing'},Yixin Ren; Yewei Xia; Hao Zhang; Jihong Guan; Shuigeng Zhou,~Yixin_Ren1; ~Yewei_Xia1; ~Hao_Zhang61; ~Jihong_Guan1; ~Shuigeng_Zhou1,"{'value': ['independence test', 'learnable Fourier feature']}","{'value': ""We propose a novel method to efficiently learn significant Fourier feature pairs for maximizing the power of Hilbert-Schmidt Independence Criterion~(HSIC) based independence tests. We first reinterpret HSIC in the frequency domain, which reveals its limited discriminative power due to the inability to adapt to specific frequency-domain features under the current inflexible configuration. To remedy this shortcoming, we introduce a module of learnable Fourier features, thereby developing a new criterion. We then derive a finite sample estimate of the test power by modeling the behavior of the criterion, thus formulating an optimization objective for significant Fourier feature pairs learning. We show that this optimization objective can be computed in linear time (with respect to the sample size $n$), which ensures fast independence tests. We also prove the convergence property of the optimization objective and establish the consistency of the independence tests. Extensive empirical evaluation on both synthetic and real datasets validates our method's superiority in effectiveness and efficiency, particularly in handling high-dimensional data and dealing with large-scale scenarios.""}",https://openreview.net{'value': '/pdf/dc1dd582e88e396ec283acc538f1c23ec2226c3c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=B7S4jJGlvl,{'value': 'Symbolic Regression with a Learned Concept Library'},Arya Grayeli; Atharva Sehgal; Omar Costilla Reyes; Miles Cranmer; Swarat Chaudhuri,~Arya_Grayeli1; ~Atharva_Sehgal1; ~Omar_Costilla_Reyes1; ~Miles_Cranmer2; ~Swarat_Chaudhuri1,"{'value': ['symbolic regression', 'genetic programming', 'program synthesis', 'generative modelling', 'foundation models']}","{'value': 'We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LaSR, \nuses zero-shot queries to a large language model (LLM) to discover and evolve concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered,  hypotheses are used in a new round of concept abstraction and evolution. We validate LaSR on the Feynman equations, a popular SR benchmark, \nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LASR can be used to discover a new and powerful scaling law for LLMs.'}",https://openreview.net{'value': '/pdf/407edd744beb8d1728086dba69df316654b6febe.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=B2cTLakrhV,{'value': 'Differentiable Structure Learning with Partial Orders'},Taiyu Ban; Lyuzhou Chen; Xiangyu Wang; Xin Wang; Derui Lyu; Huanhuan Chen,~Taiyu_Ban1; ~Lyuzhou_Chen1; ~Xiangyu_Wang7; ~Xin_Wang46; ~Derui_Lyu1; ~Huanhuan_Chen1,"{'value': ['Causal discovery', 'Continuous optimization', 'Differentiable Structure Learning', 'Partial Orders']}","{'value': ""Differentiable structure learning is a novel line of causal discovery research that transforms the combinatorial optimization of structural models into a continuous optimization problem. However, the field has lacked feasible methods to integrate partial order constraints, a critical prior information typically used in real-world scenarios, into the differentiable structure learning framework. The main difficulty lies in adapting these constraints, typically suited for the space of total orderings, to the continuous optimization context of structure learning in the graph space. To bridge this gap, this paper formalizes a set of equivalent constraints that map partial orders onto graph spaces and introduces a plug-and-play module for their efficient application. This module preserves the equivalent effect of partial order constraints in the graph space, backed by theoretical validations of correctness and completeness. It significantly enhances the quality of recovered structures while maintaining good efficiency, which learns better structures using 90\\% fewer samples than the data-based method on a real-world dataset. This result, together with a comprehensive evaluation on synthetic cases, demonstrates our method's ability to effectively improve differentiable structure learning with partial orders.""}",https://openreview.net{'value': '/pdf/fd75d3592f5e7a120079d3917c23b0683fbb55e7.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=AdS3H8SaPi,{'value': 'What does guidance do? A fine-grained analysis in a simple setting'},Muthu Chidambaram; Khashayar Gatmiry; Sitan Chen; Holden Lee; Jianfeng Lu,~Muthu_Chidambaram1; ~Khashayar_Gatmiry1; ~Sitan_Chen1; ~Holden_Lee1; ~Jianfeng_Lu1,"{'value': ['diffusion', 'guidance', 'sampling', 'probability flow ode']}","{'value': 'The use of guidance in diffusion models was originally motivated by the premise that the guidance-modified score is that of the data distribution tilted by a conditional likelihood raised to some power. In this work we clarify this misconception by rigorously proving that guidance fails to sample from the intended tilted distribution. Our main result is to give a fine-grained characterization of the dynamics of guidance in two cases, (1) mixtures of compactly supported distributions and (2) mixtures of Gaussians, which reflect salient properties of guidance that manifest on real-world data. In both cases, we prove that as the guidance parameter increases, the guided model samples more heavily from the boundary of the support of the conditional distribution. We also prove that for any nonzero level of score estimation error, sufficiently large guidance will result in sampling away from the support, theoretically justifying the empirical finding that large guidance results in distorted generations. In addition to verifying these results empirically in synthetic settings, we also show how our theoretical insights can offer useful prescriptions for practical deployment.'}",https://openreview.net{'value': '/pdf/2d9524cbc61edd7e4fb82eb624c6fefd86e277e3.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=ALISPmDPCq,{'value': 'ConStat: Performance-Based Contamination Detection in Large Language Models'},Jasper Dekoninck; Mark Niklas Mueller; Martin Vechev,~Jasper_Dekoninck1; ~Mark_Niklas_Mueller2; ~Martin_Vechev1,"{'value': ['large language models', 'model evaluation', 'contamination detection']}","{'value': 'Public benchmarks play an essential role in the evaluation of large language models.  However, data contamination can lead to inflated performance, rendering them unreliable for model comparison. It is therefore crucial to detect contamination and estimate its impact on measured performance. Unfortunately, existing detection methods can be easily evaded and fail to quantify contamination. To overcome these limitations, we propose a novel definition of *contamination as artificially inflated and non-generalizing benchmark performance* instead of the inclusion of benchmark samples in the training data. This perspective enables us to detect *any* model with inflated performance, i.e., performance that does not generalize to rephrased samples, synthetic samples from the same distribution, or different benchmarks for the same task. Based on this insight, we develop ConStat, a statistical method that reliably detects and quantifies contamination by comparing performance between a primary and reference benchmark relative to a set of reference models. We demonstrate the effectiveness of ConStat in an extensive evaluation of diverse model architectures, benchmarks, and contamination scenarios and find high levels of contamination in multiple popular models including Mistral, Llama, Yi, and the top-3 Open LLM Leaderboard models.'}",https://openreview.net{'value': '/pdf/e40d94db89ba8589fc350fe7ef4ad6072d0f041c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=A3hxp0EeNW,{'value': 'Generative Modelling of Structurally Constrained Graphs'},Manuel Madeira; Clement Vignac; Dorina Thanou; Pascal Frossard,~Manuel_Madeira1; ~Clement_Vignac1; ~Dorina_Thanou1; ~Pascal_Frossard1,"{'value': ['Graph Generative Models', 'Constrained Diffusion']}","{'value': 'Graph diffusion models have emerged as state-of-the-art techniques in graph generation; yet, integrating domain knowledge into these models remains challenging. \nDomain knowledge is particularly important in real-world scenarios, where invalid generated graphs hinder deployment in practical applications.\nUnconstrained and conditioned graph diffusion models fail to guarantee such domain-specific structural properties. \nWe present ConStruct, a novel framework that enables graph diffusion models to incorporate hard constraints on specific properties, such as planarity or acyclicity.\nOur approach ensures that the sampled graphs remain within the domain of graphs that satisfy the specified property throughout the entire trajectory in both the forward and reverse processes. This is achieved by introducing an edge-absorbing noise model and a new projector operator.\nConStruct demonstrates versatility across several structural and edge-deletion invariant constraints and achieves state-of-the-art performance for both synthetic benchmarks and attributed real-world datasets. \nFor example, by incorporating planarity constraints in digital pathology graph datasets, the proposed method outperforms existing baselines, improving data validity by up to 71.1 percentage points.'}",https://openreview.net{'value': '/pdf/9c3bce2103ac76e47aca2a8ca0dd6d75ee549d16.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=A0HSmrwtLH,{'value': 'Testing Semantic Importance via Betting'},Jacopo Teneggi; Jeremias Sulam,~Jacopo_Teneggi1; ~Jeremias_Sulam1,"{'value': ['Explainability', 'Semantic Concepts', 'Sequential Testing', 'Conditional Independence Testing']}","{'value': 'Recent works have extended notions of feature importance to semantic concepts that are inherently interpretable to the users interacting with a black-box predictive model. Yet, precise statistical guarantees such as false positive rate and false discovery rate control are needed to communicate findings transparently, and to avoid unintended consequences in real-world scenarios. In this paper, we formalize the global (i.e., over a population) and local (i.e., for a sample) statistical importance of semantic concepts for the predictions of opaque models by means of conditional independence, which allows for rigorous testing. We use recent ideas of sequential kernelized independence testing to induce a rank of importance across concepts, and we showcase the effectiveness and flexibility of our framework on synthetic datasets as well as on image classification using several vision-language models.'}",https://openreview.net{'value': '/pdf/c9c1b7b4d45cdff0018b6b9b5a490f3e5ca1a95d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=9m87e9Keq1,{'value': 'RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold'},Amrith Setlur; Saurabh Garg; Xinyang Geng; Naman Garg; Virginia Smith; Aviral Kumar,~Amrith_Setlur1; ~Saurabh_Garg3; ~Xinyang_Geng1; ~Naman_Garg1; ~Virginia_Smith1; ~Aviral_Kumar2,"{'value': ['Synthetic data', 'math reasoning', 'reinforcement learning', 'large language models']}","{'value': 'Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this question for math reasoning via an empirical study, followed by building a conceptual understanding of our observations. First, we find that while the typical approach of finetuning a model on synthetic correct or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner itself followed by subsequent fine-tuning on this self-generated data doubles the efficiency of the same synthetic problems. At the same time, training on model-generated positives can amplify various spurious  correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect by a final answer verifier. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or advantage of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by $\\mathbf{8 \\times}$. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits robustness benefits of RL over imitating positive data alone.'}",https://openreview.net{'value': '/pdf/60e93e6e5e8783ca37b6761394fa7fc039451384.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=9jgODkdH0F,{'value': 'Connectivity Shapes Implicit Regularization in Matrix Factorization Models for Matrix Completion'},Zhiwei Bai; Jiajie Zhao; Yaoyu Zhang,~Zhiwei_Bai1; ~Jiajie_Zhao1; ~Yaoyu_Zhang1,{'value': ['matrix completion; implicit regularization; training dynamics; low nuclear norm; low rank;']},"{'value': 'Matrix factorization models have been extensively studied as a valuable test-bed for understanding the implicit biases of overparameterized models. Although both low nuclear norm and low rank regularization have been studied for these models, a unified understanding of when, how, and why they achieve different implicit regularization effects remains elusive. In this work, we systematically investigate the implicit regularization of matrix factorization for solving matrix completion problems. We empirically discover that the connectivity of observed data plays a key role in the implicit bias, with a transition from low nuclear norm to low rank as data shifts from disconnected to connected with increased observations. We identify a hierarchy of intrinsic invariant manifolds in the loss landscape that guide the training trajectory to evolve from low-rank to higher-rank solutions. Based on this finding, we theoretically characterize the training trajectory as following the hierarchical invariant manifold traversal process, generalizing the characterization of Li et al.(2020)  to include the disconnected case. Furthermore, we establish conditions that guarantee minimum nuclear norm, closely aligning with our experimental findings, and we provide a dynamics characterization condition for ensuring minimum rank. Our work reveals the intricate interplay between data connectivity, training dynamics, and implicit regularization in matrix factorization models.'}",https://openreview.net{'value': '/pdf/dff44f7028fa8fce3e403214a527f714f8f3e3b9.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=9Y8zUO11EQ,{'value': 'SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents'},Niels Mündler; Mark Niklas Mueller; Jingxuan He; Martin Vechev,~Niels_Mündler1; ~Mark_Niklas_Mueller2; ~Jingxuan_He1; ~Martin_Vechev1,"{'value': ['language model', 'test generation', 'code agent']}","{'value': 'Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests.  We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent. We release all data and code at https://github.com/logic-star-ai/SWT-Bench.'}",https://openreview.net{'value': '/pdf/729fb4bb01584cf06a81f384cf8961af6ecc0d24.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=9VbGjXLzig,"{'value': 'No ""Zero-Shot"" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance'}",Vishaal Udandarao; Ameya Prabhu; Adhiraj Ghosh; Yash Sharma; Philip Torr; Adel Bibi; Samuel Albanie; Matthias Bethge,~Vishaal_Udandarao1; ~Ameya_Prabhu1; ~Adhiraj_Ghosh2; ~Yash_Sharma1; ~Philip_Torr1; ~Adel_Bibi1; ~Samuel_Albanie2; ~Matthias_Bethge1,"{'value': ['Multimodal Datasets', 'Long-tailed Concept Distribution', 'CLIP Models', 'Diffusion Models', 'Data-Centric ML']}","{'value': 'Web-crawled pretraining datasets underlie the impressive ""zero-shot"" evaluation performance of multimodal models, such as CLIP for classification and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of ""zero-shot"" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during ""zero-shot"" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets?\n\nWe comprehensively investigate this question across 34 models and 5 standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting ""zero-shot"" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream ""zero-shot"" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the Let it Wag! benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to ""zero-shot"" generalization capabilities under large-scale training data and compute paradigms remains to be found.'}",https://openreview.net{'value': '/pdf/b9c1c3dab0429c2476424236a4a272811999a36b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=9O2sVnEHor,{'value': 'Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning'},Raffaele Paolino; Sohir Maskey; Pascal Welke; Gitta Kutyniok,~Raffaele_Paolino1; ~Sohir_Maskey1; ~Pascal_Welke1; ~Gitta_Kutyniok2,"{'value': ['Graph Neural Networks', 'Weisfeiler-Leman (WL) Test', 'Homomorphism Counting', 'Theory and Expressivity in GNNs', 'Cactus Graphs']}","{'value': 'We introduce $r$-loopy Weisfeiler-Leman ($r$-$\\ell$WL), a novel hierarchy of graph isomorphism tests and a corresponding GNN framework, $r$-$\\ell$MPNN, that can count cycles up to length $r{+}2$. Most notably, we show that $r$-$\\ell$WL can count homomorphisms of cactus graphs. This extends 1-WL, which can only count homomorphisms of trees and, in fact, is incomparable to $k$-WL for any fixed $k$. We empirically validate the expressive and counting power of $r$-$\\ell$MPNN on several synthetic datasets and demonstrate the scalability and strong performance on various real-world datasets, particularly on sparse graphs.'}",https://openreview.net{'value': '/pdf/160b0368f27f6ae00575a4abc8d44870237c95f9.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=9FYat8HPpv,{'value': 'SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams'},Kang Chen; Shiyan Chen; Jiyuan Zhang; Baoyue Zhang; Yajing Zheng; Tiejun Huang; Zhaofei Yu,~Kang_Chen9; ~Shiyan_Chen1; ~Jiyuan_Zhang3; ~Baoyue_Zhang1; ~Yajing_Zheng1; ~Tiejun_Huang1; ~Zhaofei_Yu1,"{'value': ['Motion Deblur', 'Spike Camera', 'Self-supervised']}","{'value': 'Reconstructing a sequence of sharp images from the blurry input is crucial for enhancing our insights into the captured scene and poses a significant challenge due to the limited temporal features embedded in the image. Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem. Nonetheless, existing methods fall into the supervised learning paradigm, which suffers from notable performance degradation when applied to real-world scenarios that diverge from the synthetic training data domain. To address these challenges, we propose the first self-supervised framework for the task of spike-guided motion deblurring. Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences. We subsequently develop a self-supervised cascaded framework to alleviate the issues of spike noise and spatial-resolution mismatching encountered in the deblurring model. With knowledge distillation and re-blurring loss, we further design a lightweight deblur network to generate high-quality sequences with brightness and texture consistency with the original input. Quantitative and qualitative experiments conducted on our real-world and synthetic datasets with spikes validate the superior generalization of the proposed framework. Our code, data and trained models are available at \\url{https://github.com/chenkang455/S-SDM}.'}",https://openreview.net{'value': '/pdf/9df7f66462e4533aebee3c88a3dbbf45c92e9ff1.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=8lcW9ltJx9,{'value': 'Any2Policy: Learning Visuomotor Policy with Any-Modality'},Yichen Zhu; Zhicai Ou; Feifei Feng; Jian Tang,~Yichen_Zhu1; ~Zhicai_Ou1; ~Feifei_Feng1; ~Jian_Tang5,"{'value': ['multi-modal', 'robot learning']}","{'value': 'Humans can communicate and observe media with different modalities, such as texts, sounds, and images. For robots to be more generalizable embodied agents, they should be capable of following instructions and perceiving the world with adaptation to diverse modalities. Current robotic learning methodologies often focus on single-modal task specification and observation, thereby limiting their ability to process rich multi-modal information. Addressing this limitation, we present an end-to-end general-purpose multi-modal system named Any-to-Policy Embodied Agents. This system empowers robots to handle tasks using various modalities, whether in combinations like text-image, audio-image, text-point cloud, or in isolation. Our innovative approach involves training a versatile modality network that adapts to various inputs and connects with policy networks for effective control. Because of the lack of existing multi-modal robotics datasets for evaluation, we assembled a comprehensive real-world dataset encompassing 30 robotic tasks. Each task in this dataset is richly annotated across multiple modalities, providing a robust foundation for assessment. We conducted extensive validation of our proposed unified modality embodied agent using several simulation benchmarks, including Franka Kitchen, Meta-World, and Maniskill2, as well as in our real-world settings. Our experiments showcase the promising capability of building embodied agents that can adapt to diverse multi-modal in a unified framework.'}",https://openreview.net{'value': '/pdf/5d47166afae1292bf33c3eca4946a29d4f83a7ae.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=8W5ADJOKcv,{'value': 'Neuc-MDS: Non-Euclidean Multidimensional Scaling Through Bilinear Forms'},Chengyuan Deng; Jie Gao; Kevin Lu; Feng Luo; Hongbin Sun; Cheng Xin,~Chengyuan_Deng1; ~Jie_Gao6; ~Kevin_Lu4; ~Feng_Luo2; ~Hongbin_Sun1; ~Cheng_Xin2,"{'value': ['multidimensional scaling', 'dimension reduction', 'non-Euclidean geometry']}","{'value': ""We introduce \\textbf{N}on-\\textbf{Euc}lidean-\\textbf{MDS} (Neuc-MDS), which extends Multidimensional Scaling (MDS) to generate outputs that can be non-Euclidean and non-metric. The main idea is to generalize the inner product to other symmetric bilinear forms to utilize the negative eigenvalues of dissimiliarity Gram matrices. Neuc-MDS efficiently optimizes the choice of (both positive and negative) eigenvalues of the dissimilarity Gram matrix to reduce STRESS, the sum of squared pairwise error. We provide an in-depth error analysis and proofs of the optimality in minimizing lower bounds of STRESS. We demonstrate Neuc-MDS's ability to address limitations of classical MDS raised by prior research, and test it on various synthetic and real-world datasets in comparison with both linear and non-linear dimension reduction methods.""}",https://openreview.net{'value': '/pdf/4c053a52cbb56c0b19c4284656972cd6c86ae568.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=8APPypS0yN,{'value': 'On the Expressivity and Sample Complexity of Node-Individualized Graph Neural Networks'},Paolo Pellizzoni; Till Hendrik Schulz; Dexiong Chen; Karsten Borgwardt,~Paolo_Pellizzoni1; ~Till_Hendrik_Schulz1; ~Dexiong_Chen1; ~Karsten_Borgwardt2,"{'value': ['Graph neural networks', 'graph learning', 'Weisfeiler-Leman', 'VC dimension']}","{'value': 'Graph neural networks (GNNs) employing message passing for graph classification are inherently limited by the expressive power of the Weisfeiler-Leman (WL) test for graph isomorphism. Node individualization schemes, which assign unique identifiers to nodes (e.g., by adding random noise to features), are a common approach for achieving universal expressiveness. However, the ability of GNNs endowed with individualization schemes to generalize beyond the training data is still an open question. To address this question, this paper presents a theoretical analysis of the sample complexity of such GNNs from a statistical learning perspective, employing Vapnik–Chervonenkis (VC) dimension and covering number bounds. We demonstrate that node individualization schemes that are permutation-equivariant result in lower sample complexity, and design novel individualization schemes that exploit these results. As an application of this analysis, we also develop a novel architecture that can perform substructure identification (i.e., subgraph isomorphism) while having a lower VC dimension compared to competing methods. Finally, our theoretical findings are validated experimentally on both synthetic and real-world datasets.'}",https://openreview.net{'value': '/pdf/dc30fe1e4bcc37018a86765d07d56af45324493b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=7su2GfqvmN,{'value': 'ContactField: Implicit Field Representation for Multi-Person Interaction Geometry'},Hansol Lee; Tackgeun You; Hansoo Park; Woohyeon Shim; Sanghyeon Kim; Hwasup Lim,~Hansol_Lee1; ~Tackgeun_You1; ~Hansoo_Park1; ~Woohyeon_Shim2; ~Sanghyeon_Kim2; ~Hwasup_Lim1,"{'value': ['3D recontruction', 'multi-view', '3D human estimation']}","{'value': 'We introduce a novel implicit field representation tailored for multi-person interaction geometry in 3D spaces, capable of simultaneously reconstructing occupancy, instance identification (ID) tags, and contact fields. Volumetric representation of interacting human bodies presents significant  challenges, including inaccurately captured geometries, varying degrees of occlusion, and data scarcity. Existing multi-view methods, which either reconstruct each subject in isolation or merge nearby 3D surfaces into a single unified mesh, often fail to capture the intricate geometry between interacting bodies and exploit on datasets with many views and a small group of people for training. Our approach utilizes an implicit representation for interaction geometry contextualized by a multi-view local-global feature module. This module adeptly aggregates both local and global information from individual views and interacting groups, enabling precise modeling of close physical interactions through dense point retrieval in small areas, supported by the implicit fields. Furthermore, we develop a synthetic dataset encompassing diverse multi-person interaction scenarios to enhance the robustness of our geometry estimation. The experimental results demonstrate the superiority of our method to accurately reconstruct human geometries and ID tags within three-dimensional spaces, outperforming conventional multi-view techniques. Notably, our method facilitates unsupervised estimation of contact points without the need for specific training data on contact supervision.'}",https://openreview.net{'value': '/pdf/3503c030ccbf28bb3177944f8cd9e90c4bed6a41.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=7WoOphIZ8u,{'value': 'Derivatives of Stochastic Gradient Descent in parametric optimization'},Franck Iutzeler; Edouard Pauwels; Samuel Vaiter,~Franck_Iutzeler1; ~Edouard_Pauwels1; ~Samuel_Vaiter1,"{'value': ['automatic differentiation', 'stochastic gradient descent', 'optimization']}","{'value': 'We consider stochastic optimization problems where the objective depends on some parameter, as commonly found in hyperparameter optimization for instance. We investigate the behavior of the derivatives of the iterates of Stochastic Gradient Descent (SGD) with respect to that parameter and show that they are driven by an inexact SGD recursion on a different objective function, perturbed by the convergence of the original SGD. This enables us to establish that the derivatives of SGD converge to the derivative of the solution mapping in terms of mean squared error whenever the objective is strongly convex. Specifically, we demonstrate that with constant step-sizes, these derivatives stabilize within a noise ball centered at the solution derivative, and that with vanishing step-sizes they exhibit $O(\\log(k)^2 / k)$ convergence rates. Additionally, we prove exponential convergence in the interpolation regime.  Our theoretical findings are illustrated by numerical experiments on synthetic tasks.'}",https://openreview.net{'value': '/pdf/423c86dd162502363f0560c9b74988efc9fb4f92.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=7W0f7lifDk,{'value': 'Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models'},Yuxuan Xue; Xianghui Xie; Riccardo Marin; Gerard Pons-Moll,~Yuxuan_Xue1; ~Xianghui_Xie1; ~Riccardo_Marin1; ~Gerard_Pons-Moll2,"{'value': ['3D Reconstruction', '3D Human Reconstruction', 'Diffusion Models', '3D Generative Models', '2D Foundation Models']}","{'value': 'Creating realistic avatars from a single RGB image is an attractive yet challenging problem. To deal with challenging loose clothing or occlusion by interaction objects, we leverage powerful shape prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot provide multi-view shape priors with guaranteed 3D consistency. We propose Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion. Our key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other. By coupling them in a tight manner, we can fully leverage the potential of both models. We introduce a novel image-conditioned generative 3D Gaussian Splats reconstruction model that leverages the prior from 2D multi-view diffusion models, and provides an explicit 3D representation, which further guides the 2D reverse sampling process\nto have better 3D consistency. Experiments show that our proposed framework outperforms state-of-the-art methods and enables the creation of realistic avatars from a single RGB image, achieving high-fidelity in both geometry and appearance. Extensive ablations also validate the efficacy of our design, (1) multi-view 2D priors conditioning in generative 3D reconstruction and (2) consistency refinement of sampling trajectory via the explicit 3D representation. Our code and models are released at https://yuxuan-xue.com/human-3diffusion/.'}",https://openreview.net{'value': '/pdf/75f7a0f999b597254f6d5c8883618b41c682659f.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=7Tir0u0ukg,{'value': 'Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning'},Hao-Lun Hsu; Weixin Wang; Miroslav Pajic; Pan Xu,~Hao-Lun_Hsu1; ~Weixin_Wang2; ~Miroslav_Pajic2; ~Pan_Xu1,"{'value': ['Multi-Agent Reinforcement Learning', 'Randomized Exploration', 'Deep Reinforcement Learning']}","{'value': 'We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication complexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (i.e., $N$-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.'}",https://openreview.net{'value': '/pdf/2ac53b19e0a9ca2b8f06a74ca163d25fef316583.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=7NrYnCN2be,{'value': 'Boosting Semi-Supervised Scene Text Recognition via Viewing and Summarizing'},Yadong Qu; Yuxin Wang; Bangbang Zhou; Zixiao Wang; Hongtao Xie; Yongdong Zhang,~Yadong_Qu1; ~Yuxin_Wang1; ~Bangbang_Zhou1; ~Zixiao_Wang3; ~Hongtao_Xie2; ~Yongdong_Zhang2,"{'value': ['Scene text recognition', 'Semi-supervised learning', 'Contrastive Learning']}","{'value': 'Existing scene text recognition (STR) methods struggle to recognize challenging texts, especially for artistic and severely distorted characters. The limitation lies in the insufficient exploration of character morphologies, including the monotonousness of widely used synthetic training data and the sensitivity of the model to character morphologies. To address these issues, inspired by the human learning process of viewing and summarizing, we facilitate the contrastive learning-based STR framework in a self-motivated manner by leveraging synthetic and real unlabeled data without any human cost. In the viewing process, to compensate for the simplicity of synthetic data and enrich character morphology diversity, we propose an Online Generation Strategy to generate background-free samples with diverse character styles. By excluding background noise distractions, the model is encouraged to focus on character morphology and generalize the ability to recognize complex samples when trained with only simple synthetic data. To boost the summarizing process, we theoretically demonstrate the derivation error in the previous character contrastive loss, which mistakenly causes the sparsity in the intra-class distribution and exacerbates ambiguity on challenging samples. Therefore, a new Character Unidirectional Alignment Loss is proposed to correct this error and unify the representation of the same characters in all samples by aligning the character features in the student model with the reference features in the teacher model. Extensive experiment results show that our method achieves SOTA performance (94.7\\% and 70.9\\% average accuracy on common benchmarks and Union14M-Benchmark). Code will be available.'}",https://openreview.net{'value': '/pdf/bf2fac25c1e0c1368a45293d71a7f4484f021e1c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=7Jb4NJS8Yk,{'value': 'Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy'},Zhenyu Guan; Xiangyu Kong; Fangwei Zhong; Yizhou Wang,~Zhenyu_Guan3; ~Xiangyu_Kong1; ~Fangwei_Zhong3; ~Yizhou_Wang1,"{'value': ['AI Diplomacy', 'LLM-based Agent', 'Self-play']}","{'value': ""Diplomacy is one of the most sophisticated activities in human society, involving complex interactions among multiple parties that require skills in social reasoning, negotiation, and long-term strategic planning. Previous AI agents have demonstrated their ability to handle multi-step games and large action spaces in multi-agent tasks. However, diplomacy involves a staggering magnitude of decision spaces, especially considering the negotiation stage required. While recent agents based on large language models (LLMs) have shown potential in various applications, they still struggle with extended planning periods in complex multi-agent settings. Leveraging recent technologies for LLM-based agents, we aim to explore AI's potential to create a human-like agent capable of executing comprehensive multi-agent missions by integrating three fundamental capabilities: 1) strategic planning with memory and reflection; 2) goal-oriented negotiation with social reasoning; and 3) augmenting memory through self-play games for self-evolution without human in the loop. Project page: https://sites.google.com/view/richelieu-diplomacy.""}",https://openreview.net{'value': '/pdf/b498dfad8494b5e4819660e1f18ed57dcc6e93ae.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=7G362fgJFd,{'value': 'Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation'},Xin Yuan; Michael Maire,~Xin_Yuan5; ~Michael_Maire1,"{'value': ['diffusion models', 'unsupervised learning', 'image segmentation', 'neural network architecture']}","{'value': 'We develop a neural network architecture which, trained in an unsupervised manner as a denoising diffusion model, simultaneously learns to both generate and segment images.  Learning is driven entirely by the denoising diffusion objective, without any annotation or prior knowledge about regions during training.  A computational bottleneck, built into the neural architecture, encourages the denoising network to partition an input into regions, denoise them in parallel, and combine the results.  Our trained model generates both synthetic images and, by simple examination of its internal predicted partitions, semantic segmentations of those images.  Without fine-tuning, we directly apply our unsupervised model to the downstream task of segmenting real images via noising and subsequently denoising them.  Experiments demonstrate that our model achieves accurate unsupervised image segmentation and high-quality synthetic image generation across multiple datasets.'}",https://openreview.net{'value': '/pdf/0b0e26bd5cb8b993746d295c433c593d7ad86d9c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=7AXY27kdNH,{'value': 'Amortized Active Causal Induction with Deep Reinforcement Learning'},Yashas Annadani; Panagiotis Tigas; Stefan Bauer; Adam Foster,~Yashas_Annadani1; ~Panagiotis_Tigas1; ~Stefan_Bauer1; ~Adam_Foster1,"{'value': ['Active Causal Structure Learning', 'Adaptive Intervention Design', 'Reinforcement Learning']}","{'value': 'We present Causal Amortized Active Structure Learning (CAASL), an active intervention design policy that can select interventions that are adaptive, real-time and that does not require access to the likelihood. This policy, an amortized network based on the transformer, is trained with reinforcement learning on a simulator of the design environment, and a reward function that measures how close the true causal graph is to a causal graph posterior inferred from the gathered data. On synthetic data and a single-cell gene expression simulator, we demonstrate empirically that the data acquired through our policy results in a better estimate of the underlying causal graph than alternative strategies.  Our design policy successfully achieves amortized intervention design on the distribution of the training environment while also generalizing well to distribution shifts in test-time design environments. Further, our policy also demonstrates excellent zero-shot generalization to design environments with dimensionality higher than that during training, and to intervention types that it has not been trained on.'}",https://openreview.net{'value': '/pdf/bbc0abf6bfbdb21646362fb7cd41326593efb1b9.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=7AWMTPMZES,{'value': 'Discrete Modeling via Boundary Conditional Diffusion Processes'},Yuxuan Gu; Xiaocheng Feng; Lei Huang; Yingsheng Wu; Zekun Zhou; Weihong Zhong; kun Zhu; Bing Qin,~Yuxuan_Gu1; ~Xiaocheng_Feng1; ~Lei_Huang4; ~Yingsheng_Wu1; ~Zekun_Zhou1; ~Weihong_Zhong1; ~kun_Zhu2; ~Bing_Qin2,"{'value': ['Diffusion Models', 'Discrete Modeling', 'Language Generation', 'Discrete Image Generation']}","{'value': 'We present an novel framework for efficiently and effectively extending the powerful continuous diffusion processes to discrete modeling.\nPrevious approaches have suffered from the discrepancy between discrete data and continuous modeling.\nOur study reveals that the absence of guidance from discrete boundaries in learning probability contours is one of the main reasons.\nTo address this issue, we propose a two-step forward process that first estimates the boundary as a prior distribution and then rescales the forward trajectory to construct a boundary conditional diffusion model.\nThe reverse process is proportionally adjusted to guarantee that the learned contours yield more precise discrete data.\nExperimental results indicate that our approach achieves strong performance in both language modeling and discrete image generation tasks.\nIn language modeling, our approach surpasses previous state-of-the-art continuous diffusion language models in three translation tasks and a summarization task, while also demonstrating competitive performance compared to auto-regressive transformers. Moreover, our method achieves comparable results to continuous diffusion models when using discrete ordinal pixels and establishes a new state-of-the-art for categorical image generation on the Cifar-10 dataset.'}",https://openreview.net{'value': '/pdf/7f76468ef9524e51fc230dd01d451a25d531af01.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=7ANmKBfP88,{'value': 'Right this way: Can VLMs Guide Us to See More to Answer Questions?'},Li Liu; Diji Yang; Sijia Zhong; Kalyana Suma Sree Tholeti; Lei Ding; Yi Zhang; Leilani H. Gilpin,~Li_Liu7; ~Diji_Yang1; ~Sijia_Zhong1; ~Kalyana_Suma_Sree_Tholeti1; ~Lei_Ding10; ~Yi_Zhang91; ~Leilani_H._Gilpin1,"{'value': ['visual accessibility', 'self-knowledge', 'vision language models']}","{'value': ""In question-answering scenarios, humans can assess whether the available information is sufficient and seek additional information if necessary, rather than providing a forced answer. In contrast, Vision Language Models (VLMs) typically generate direct, one-shot responses without evaluating the sufficiency of the information. To investigate this gap, we identify a critical and challenging task in the Visual Question Answering (VQA) scenario: can VLMs indicate how to adjust an image when the visual information is insufficient to answer a question? This capability is especially valuable for assisting visually impaired individuals who often need guidance to capture images correctly. To evaluate this capability of current VLMs, we introduce a human-labeled dataset as a benchmark for this task. Additionally, we present an automated framework that generates synthetic training data by simulating ``where to know'' scenarios. Our empirical results show significant performance improvements in mainstream VLMs when fine-tuned with this synthetic data. This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans.""}",https://openreview.net{'value': '/pdf/7eefc1dd2f32c4f6d7284fcd114adabee5cad87a.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=6jOScqwdHU,{'value': 'Fisher Flow Matching for Generative Modeling over Discrete Data'},Oscar Davis; Samuel Kessler; Mircea Petrache; Ismail Ilkan Ceylan; Michael M. Bronstein; Joey Bose,~Oscar_Davis1; ~Samuel_Kessler1; ~Mircea_Petrache1; ~Ismail_Ilkan_Ceylan2; ~Michael_M._Bronstein1; ~Joey_Bose1,"{'value': ['Flow matching', 'Generative models', 'Riemannian manifolds', 'Discrete data']}","{'value': 'Generative modeling over discrete data has recently seen numerous success stories, with applications spanning language modeling, biological sequence design, and graph-structured molecular data. The predominant generative modeling paradigm for discrete data is still autoregressive, with more recent alternatives based on diffusion or flow-matching falling short of their impressive performance in continuous data settings, such as image or video generation. In this work, we introduce Fisher-Flow, a novel flow-matching model for discrete data. Fisher-Flow takes a manifestly geometric perspective\nby considering categorical distributions over discrete data as points residing on a statistical manifold equipped with its natural Riemannian metric: the \\emph{Fisher-Rao metric}. As a result, we demonstrate discrete data itself can be continuously reparameterised to points on the positive orthant of the $d$-hypersphere $\\mathbb{S}^d_+$, \nwhich allows us to define flows that map any source distribution to target in a principled manner by transporting mass along (closed-form) geodesics of $\\mathbb{S}^d_+$. Furthermore, the learned flows in Fisher-Flow can be further bootstrapped by leveraging Riemannian optimal transport leading to improved training dynamics. We prove that the gradient flow induced by Fisher-FLow is optimal in reducing the forward KL divergence. We evaluate Fisher-Flow on an array of synthetic and diverse real-world benchmarks, including designing DNA Promoter, and DNA Enhancer sequences. Empirically, we find that Fisher-Flow improves over prior diffusion and flow-matching models on these benchmarks.'}",https://openreview.net{'value': '/pdf/c16ab010bac44658a3c695f87e0c8d925deca3d4.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=6emETARnWi,{'value': 'Transfer Learning for Diffusion Models'},Yidong Ouyang; Liyan Xie; Hongyuan Zha; Guang Cheng,~Yidong_Ouyang1; ~Liyan_Xie2; ~Hongyuan_Zha1; ~Guang_Cheng1,"{'value': ['Diffusion Model', 'Transfer Learning']}","{'value': 'Diffusion models, a specific type of generative model, have achieved unprecedented performance in recent years and consistently produce high-quality synthetic samples. A critical prerequisite for their notable success lies in the presence of a substantial number of training samples, which can be impractical in real-world applications due to high collection costs or associated risks. Consequently, various finetuning and regularization approaches have been proposed to transfer knowledge from existing pre-trained models to specific target domains with limited data. This paper introduces the Transfer Guided Diffusion Process (TGDP), a novel approach distinct from conventional finetuning and regularization methods. \nWe prove that the optimal diffusion model for the target domain integrates pre-trained diffusion models on the source domain with additional guidance from a domain classifier. \nWe further extend TGDP to a conditional version for modeling the joint distribution of data and its corresponding labels, together with two additional regularization terms to enhance the model performance. We validate the effectiveness of TGDP on both simulated and real-world datasets.'}",https://openreview.net{'value': '/pdf/15b4cc70a6d50ffde3529d15cf984bd7ed55de88.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=6aJrEC28hR,{'value': 'Graph neural networks and non-commuting operators'},Mauricio Velasco; Kaiying O'Hare; Bernardo Rychtenberg; Soledad Villar,~Mauricio_Velasco1; ~Kaiying_O'Hare1; ~Bernardo_Rychtenberg1; ~Soledad_Villar2,"{'value': ['graph neural networks', 'trasferability', 'stability', 'non-commuting operators', 'graphons']}","{'value': 'Graph neural networks (GNNs) provide state-of-the-art results in a wide variety of tasks which typically involve predicting features at the vertices of a graph. They are built from layers of graph convolutions which serve as a powerful inductive bias for describing the flow of information among the vertices. Often, more than one data modality is available. This work considers a setting in which several graphs have the same vertex set and a common vertex-level learning task. This generalizes standard GNN models to GNNs with several graph operators that do not commute. We may call this model graph-tuple neural networks (GtNN). \n\nIn this work, we develop the mathematical theory to address the stability and transferability of GtNNs using properties of non-commuting non-expansive operators. We develop a limit theory of graphon-tuple neural networks and use it to prove a universal transferability theorem that guarantees that all graph-tuple neural networks are transferable on convergent graph-tuple sequences. In particular, there is no non-transferable energy under the convergence we consider here. Our theoretical results extend well-known transferability theorems for GNNs to the case of several simultaneous graphs (GtNNs) and provide a strict improvement on what is currently known even in the GNN case.\n\nWe illustrate our theoretical results with simple experiments on synthetic and real-world data. To this end, we derive a training procedure that provably enforces the stability of the resulting model.'}",https://openreview.net{'value': '/pdf/c7c518f298e5899f7e9285c55ca3425a0e7104ce.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=65htepluYE,{'value': 'Locating What You Need: Towards Adapting Diffusion Models to OOD Concepts In-the-Wild'},Jianan Yang; Chenchao Gao; Zhiqing Xiao; Junbo Zhao; Sai Wu; Gang Chen; Haobo Wang,~Jianan_Yang1; ~Chenchao_Gao1; ~Zhiqing_Xiao1; ~Junbo_Zhao1; ~Sai_Wu2; ~Gang_Chen6; ~Haobo_Wang1,"{'value': ['Image Genearation', 'Text-to-Image', 'Diffusion', 'Out-of-Distribution', 'Active Learning']}","{'value': 'The recent large-scale text-to-image generative models have attained unprecedented performance, while people established *adaptor* modules like LoRA and DreamBooth to extend this performance to even more unseen concept tokens. However, we empirically find that this workflow often fails to accurately depict the *out-of-distribution* concepts. This failure is highly related to the low quality of training data. To resolve this, we present a framework called Controllable Adaptor Towards Out-of-Distribution Concepts (CATOD). Our framework follows the active learning paradigm which includes high-quality data accumulation and adaptor training, enabling a finer-grained enhancement of generative results. The *aesthetics* score and *concept-matching* score are two major factors that impact the quality of synthetic results. One key component of CATOD is the weighted scoring system that automatically balances between these two scores and we also offer comprehensive theoretical analysis for this point. Then, it determines how to select data and schedule the adaptor training based on this scoring system. The extensive results show that CATOD significantly outperforms the prior approaches with an 11.10 boost on the CLIP score and a 33.08% decrease on the CMMD metric.'}",https://openreview.net{'value': '/pdf/0fc543fd65a2897c2f5093d8d26f5cfa1377d114.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=6292sp7HiE,{'value': 'One-shot Federated Learning via Synthetic Distiller-Distillate Communication'},Junyuan Zhang; Songhua Liu; Xinchao Wang,~Junyuan_Zhang2; ~Songhua_Liu2; ~Xinchao_Wang1,"{'value': ['Federated Learning', 'Dataset Distillation']}","{'value': 'One-shot Federated learning (FL) is a powerful technology facilitating collaborative training of machine learning models in a single round of communication. While its superiority lies in communication efficiency and privacy preservation compared to iterative FL, one-shot FL often compromises model performance. Prior research has primarily focused on employing data-free knowledge distillation to optimize data generators and ensemble models for better aggregating local knowledge into the server model. However, these methods typically struggle with data heterogeneity, where inconsistent local data distributions can cause teachers to provide misleading knowledge. Additionally, they may encounter scalability issues with complex datasets due to inherent two-step information loss: first, during local training (from data to model), and second, when transferring knowledge to the server model (from model to inversed data). In this paper, we propose FedSD2C, a novel and practical one-shot FL framework designed to address these challenges. FedSD2C introduces a distiller to synthesize informative distillates directly from local data to reduce information loss and proposes sharing synthetic distillates instead of inconsistent local models to tackle data heterogeneity. Our empirical results demonstrate that FedSD2C consistently outperforms other one-shot FL methods with more complex and real datasets, achieving up to 2.6 $\\times$ the performance of the best baseline. Code: https://github.com/Carkham/FedSD2C'}",https://openreview.net{'value': '/pdf/e9fc9ea641481d8b3a60bc33d30efb2afc5cdb6f.pdf'},{'title_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=5l5bhYexYO,{'value': 'Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers'},Kai Yan; Alex Schwing; Yu-Xiong Wang,~Kai_Yan1; ~Alex_Schwing1; ~Yu-Xiong_Wang1,"{'value': ['Decision transformer', 'reinforcement learning']}","{'value': ""Decision Transformers have recently emerged as a new and compelling paradigm for offline Reinforcement Learning (RL), completing a trajectory in an autoregressive way. While improvements have been made to overcome  initial shortcomings, online finetuning of decision transformers has been surprisingly under-explored. The widely adopted state-of-the-art Online Decision Transformer (ODT) still struggles when pretrained with low-reward offline data. In this paper, we theoretically analyze the online-finetuning of the decision transformer,  showing that the commonly used Return-To-Go (RTG) that's far from the expected return hampers the online fine-tuning process. This problem, however, is well-addressed by the value function and advantage of standard RL algorithms. As suggested by our analysis, in our experiments, we hence find that simply adding TD3 gradients to the finetuning process of ODT effectively improves the online finetuning performance of ODT, especially if ODT is pretrained with low-reward offline data. These findings provide new directions to further improve decision transformers.""}",https://openreview.net{'value': '/pdf/ecce2f00d7d353921acab4fa6ec0c37938a55c2a.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=5ai2YFAXV7,{'value': 'Hypothesis Testing the Circuit Hypothesis in LLMs'},Claudia Shi; Nicolas Beltran-Velez; Achille Nazaret; Carolina Zheng; Adrià Garriga-Alonso; Andrew Jesson; Maggie Makar; David Blei,~Claudia_Shi1; ~Nicolas_Beltran-Velez1; ~Achille_Nazaret1; ~Carolina_Zheng1; ~Adrià_Garriga-Alonso1; ~Andrew_Jesson1; ~Maggie_Makar1; ~David_Blei2,"{'value': ['Hypothesis testings', 'mechanistic interpretability', 'circuit']}","{'value': ""Large language models (LLMs) demonstrate surprising capabilities, but we do not understand how they are implemented. \nOne hypothesis suggests that these capabilities are primarily executed by small subnetworks within the LLM, known as circuits. But how can we evaluate this hypothesis?\nIn this paper, we formalize a set of criteria that a circuit is hypothesized to meet and develop a suite of hypothesis tests to evaluate how well circuits satisfy them. \nThe criteria focus on the extent to which the LLM's behavior is preserved, the degree of localization of this behavior, and whether the circuit is minimal.\nWe apply these tests to six circuits described in the research literature. \nWe find that synthetic circuits -- circuits that are hard-coded in the model -- align with the idealized properties. \nCircuits discovered in Transformer models satisfy the criteria to varying degrees.\nTo facilitate future empirical studies of circuits, we created the \\textit{circuitry} package, a wrapper around the \\textit{TransformerLens} library, which abstracts away lower-level manipulations of hooks and activations. The software is available at \\url{https://github.com/blei-lab/circuitry}.""}",https://openreview.net{'value': '/pdf/d42b43708ca0c06c98f6b5d7a422bd9082f54bdf.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=5PrShrKxoX,{'value': 'Transfer Q-star : Principled Decoding for LLM Alignment'},Souradip Chakraborty; Soumya Suvra Ghosal; Ming Yin; Dinesh Manocha; Mengdi Wang; Amrit Bedi; Furong Huang,~Souradip_Chakraborty1; ~Soumya_Suvra_Ghosal2; ~Ming_Yin4; ~Dinesh_Manocha3; ~Mengdi_Wang1; ~Amrit_Bedi1; ~Furong_Huang1,"{'value': ['RLHF', 'AI Alignment', 'Decoding', 'LLM', 'Transfer Decoding']}","{'value': 'Aligning foundation models is essential for their safe and trustworthy deployment. However, traditional fine-tuning methods are computationally intensive and require updating billions of model parameters. A promising alternative, alignment via decoding, adjusts the response distribution directly without model updates to maximize a target reward $r$, thus providing a lightweight and adaptable framework for alignment. However, principled decoding methods rely on oracle access to an optimal Q-function ($Q^*$), which is often unavailable in practice. Hence, prior SoTA methods either approximate this $Q^*$ using $Q^{\\pi_{\\text{sft}}}$ (derived from the reference $\\texttt{SFT}$ model) or rely on short-term rewards, resulting in sub-optimal decoding performance. In this work, we propose $\\texttt{Transfer Q}^*$, which implicitly estimates the optimal value function for a target reward $r$ through a baseline model $\\rho_{\\texttt{BL}}$  aligned with a baseline reward $r_{\\texttt{BL}}$ (which can be different from the target reward $r$). Theoretical analyses of $\\texttt{Transfer Q}^*$ provide a rigorous characterization of its optimality, deriving an upper bound on the sub-optimality gap and identifying a hyperparameter to control the deviation from the pre-trained reference $\\texttt{SFT}$ model based on user needs. Our approach significantly reduces the sub-optimality gap observed in prior SoTA methods and demonstrates superior empirical performance across key metrics such as coherence, diversity, and quality in extensive tests on several synthetic and real datasets.'}",https://openreview.net{'value': '/pdf/0b107f4dab3fc053e007ba6aed8b411f2fcb5373.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=5HQhYiGnYb,{'value': 'FIDE: Frequency-Inflated Conditional Diffusion Model for Extreme-Aware Time Series Generation'},Asadullah Hill Galib; Pang-Ning Tan; Lifeng Luo,~Asadullah_Hill_Galib1; ~Pang-Ning_Tan1; ~Lifeng_Luo1,"{'value': ['Diffusion model', 'time series', 'extreme values']}","{'value': ""Time series generation is a crucial aspect of data analysis, playing a pivotal role in learning the temporal patterns and their underlying dynamics across diverse fields. Conventional time series generation methods often struggle to capture extreme values adequately, diminishing their value in critical applications such as scenario planning and management for healthcare, finance, climate change adaptation, and beyond. In this paper, we introduce a conditional diffusion model called FIDE to address the challenge of preserving the distribution of extreme values in generative modeling for time series. FIDE employs a novel high-frequency inflation strategy in the frequency domain, preventing premature fade-out of the extreme value. It also extends traditional diffusion-based model, enabling the generation of samples conditioned on the block maxima, thereby enhancing the model's capacity to capture extreme events. Additionally, the FIDE framework incorporates the Generalized Extreme Value (GEV) distribution within its generative modeling framework, ensuring fidelity to both block maxima and overall data distribution. Experimental results on real-world and synthetic data showcase the efficacy of FIDE over baseline methods, highlighting its potential in advancing Generative AI for time series analysis, specifically in accurately modeling extreme events.""}",https://openreview.net{'value': '/pdf/285d2747b600e6d3316e485f911ea40684b85114.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=5GCgNFZSyo,{'value': 'Minimizing UCB: a Better Local Search Strategy in Local Bayesian Optimization'},Zheyi Fan; Wenyu Wang; Szu Hui Ng; Qingpei Hu,~Zheyi_Fan2; ~Wenyu_Wang4; ~Szu_Hui_Ng1; ~Qingpei_Hu1,"{'value': ['Bayesian Optimization', 'Local Optimization', 'High dimensional', 'Minimizing upper confidence Bound']}","{'value': 'Local Bayesian optimization is a promising practical approach to solve the high dimensional black-box function optimization problem. Among them is the approximated gradient class of methods, which implements a strategy similar to gradient descent. These methods have achieved good experimental results and theoretical guarantees. However, given the distributional properties of the Gaussian processes applied on these methods, there may be potential to further exploit the information of the Gaussian processes to facilitate the BO search. In this work, we develop the relationship between the steps of the gradient descent method and one that minimizes the Upper Confidence Bound (UCB), and show that the latter can be a better strategy than direct gradient descent when a Gaussian process is applied as a surrogate. Through this insight, we propose a new local Bayesian optimization algorithm, MinUCB, which replaces the gradient descent step with minimizing UCB in GIBO. We further show that MinUCB maintains a similar convergence rate with GIBO. We then improve the acquisition function of MinUCB further through a look ahead strategy, and obtain a more efficient algorithm LA-MinUCB. We apply our algorithms on different synthetic and real-world functions, and the results show the effectiveness of our method. Our algorithms also illustrate improvements on local search strategies from an upper bound perspective in Bayesian optimization, and provides a new direction for future algorithm design.'}",https://openreview.net{'value': '/pdf/e92c9b1a43b506609a5e918898b9706004f3cdbc.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=5G7MRfPngt,{'value': 'VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought'},Gabriel Herbert Sarch; Lawrence Jang; Michael J. Tarr; William W. Cohen; Kenneth Marino; Katerina Fragkiadaki,~Gabriel_Herbert_Sarch1; ~Lawrence_Jang1; ~Michael_J._Tarr1; ~William_W._Cohen2; ~Kenneth_Marino1; ~Katerina_Fragkiadaki1,"{'value': ['Multimodal Agents', 'Multimodal Large Language Models', 'Visual Demonstrations', 'Human-In-The-Loop', 'Instruction Following', 'Autonomous Web Agents', 'Ego4D']}","{'value': ""Large-scale generative language and vision-language models (LLMs and VLMs) excel in few-shot in-context learning for decision making and instruction following. However, they require high-quality exemplar demonstrations to be included in their context window. In this work, we ask: Can LLMs and VLMs generate their own examples from generic, sub-optimal demonstrations? We propose In-Context Abstraction Learning (ICAL), a method that builds a memory of multimodal experience from sub-optimal demonstrations and human feedback. Given a task demonstration that may contain inefficiencies or mistakes, a VLM abstracts the trajectory into a generalized program by correcting inefficient actions and annotating cognitive abstractions: causal relationships, object state changes, temporal subgoals, and task-relevant visual elements. These abstractions are iteratively improved and adapted through human feedback while the agent attempts to execute the trajectory in a similar environment. The resulting examples, when used as exemplars in the prompt, significantly improve decision-making in retrieval-augmented LLM and VLM agents. Moreover, as the agent's library of examples grows, it becomes more efficient, relying less on human feedback and requiring fewer environment interactions per demonstration. Our ICAL agent surpasses the state-of-the-art in dialogue-based instruction following in TEACh, multimodal web agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6% improvement in goal-condition success. In VisualWebArena, our task success rate improves over the SOTA from 14.3% to 22.7% using GPT4V. In Ego4D action forecasting, we improve over few-shot GPT-4V and remain competitive with supervised models. We show finetuning our retrieval-augmented in-context agent yields additional improvements. Our approach significantly reduces reliance on manual prompt engineering and consistently outperforms in-context learning from action plans that lack such abstractions.""}",https://openreview.net{'value': '/pdf/5e9a92d28e4a08ae32f2c2c3ade645443ae537cb.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=52PTSrAQQM,{'value': 'Bootstrapping Top-down Information for Self-modulating Slot Attention'},Dongwon Kim; Seoyeon Kim; Suha Kwak,~Dongwon_Kim1; ~Seoyeon_Kim1; ~Suha_Kwak3,"{'value': ['object-centric learning', 'object discovery', 'slot attention']}","{'value': 'Object-centric learning (OCL) aims to learn representations of individual objects within visual scenes without manual supervision, facilitating efficient and effective visual reasoning. Traditional OCL methods primarily employ bottom-up approaches that aggregate homogeneous visual features to represent objects. However, in complex visual environments, these methods often fall short due to the heterogeneous nature of visual features within an object. To address this, we propose a novel OCL framework incorporating a top-down pathway. This pathway first bootstraps the semantics of individual objects and then modulates the model to prioritize features relevant to these semantics. By dynamically modulating the model based on its own output, our top-down pathway enhances the representational quality of objects. Our framework achieves state-of-the-art performance across multiple synthetic and real-world object-discovery benchmarks.'}",https://openreview.net{'value': '/pdf/30da89369f1f5af3dd3b8c07b97b134029bcc2e3.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=4mxzxYhMuN,{'value': 'Motion Forecasting in Continuous Driving'},Nan Song; Bozhou Zhang; Xiatian Zhu; Li Zhang,~Nan_Song4; ~Bozhou_Zhang1; ~Xiatian_Zhu3; ~Li_Zhang5,{'value': ['Motion forecasting; Autonomous Driving;']},"{'value': ""Motion forecasting for agents in autonomous driving is highly challenging due to the numerous possibilities for each agent's next action and their complex interactions in space and time. \nIn real applications, motion forecasting takes place repeatedly and continuously as the self-driving car moves. However, existing forecasting methods typically process each driving scene within a certain range independently, totally ignoring the situational and contextual relationships between successive driving scenes. This significantly simplifies the forecasting task, making the solutions suboptimal and inefficient to use in practice. To address this fundamental limitation, we propose a novel motion forecasting framework for continuous driving, named RealMotion.\nIt comprises two integral streams both at the scene level:\n(1) The scene context stream progressively accumulates historical scene information until the present moment, capturing temporal interactive relationships among scene elements.\n(2) The agent trajectory stream optimizes current forecasting by sequentially relaying past predictions.\nBesides, a data reorganization strategy is introduced to narrow the gap between existing benchmarks and real-world applications, consistent with our network. These approaches enable exploiting more broadly the situational and progressive insights of dynamic motion across space and time. \nExtensive experiments on Argoverse series with different settings demonstrate that our RealMotion achieves state-of-the-art performance, along with the advantage of efficient real-world inference.""}",https://openreview.net{'value': '/pdf/2a125e7a1eda481ef4cf87ee407026f462a9ee79.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=4jegYnUMHb,{'value': 'MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning'},Bin-Bin Gao,~Bin-Bin_Gao1,"{'value': ['Anomaly Classification', 'Anomaly Segmentation', 'One-Prompt Meta-Learning', 'Universal Anomaly Segmentation']}","{'value': 'Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation.\nWe present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods. Code and Models: https://github.com/gaobb/MetaUAS.'}",https://openreview.net{'value': '/pdf/c3c24c493f2de20c90611891da523e1a4fe7158a.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=4XTvXMSZPO,{'value': 'DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning'},Hao Bai; Yifei Zhou; Jiayi Pan; Mert Cemri; Alane Suhr; Sergey Levine; Aviral Kumar,~Hao_Bai1; ~Yifei_Zhou1; ~Jiayi_Pan1; ~Mert_Cemri1; ~Alane_Suhr1; ~Sergey_Levine1; ~Aviral_Kumar2,"{'value': ['LLM/VLM Agent', 'Device Control', 'GUI Navigation', 'Web Agent', 'Reinforcement Learning']}","{'value': 'Pre-trained vision language models (VLMs), though powerful, typically lack training on decision-centric data, rendering them sub-optimal for decision-making tasks such as in-the-wild device control through Graphical User Interfaces (GUIs) when used off-the-shelf. While training with static demonstrations has shown some promise, we show that such methods fall short when controlling real GUIs due to their failure to deal with real world stochasticity and dynamism not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline and offline-to-online RL. We first build a scalable and parallelizable Android learning environment equipped with a VLM-based general-purpose evaluator and then identify the key design choices for simple and effective RL in this domain. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.5B VLM trained with RL achieves a 49.5\\% absolute improvement -- from 17.7 to 67.2\\% success rate -- over supervised fine-tuning with static human demonstration data. It is worth noting that such improvement is achieved without any additional supervision or demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3\\% success rate) and the 17B CogAgent trained with AitW data (14.4\\%), but also our implementation of prior best autonomous RL approach based on filtered behavior cloning (57.8\\%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.'}",https://openreview.net{'value': '/pdf/53508ea1db0056abe7a6fb24ad516a8a40675570.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=4WIBvL6ZF4,{'value': 'Dynamic Subgroup Identification in Covariate-adjusted Response-adaptive Randomization Experiments'},Yanping Li; Jingshen Wang; Waverly Wei,~Yanping_Li3; ~Jingshen_Wang1; ~Waverly_Wei1,"{'value': ['Covariate-adjusted response-adaptive randomization design', 'Response-adaptive randomization design', 'Subgroup identification', 'Causal inference', 'Treatment effect heterogeneity']}","{'value': 'Identifying subgroups with differential responses to treatment is pivotal in randomized clinical trials, as tailoring treatments to specific subgroups can advance personalized medicine. Upon trial completion, identifying best-performing subgroups–those with the most beneficial treatment effects–is crucial for optimizing resource allocation or mitigating adverse treatment effects. However, traditional clinical trials are not customized for the goal of identifying best-performing subgroups because they typically pre-define subgroups at the beginning of the trial and adhere to a fixed subgroup treatment allocation rule, leading to inefficient use of experimental efforts. While some adaptive experimental strategies exist for the identification of the single best subgroup, they commonly do not enable the identification of the best set of subgroups.  To address these challenges, we propose a dynamic subgroup identification covariate-adjusted response-adaptive randomization (CARA) design strategy with the following key features: (i) Our approach is an adaptive experimental strategy that allows the dynamic identification of the best subgroups and the revision of treatment allocation towards the goal of correctly identifying the best subgroups based on collected experimental data. (ii) Our design handles ties between subgroups effectively, merging those with similar treatment effects to maximize experimental efficiency. In the theoretical investigations, we demonstrate that our design has a higher probability of correctly identifying the best set of subgroups compared to conventional designs. Additionally, we prove the statistical validity of our estimator for the best subgroup treatment effect, demonstrating its asymptotic normality and semiparametric efficiency. Finally, we validate our design using synthetic data from a clinical trial on cirrhosis.'}",https://openreview.net{'value': '/pdf/d286a95fd2a41caec6ac57fdef23c9a9790f6a9c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=4NGrHrhJPx,{'value': 'The Dormant Neuron Phenomenon in Multi-Agent Reinforcement Learning Value Factorization'},Haoyuan Qin; Chennan Ma; Mian Deng; Zhengzhu Liu; Songzhu Mei; Xinwang Liu; Cheng Wang; Siqi Shen,~Haoyuan_Qin1; ~Chennan_Ma1; ~Mian_Deng2; ~Zhengzhu_Liu1; ~Songzhu_Mei1; ~Xinwang_Liu1; ~Cheng_Wang2; ~Siqi_Shen5,{'value': ['dormant neurons; Multi-agent reinforcement learning']},"{'value': 'In this work, we study the dormant neuron phenomenon in multi-agent reinforcement learning value factorization, where the mixing network suffers from reduced network expressivity caused by an increasing number of inactive neurons. We demonstrate the presence of the dormant neuron phenomenon across multiple environments and algorithms, and show that this phenomenon negatively affects the learning process. We show that dormant neurons correlates with the existence of over-active neurons, which have large activation scores. To address the dormant neuron issue, we propose ReBorn, a simple but effective method that transfers the weights from over-active neurons to dormant neurons. We theoretically show that this method can ensure the learned action preferences are not forgotten after the weight-transferring procedure, which increases learning effectiveness. Our extensive experiments reveal that ReBorn achieves promising results across various environments and improves the performance of multiple popular value factorization approaches. The source code of ReBorn is available in \\url{https://github.com/xmu-rl-3dv/ReBorn}.'}",https://openreview.net{'value': '/pdf/adcdf7fb5e791061801164bd5612eca66289c9de.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=4KeSvAvNMr,{'value': 'Treeffuser: probabilistic prediction via conditional diffusions with gradient-boosted trees'},Nicolas Beltran-Velez; Alessandro Antonio Grande; Achille Nazaret; Alp Kucukelbir; David Blei,~Nicolas_Beltran-Velez1; ~Alessandro_Antonio_Grande1; ~Achille_Nazaret1; ~Alp_Kucukelbir1; ~David_Blei2,"{'value': ['trees', 'gradient boosting', 'diffusion', 'probabilistic predictions', 'uncertainty quantification']}","{'value': 'Probabilistic prediction aims to compute predictive distributions rather than single point predictions. These distributions enable practitioners to quantify uncertainty, compute risk, and detect outliers. However, most probabilistic methods assume parametric responses, such as Gaussian or Poisson distributions. When these assumptions fail, such models lead to bad predictions and poorly calibrated uncertainty.  In this paper, we propose Treeffuser, an easy-to-use method for probabilistic prediction on tabular data. The idea is to learn a conditional diffusion model where the score function is estimated using gradient-boosted trees. The conditional diffusion model makes Treeffuser flexible and non-parametric, while the gradient-boosted trees make it robust and easy to train on CPUs. Treeffuser learns well-calibrated predictive distributions and can handle a wide range of regression tasks---including those with multivariate, multimodal, and skewed responses. We study Treeffuser on synthetic and real data and show that it outperforms existing methods, providing better calibrated probabilistic predictions. We further demonstrate its versatility with an application to inventory allocation under uncertainty using sales data from Walmart. We implement Treeffuser in https://github.com/blei-lab/treeffuser.'}",https://openreview.net{'value': '/pdf/37f4b86045c3b7538c3dc547e7b148f8b2e6e419.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=4A5IQEjG8c,{'value': 'Slack-Free Spiking Neural Network Formulation for Hypergraph Minimum Vertex Cover'},Tam Ngoc-Bang Nguyen; Anh-Dzung Doan; zhipeng cai; Tat-Jun Chin,~Tam_Ngoc-Bang_Nguyen1; ~Anh-Dzung_Doan2; ~zhipeng_cai3; ~Tat-Jun_Chin2,"{'value': ['Neuromorphic computing', 'spiking neural network', 'hypergraph minimum vertex cover']}","{'value': 'Neuromorphic computers open up the potential of energy-efficient computation using spiking neural networks (SNN), which consist of neurons that exchange spike-based information asynchronously. In particular, SNNs have shown promise in solving combinatorial optimization. Underpinning the SNN methods is the concept of energy minimization of an Ising model, which is closely related to quadratic unconstrained binary optimization (QUBO). Thus, the starting point for many SNN methods is reformulating the target problem as QUBO, then executing an SNN-based QUBO solver. For many combinatorial problems, the reformulation entails introducing penalty terms, potentially with slack variables, that implement feasibility constraints in the QUBO objective. For more complex problems such as hypergraph minimum vertex cover (HMVC), numerous slack variables are introduced which drastically increase the search domain and reduce the effectiveness of the SNN solver. In this paper, we propose a novel SNN formulation for HMVC. Rather than using penalty terms with slack variables, our SNN architecture introduces additional spiking neurons with a constraint checking and correction mechanism that encourages convergence to feasible solutions. In effect, our method obviates the need for reformulating HMVC as QUBO. Experiments on neuromorphic hardware show that our method consistently yielded high quality solutions for HMVC on real and synthetic instances where the SNN-based QUBO solver often failed, while consuming measurably less energy than global solvers on CPU.'}",https://openreview.net{'value': '/pdf/a54da240ba210ec568b111aff5136b38d97a6650.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3vJbgcjgvd,{'value': 'Higher-Order Causal Message Passing for Experimentation with Complex Interference'},Mohsen Bayati; Yuwei Luo; William Overman; Sadegh Shirani; Ruoxuan Xiong,~Mohsen_Bayati1; ~Yuwei_Luo1; ~William_Overman1; ~Sadegh_Shirani1; ~Ruoxuan_Xiong1,"{'value': ['Causal Inference', 'Network Interference', 'Dynamic Treatment Effect', 'Randomized Experiment', 'Approximate Message Passing']}","{'value': 'Accurate estimation of treatment effects is essential for decision-making across various scientific fields. This task, however, becomes challenging in areas like social sciences and online marketplaces, where treating one experimental unit can influence outcomes for others through direct or indirect interactions. Such interference can lead to biased treatment effect estimates, particularly when the structure of these interactions is unknown. We address this challenge by introducing a new class of estimators based on causal message-passing, specifically designed for settings with pervasive, unknown interference. Our estimator draws on information from the sample mean and variance of unit outcomes and treatments over time, enabling efficient use of observed data to estimate the evolution of the system state. Concretely, we construct non-linear features from the moments of unit outcomes and treatments and then learn a function that maps these features to future mean and variance of unit outcomes. This allows for the estimation of the treatment effect over time. Extensive simulations across multiple domains, using synthetic and real network data, demonstrate the efficacy of our approach in estimating total treatment effect dynamics, even in cases where interference exhibits non-monotonic behavior in the probability of treatment.'}",https://openreview.net{'value': '/pdf/35c8bfb7d5f5fd10dbf5d84f2466623362716754.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3uUIwMxYbR,{'value': 'Revisiting Differentially Private ReLU Regression'},Meng Ding; Mingxi Lei; Liyang Zhu; Shaowei Wang; Di Wang; Jinhui Xu,~Meng_Ding3; ~Mingxi_Lei1; ~Liyang_Zhu1; ~Shaowei_Wang1; ~Di_Wang1; ~Jinhui_Xu1,"{'value': ['Differential Privacy', 'Generalized Linear Model']}","{'value': 'As one of the most fundamental non-convex learning problems, ReLU regression under differential privacy (DP) constraints, especially in high-dimensional settings, remains a challenging area in privacy-preserving machine learning. Existing results are limited to the assumptions of bounded norm $ \\|\\mathbf{x}\\|_2 \\leq 1$, which becomes meaningless with increasing data dimensionality. In this work, we revisit the problem of DP ReLU regression in high-dimensional regimes. We propose two innovative algorithms DP-GLMtron and DP-TAGLMtron that outperform the conventional DPSGD. \nDP-GLMtron is based on a generalized linear model perceptron approach, integrating adaptive clipping and Gaussian mechanism for enhanced privacy. To overcome the constraints of small privacy budgets in DP-GLMtron, represented by $\\widetilde{O}(\\sqrt{1/N})$ where $N$ is the sample size, we introduce DP-TAGLMtron, which utilizes a tree aggregation protocol to balance privacy and utility effectively, showing that DP-TAGLMtron achieves comparable performance with only an additional factor of $O(\\log N)$ in the utility upper bound.\nMoreover, our theoretical analysis extends beyond Gaussian-like data distributions to settings with eigenvalue decay, showing how data distribution impacts learning in high dimensions. Notably, our findings suggest that the utility upper bound could be independent of the dimension $d$, even when $d \\gg N$. \nExperiments on synthetic and real-world datasets also validate our results.'}",https://openreview.net{'value': '/pdf/1b8bed0fd41cb46007e7290717266daa2cadf94d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3mCr7ZNdSw,{'value': 'Privacy without Noisy Gradients: Slicing Mechanism for Generative Model Training'},Kristjan Greenewald; Yuancheng Yu; Hao Wang; Kai Xu,~Kristjan_Greenewald1; ~Yuancheng_Yu1; ~Hao_Wang22; ~Kai_Xu4,"{'value': ['differential privacy', 'synthetic data generation', 'tabular data', 'GAN', 'f divergence', 'slicing']}","{'value': ""Training generative models with differential privacy (DP)  typically involves injecting noise into gradient updates or adapting the discriminator's training procedure. As a result, such approaches often struggle with hyper-parameter tuning and convergence. \nWe consider the \\emph{slicing privacy mechanism} that injects noise into random low-dimensional projections of the private data, and provide strong privacy guarantees for it. These noisy projections are used for training generative models.\nTo enable optimizing generative models using this DP approach, we introduce the \\emph{smoothed-sliced $f$-divergence} and show it enjoys statistical consistency.  \nMoreover, we present a kernel-based estimator for this divergence, circumventing the need for adversarial training. \nExtensive numerical experiments demonstrate that our approach can generate synthetic data of higher quality compared with baselines. Beyond performance improvement, our method, by sidestepping the need for noisy gradients, offers data scientists the flexibility to adjust generator architecture and hyper-parameters, run the optimization over any number of epochs, and even restart the optimization process---all without incurring additional privacy costs.""}",https://openreview.net{'value': '/pdf/2675a0c1e4020d53b82bc7fd0775ff672f7ba05c.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3l2HnZXNou,{'value': 'Multi-Agent Coordination via Multi-Level Communication'},Ziluo Ding; Zeyuan Liu; Zhirui Fang; Kefan Su; Liwen Zhu; Zongqing Lu,~Ziluo_Ding1; ~Zeyuan_Liu2; ~Zhirui_Fang1; ~Kefan_Su1; ~Liwen_Zhu1; ~Zongqing_Lu2,{'value': ['multi-agent reinforcement learning']},"{'value': 'The partial observability and stochasticity in multi-agent settings can be mitigated by accessing more information about others via communication. However, the coordination problem still exists since agents cannot communicate actual actions with each other at the same time due to the circular dependencies. In this paper, we propose a novel multi-level communication scheme, Sequential Communication (SeqComm). SeqComm treats agents asynchronously (the upper-level agents make decisions before the lower-level ones) and has two communication phases. In the negotiation phase, agents determine the priority of decision-making by communicating hidden states of observations and comparing the value of intention, which is obtained by modeling the environment dynamics. In the launching phase, the upper-level agents take the lead in making decisions and then communicate their actions with the lower-level agents. Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, we show that SeqComm outperforms existing methods in a variety of cooperative multi-agent tasks.'}",https://openreview.net{'value': '/pdf/111bdb476557b41f6d7f48240d4c392f82fcbd34.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3dn1hINA6o,{'value': 'The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning'},Anya Sims; Cong Lu; Jakob Nicolaus Foerster; Yee Whye Teh,~Anya_Sims1; ~Cong_Lu1; ~Jakob_Nicolaus_Foerster1; ~Yee_Whye_Teh2,"{'value': ['Offline Reinforcement Learning', 'Model-Based Reinforcement Learning']}","{'value': 'Offline reinforcement learning (RL) aims to train agents from pre-collected datasets. However, this comes with the added challenge of estimating the value of behaviors not covered in the dataset. Model-based methods offer a potential solution by training an approximate dynamics model, which then allows collection of additional synthetic data via rollouts in this model. The prevailing theory treats this approach as online RL in an approximate dynamics model, and any remaining performance gap is therefore understood as being due to dynamics model errors. In this paper, we analyze this assumption and investigate how popular algorithms perform as the learned dynamics model is improved. In contrast to both intuition and theory, if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail. This reveals a key oversight: The theoretical foundations assume sampling of full horizon rollouts in the learned dynamics model; however, in practice, the number of model-rollout steps is aggressively reduced to prevent accumulating errors. We show that this truncation of rollouts results in a set of edge-of-reach states at which we are effectively ""bootstrapping from the void."" This triggers pathological value overestimation and complete performance collapse. We term this the edge-of-reach problem. Based on this new insight, we fill important gaps in existing theory, and reveal how prior model-based methods are primarily addressing the edge-of-reach problem, rather than model-inaccuracy as claimed. Finally, we propose Reach-Aware Value Learning (RAVL), a simple and robust method that directly addresses the edge-of-reach problem and hence - unlike existing methods - does not fail as the dynamics model is improved. Since world models will inevitably improve, we believe this is a key step towards future-proofing offline RL.'}",https://openreview.net{'value': '/pdf/6797eaef1557c755c3627309219c7f61c718c588.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3apt5AJ5QN,{'value': 'Global Rewards in Restless Multi-Armed Bandits'},Naveen Janaki Raman; Zheyuan Ryan Shi; Fei Fang,~Naveen_Janaki_Raman1; ~Zheyuan_Ryan_Shi1; ~Fei_Fang1,"{'value': ['Restless Bandits', 'Multi-Armed Bandit', 'Submodular', 'Food Rescue']}","{'value': 'Restless multi-armed bandits (RMAB) extend multi-armed bandits so arm pulls impact future arm states. Despite the success of RMABs, a key limiting assumption is the separability of rewards into a sum across arms. We address this deficiency by proposing restless-multi-armed bandit with global rewards (RMAB-G), a generalization of RMABs to global non-separable rewards. To solve RMAB-G, we develop the Linear-Whittle and Shapley-Whittle indices, which extend Whittle indices from RMABs to RMAB-Gs. We prove approximation bounds which demonstrate how Linear and Shapley-Whittle indices fail for non-linear rewards. To overcome this limitation, we propose two sets of adaptive policies: the first computes indices iteratively and the second combines indices with Monte-Carlo Tree Search (MCTS). Empirically, we demonstrate that adaptive policies outperform both pre-computed index policies and baselines in synthetic and real-world food rescue datasets.'}",https://openreview.net{'value': '/pdf/c94e70e7b28d901a74a3fb2df5e9d23afe6565dd.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3ZAfFoAcUI,{'value': 'On the Inductive Bias of Stacking Towards Improving Reasoning'},Nikunj Saunshi; Stefani Karp; Shankar Krishnan; Sobhan Miryoosefi; Sashank J. Reddi; Sanjiv Kumar,~Nikunj_Saunshi1; ~Stefani_Karp1; ~Shankar_Krishnan1; ~Sobhan_Miryoosefi1; ~Sashank_J._Reddi1; ~Sanjiv_Kumar1,"{'value': ['stacking', 'language model', 'reasoning', 'inductive bias', 'efficient training']}","{'value': 'Given the increasing scale of model sizes, efficient training strategies like gradual stacking have garnered interest. Stacking enables efficient training by gradually growing the depth of a model in stages and using layers from a smaller model in an earlier stage to initialize the next stage. Although efficient for training, the model biases induced by such growing approaches are largely unexplored. In this work, we examine this fundamental aspect of gradual stacking, going beyond its efficiency benefits. We propose a variant of gradual stacking called MIDAS that can speed up language model training by up to 40\\%. Furthermore we discover an intriguing phenomenon: MIDAS is not only training-efficient but surprisingly also has an inductive bias towards improving downstream tasks, especially tasks that require reasoning abilities like reading comprehension and math problems, despite having similar or slightly worse perplexity compared to baseline training. To further analyze this inductive bias, we construct {\\em reasoning primitives} – simple synthetic tasks that are building blocks for reasoning – and find that a model pretrained with stacking is significantly better than standard pretraining on these primitives, with and without fine-tuning. This provides stronger and more robust evidence for this inductive bias towards reasoning. These findings of training efficiency and inductive bias towards reasoning are verified at 1B, 2B and 8B parameter language models. Finally, we conjecture the underlying reason for this inductive bias by exploring the connection of stacking to looped models and provide strong supporting empirical analysis.'}",https://openreview.net{'value': '/pdf/30d478429b58478c4650f019a85f0ef30d3e5b85.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3RxcarQFRn,{'value': 'Generative Adversarial Model-Based Optimization via Source Critic Regularization'},Michael S Yao; Yimeng Zeng; Hamsa Bastani; Jacob R. Gardner; James Gee; Osbert Bastani,~Michael_S_Yao1; ~Yimeng_Zeng1; ~Hamsa_Bastani1; ~Jacob_R._Gardner1; ~James_Gee1; ~Osbert_Bastani1,"{'value': ['Offline Optimization', 'Bayesian Optimization', 'Surrogate Objectives']}","{'value': 'Offline model-based optimization seeks to optimize against a learned surrogate model without querying the true oracle objective function during optimization. Such tasks are commonly encountered in protein design, robotics, and clinical medicine where evaluating the oracle function is prohibitively expensive. However, inaccurate surrogate model predictions are frequently encountered along offline optimization trajectories. To address this limitation, we propose *generative adversarial model-based optimization* using **adaptive source critic regularization (aSCR)**—a task- and optimizer- agnostic framework for constraining the optimization trajectory to regions of the design space where the surrogate function is reliable. We propose a computationally tractable algorithm to dynamically adjust the strength of this constraint, and show how leveraging aSCR with standard Bayesian optimization outperforms existing methods on a suite of offline generative design tasks. Our code is available at https://github.com/michael-s-yao/gabo.'}",https://openreview.net{'value': '/pdf/5e9c7395873a86fe0b8430642a4679960f1a27ef.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3LZHatxUa9,{'value': 'On the Impact of Feature Heterophily on Link Prediction with Graph Neural Networks'},Jiong Zhu; Gaotang Li; Yao-An Yang; Jing Zhu; Xuehao Cui; Danai Koutra,~Jiong_Zhu1; ~Gaotang_Li1; ~Yao-An_Yang1; ~Jing_Zhu4; ~Xuehao_Cui1; ~Danai_Koutra1,"{'value': ['graph neural networks', 'heterophily', 'link prediction']}","{'value': 'Heterophily, or the tendency of connected nodes in networks to have different class labels or dissimilar features, has been identified as challenging for many Graph Neural Network (GNN) models. While the challenges of applying GNNs for node classification when class labels display strong heterophily are well understood, it is unclear how heterophily affects GNN performance in other important graph learning tasks where class labels are not available. In this work, we focus on the link prediction task and systematically analyze the impact of heterophily in node features on GNN performance. We first introduce formal definitions of homophilic and heterophilic link prediction tasks, and present a theoretical framework that highlights the different optimizations needed for the respective tasks. We then analyze how different link prediction encoders and decoders adapt to varying levels of feature homophily and introduce designs for improved performance. Based on our definitions, we identify and analyze six real-world benchmarks spanning from homophilic to heterophilic link prediction settings, with graphs containing up to 30M edges. Our empirical analysis on a variety of synthetic and real-world datasets confirms our theoretical insights and highlights the importance of adopting learnable decoders and GNN encoders with ego- and neighbor-embedding separation in message passing for link prediction tasks beyond homophily.'}",https://openreview.net{'value': '/pdf/7c0d24d8c5b940086df83fb002c3e92da763b36b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3J5hvO5UaW,{'value': 'Optimal Classification under Performative Distribution Shift'},Edwige Cyffers; Muni Sreenivas Pydi; Jamal Atif; Olivier Cappé,~Edwige_Cyffers1; ~Muni_Sreenivas_Pydi1; ~Jamal_Atif1; ~Olivier_Cappé2,"{'value': ['Performative learning', 'classification', 'performative gradient', 'distribution shift', 'pushforward', 'robustness']}","{'value': 'Performative learning addresses the increasingly pervasive situations in which algorithmic decisions may induce changes in the data distribution as a consequence of their public deployment.  We propose a novel view in which these performative effects are modelled as push forward measures. This general framework encompasses existing models and enables novel performative gradient estimation methods, leading to more efficient and scalable learning strategies. For distribution shifts, unlike previous models which require full specification of the data distribution, we only assume knowledge of the shift operator that represents the performative changes. This approach can also be integrated into various change-of-variable-based models, such as VAEs or normalizing flows. Focusing on classification with a linear-in-parameters performative effect, we prove the convexity of the performative risk under a new set of assumptions. Notably, we do not limit the strength of performative effects but rather their direction, requiring only that classification becomes harder when deploying more accurate models. In this case, we also establish a connection with adversarially robust classification by reformulating the performative risk as a min-max variational problem. Finally, we illustrate our approach on synthetic and real datasets.'}",https://openreview.net{'value': '/pdf/ccad5b419763a0dc6b2014e2fd9874930f4e3bab.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=3HpgVs22UJ,{'value': 'Adaptive $Q$-Aid for Conditional Supervised Learning in Offline Reinforcement Learning'},Jeonghye Kim; Suyoung Lee; Woojun Kim; Youngchul Sung,~Jeonghye_Kim1; ~Suyoung_Lee4; ~Woojun_Kim1; ~Youngchul_Sung1,"{'value': ['offline reinforcement learning', 'return conditioned supervised learning', 'Q function', 'dynamic programming']}","{'value': ""Offline reinforcement learning (RL) has progressed with return-conditioned supervised learning (RCSL), but its lack of stitching ability remains a limitation. We introduce $Q$-Aided Conditional Supervised Learning (QCS), which effectively combines the stability of RCSL with the stitching capability of $Q$-functions. By analyzing $Q$-function over-generalization, which impairs stable stitching, QCS adaptively integrates $Q$-aid into RCSL's loss function based on trajectory return. Empirical results show that QCS significantly outperforms RCSL and value-based methods, consistently achieving or exceeding the highest trajectory returns across diverse offline RL benchmarks. QCS represents a breakthrough in offline RL, pushing the limits of what can be achieved and fostering further innovations.""}",https://openreview.net{'value': '/pdf/fff9167af1010fda153f43bbde087bd01d94d010.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=35WwZhkush,{'value': 'BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation'},Xiang Zhang; Bingxin Ke; Hayko Riemenschneider; Nando Metzger; Anton Obukhov; Markus Gross; Konrad Schindler; Christopher Schroers,~Xiang_Zhang21; ~Bingxin_Ke1; ~Hayko_Riemenschneider3; ~Nando_Metzger1; ~Anton_Obukhov1; ~Markus_Gross1; ~Konrad_Schindler1; ~Christopher_Schroers1,"{'value': ['Monocular depth estimation', 'diffusion model', 'zero-shot transfer', 'plug-and-play']}","{'value': 'By training over large-scale datasets, zero-shot monocular depth estimation (MDE) methods show robust performance in the wild but often suffer from insufficient detail. Although recent diffusion-based MDE approaches exhibit a superior ability to extract details, they struggle in geometrically complex scenes that challenge their geometry prior, trained on less diverse 3D  data. To leverage the complementary merits of both worlds, we propose BetterDepth to achieve geometrically correct affine-invariant MDE while capturing fine details. Specifically, BetterDepth is a conditional diffusion-based refiner that takes the prediction from pre-trained MDE models as depth conditioning, in which the global depth layout is well-captured, and iteratively refines details based on the input image. For the training of such a refiner, we propose global pre-alignment and local patch masking methods to ensure BetterDepth remains faithful to the depth conditioning while learning to add fine-grained scene details. With efficient training on small-scale synthetic datasets, BetterDepth achieves state-of-the-art zero-shot MDE performance on diverse public datasets and on in-the-wild scenes. Moreover, BetterDepth can improve the performance of other MDE models in a plug-and-play manner without further re-training.'}",https://openreview.net{'value': '/pdf/0fba6477df5fed26b30216cbabd2d5bfabf65133.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=2vMvh5XP0P,{'value': 'Subsurface Scattering for Gaussian Splatting'},Jan-Niklas Dihlmann; Arjun Majumdar; Andreas Engelhardt; Raphael Braun; Hendrik Lensch,~Jan-Niklas_Dihlmann1; ~Arjun_Majumdar3; ~Andreas_Engelhardt1; ~Raphael_Braun1; ~Hendrik_Lensch2,"{'value': ['gaussian splatting', 'inverse rendering', 'relighting', 'subsurface scattering', 'computer graphics', 'brdf decomposition', 'pbr', 'differentiable rendering', 'nerf']}","{'value': ""3D reconstruction and relighting of objects made from scattering materials present a significant challenge due to the complex light transport beneath the surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at real-time speeds. While 3D Gaussians efficiently approximate an object's surface, they fail to capture the volumetric properties of subsurface scattering. We propose a framework for optimizing an object's shape together with the radiance transfer field given multi-view OLAT (one light at a time) data. Our method decomposes the scene into an explicit surface represented as 3D Gaussians, with a spatially varying BRDF, and an implicit volumetric representation of the scattering component. A learned incident light field accounts for shadowing. We optimize all parameters jointly via ray-traced differentiable rendering. Our approach enables material editing, relighting, and novel view synthesis at interactive rates. We show successful application on synthetic data and contribute a newly acquired multi-view multi-light dataset of objects in a light-stage setup. Compared to previous work we achieve comparable or better results at a fraction of optimization and rendering time while enabling detailed control over material attributes.""}",https://openreview.net{'value': '/pdf/e47f0c8da42e87fc6881bc7f968014b6ab73998d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=2uy3LZHNIG,{'value': 'SMART: Scalable Multi-agent Real-time Motion Generation via Next-token Prediction'},Wei Wu; Xiaoxin Feng; Ziyan Gao; Yuheng KAN,~Wei_Wu14; ~Xiaoxin_Feng1; ~Ziyan_Gao1; ~Yuheng_KAN1,{'value': ['Autonomous driving; Generative motion model; Multi-agent motion simulation; Motion prediction']},"{'value': ""Data-driven autonomous driving motion generation tasks are frequently impacted by the limitations of dataset size and the domain gap between datasets, which precludes their extensive application in real-world scenarios. To address this issue, we introduce SMART, a novel autonomous driving motion generation paradigm that models vectorized map and agent trajectory data into discrete sequence tokens. These tokens are then processed through a decoder-only transformer architecture to train for the next token prediction task across spatial-temporal series. This GPT-style method allows the model to learn the motion distribution in real driving scenarios. SMART achieves state-of-the-art performance across most of the metrics on the generative Sim Agents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset (WOMD), demonstrating remarkable inference speed. Moreover, SMART represents the generative model in the autonomous driving motion domain, exhibiting zero-shot generalization capabilities: Using only the NuPlan dataset for training and WOMD for validation, SMART achieved a competitive score of 0.72 on the Sim Agents challenge. Lastly, we have collected over 1 billion motion tokens from multiple datasets, validating the model's scalability. These results suggest that SMART has initially emulated two important properties: scalability and zero-shot generalization, and preliminarily meets the needs of large-scale real-time simulation applications. We have released all the code to promote the exploration of models for motion generation in the autonomous driving field. The source code is available at https://github.com/rainmaker22/SMART.""}",https://openreview.net{'value': '/pdf/88366a6ac7a1f9f458e3ecd015714df2e674691c.pdf'},{'abstract_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=2squ766Iq4,{'value': 'Towards Understanding Extrapolation: a Causal Lens'},Lingjing Kong; Guangyi Chen; Petar Stojanov; Haoxuan Li; Eric P. Xing; Kun Zhang,~Lingjing_Kong1; ~Guangyi_Chen1; ~Petar_Stojanov2; ~Haoxuan_Li6; ~Eric_Xing1; ~Kun_Zhang1,"{'value': ['generalization', 'extrapolation', 'identification', 'adaptation']}","{'value': ""Canonical work handling distribution shifts typically necessitates an entire target distribution that lands inside the training distribution.\nHowever, practical scenarios often involve only a handful target samples, potentially lying outside the training support, which requires the capability of extrapolation.\nIn this work, we aim to provide a theoretical understanding of when extrapolation is possible and offer principled methods to achieve it without requiring an on-support target distribution.\nTo this end, we formulate the extrapolation problem with a latent-variable model that embodies the minimal change principle in causal mechanisms.\nUnder this formulation, we cast the extrapolation problem into a latent-variable identification problem.\nWe provide realistic conditions on shift properties and the estimation objectives that lead to identification even when only one off-support target sample is available, tackling the most challenging scenarios.\nOur theory reveals the intricate interplay between the underlying manifold's smoothness and the shift properties.\nWe showcase how our theoretical results inform the design of practical adaptation algorithms. Through experiments on both synthetic and real-world data, we validate our theoretical findings and their practical implications.""}",https://openreview.net{'value': '/pdf/2d712890989fcfe36f55521427e88278d3505737.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=2nisrxMMQR,{'value': 'Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning'},Fei Zhou; Peng Wang; Lei Zhang; Zhenghua Chen; Wei Wei; Chen Ding; Guosheng Lin; Yanning Zhang,~Fei_Zhou3; ~Peng_Wang19; ~Lei_Zhang28; ~Zhenghua_Chen2; ~Wei_Wei4; ~Chen_Ding3; ~Guosheng_Lin1; ~Yanning_Zhang1,{'value': ['Meta-learning Few-shot learning Cross-domain few-shot learning']},"{'value': ""Meta-learning offers a promising avenue for few-shot learning (FSL), enabling models to glean a generalizable feature embedding through episodic training on synthetic FSL tasks in a source domain. Yet, in practical scenarios where the target task diverges from that in the source domain, meta-learning based method is susceptible to over-fitting. To overcome this, we introduce a novel framework, Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning, which is crafted to comprehensively exploit the cross-domain transferable image prior that each image can be decomposed into complementary low-frequency content details and high-frequency robust structural characteristics. Motivated by this insight, we propose to decompose each query image into its high-frequency and low-frequency components, and parallel incorporate them into the feature embedding network to enhance the final category prediction. More importantly, we introduce a feature reconstruction prior and a prediction consistency prior to separately encourage the consistency of the intermediate feature as well as the final category prediction between the original query image and its decomposed frequency components. This allows for collectively guiding the network's meta-learning process with the aim of learning generalizable image feature embeddings, while not introducing any extra computational cost in the inference phase. Our framework establishes new state-of-the-art results on multiple cross-domain few-shot learning benchmarks.""}",https://openreview.net{'value': '/pdf/f8c6bad088545553ebb1115035902aaff2c7d2a3.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=2kZMtdjzSV,{'value': 'Beyond task diversity: provable representation transfer for sequential multitask linear bandits'},Thang Duong; Zhi Wang; Chicheng Zhang,~Thang_Duong1; ~Zhi_Wang3; ~Chicheng_Zhang1,"{'value': ['Bandits', 'multi-task', 'meta-learning', 'representation learning', 'online learning', 'task diversity']}","{'value': 'We study lifelong learning in linear bandits, where a learner interacts with a sequence of linear bandit tasks whose parameters lie in an $m$-dimensional subspace of $\\mathbb{R}^d$, thereby sharing a low-rank representation. Current literature typically assumes that the tasks are diverse, i.e., their parameters uniformly span the $m$-dimensional subspace. This assumption allows the low-rank representation to be learned before all tasks are revealed, which can be unrealistic in real-world applications. In this work, we present the first nontrivial result for sequential multi-task linear bandits without the task diversity assumption. We develop an algorithm that efficiently learns and transfers low-rank representations. When facing $N$ tasks, each played over $\\tau$ rounds, our algorithm achieves a regret guarantee of $\\tilde{O}\\big (Nm \\sqrt{\\tau} + N^{\\frac{2}{3}} \\tau^{\\frac{2}{3}} d m^{\\frac13} + Nd^2 + \\tau m d \\big)$ under the ellipsoid action set assumption.\nThis result can significantly improve upon the baseline of $\\tilde{O} \\left (Nd \\sqrt{\\tau}\\right)$ that does not leverage the low-rank structure when the number of tasks $N$ is sufficiently large and $m \\ll d$. We also demonstrate empirically on synthetic data that our algorithm outperforms baseline algorithms, which rely on the task diversity assumption.'}",https://openreview.net{'value': '/pdf/fee5ed7f4a4de4938110d076f487bafdde62b32b.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=2cFUYnNL1m,{'value': 'Weight Diffusion for Future: Learn to Generalize in Non-Stationary Environments'},Mixue Xie; Shuang Li; Binhui Xie; Chi Harold Liu; Jian Liang; Zixun Sun; Ke Feng; Chengwei Zhu,~Mixue_Xie2; ~Shuang_Li6; ~Binhui_Xie1; ~Chi_Harold_Liu1; ~Jian_Liang3; ~Zixun_Sun1; ~Ke_Feng1; ~Chengwei_Zhu1,"{'value': ['domain generalization', 'evolving pattern', 'diffuison model', 'weight generation', 'domain-incremental']}","{'value': 'Enabling deep models to generalize in non-stationary environments is vital for real-world machine learning, as data distributions are often found to continually change. Recently, evolving domain generalization (EDG) has emerged to tackle the domain generalization in a time-varying system, where the domain gradually evolves over time in an underlying continuous structure. Nevertheless, it typically assumes multiple source domains simultaneously ready. It still remains an open problem to address EDG in the domain-incremental setting, where source domains are non-static and arrive sequentially to mimic the evolution of training domains. To this end, we propose Weight Diffusion (W-Diff), a novel framework that utilizes the conditional diffusion model in the parameter space to learn the evolving pattern of classifiers during the domain-incremental training process. Specifically, the diffusion model is conditioned on the classifier weights of different historical domain (regarded as a reference point) and the prototypes of current domain, to learn the evolution from the reference point to the classifier weights of current domain (regarded as the anchor point). In addition, a domain-shared feature encoder is learned by enforcing prediction consistency among multiple classifiers, so as to mitigate the overfitting problem and restrict the evolving pattern to be reflected in the classifier as much as possible. During inference, we adopt the ensemble manner based on a great number of target domain-customized classifiers, which are cheaply obtained via the conditional diffusion model, for robust prediction. Comprehensive experiments on both synthetic and real-world datasets show the superior generalization performance of W-Diff on unseen domains in the future.'}",https://openreview.net{'value': '/pdf/a91611475a0cb860eb07bc5e0074c428b7cdb839.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=2YSHEBRRol,{'value': 'Aligning Individual and Collective Objectives in Multi-Agent Cooperation'},Yang Li; Wenhao Zhang; Jianhong Wang; Shao Zhang; Yali Du; Ying Wen; Wei Pan,~Yang_Li40; ~Wenhao_Zhang8; ~Jianhong_Wang1; ~Shao_Zhang1; ~Yali_Du1; ~Ying_Wen1; ~Wei_Pan2,"{'value': ['Mixed-motive cooperation', 'Mixed-motive game', 'cooperative AI']}","{'value': 'Among the research topics in multi-agent learning, mixed-motive cooperation is one of the most prominent challenges, primarily due to the mismatch between individual and collective goals. The cutting-edge research is focused on incorporating domain knowledge into rewards and introducing additional mechanisms to incentivize cooperation. However, these approaches often face shortcomings such as the effort on manual design and the absence of theoretical groundings. To close this gap, we model the mixed-motive game as a differentiable game for the ease of illuminating the learning dynamics towards cooperation. More detailed, we introduce a novel optimization method named \\textbf{\\textit{A}}ltruistic \\textbf{\\textit{G}}radient \\textbf{\\textit{A}}djustment (\\textbf{\\textit{AgA}}) that employs gradient adjustments to progressively align individual and collective objectives. Furthermore, we theoretically prove that AgA effectively attracts gradients to stable fixed points of the collective objective while considering individual interests, and we validate these claims with empirical evidence. We evaluate the effectiveness of our algorithm AgA through benchmark environments for testing mixed-motive collaboration with small-scale agents such as the two-player public good game and the sequential social dilemma games, Cleanup and Harvest, as well as our self-developed large-scale environment in the game StarCraft II.'}",https://openreview.net{'value': '/pdf/6f11dcfe0c49634b10f7b0ba2f23b10acced59d3.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=2UJLv3KPGO,{'value': 'Automating Data Annotation under Strategic Human Agents: Risks and Potential Solutions'},Tian Xie; Xueru Zhang,~Tian_Xie4; ~Xueru_Zhang2,"{'value': ['Strategic Classification', 'Long-term Fairness']}","{'value': 'As machine learning (ML) models are increasingly used in social domains to make consequential decisions about humans, they often have the power to reshape data distributions. Humans, as strategic agents, continuously adapt their behaviors in response to the learning system. As populations change dynamically, ML systems may need frequent updates to ensure high performance. However, acquiring high-quality *human-annotated* samples can be highly challenging and even infeasible in social domains. A common practice to address this issue is using the model itself to annotate unlabeled data samples. This paper investigates the long-term impacts when ML models are retrained with *model-annotated* samples when they incorporate human strategic responses. We first formalize the interactions between strategic agents and the model and then analyze how they evolve under such dynamic interactions. We find that agents are increasingly likely to receive positive decisions as the model gets retrained, whereas the proportion of agents with positive labels may decrease over time. We thus propose a *refined retraining process* to stabilize the dynamics. Last, we examine how algorithmic fairness can be affected by these retraining processes and find that enforcing common fairness constraints at every round may not benefit the disadvantaged group in the long run. Experiments on (semi-)synthetic and real data validate the theoretical findings.'}",https://openreview.net{'value': '/pdf/44841391152cc9c0c9a8aa7e562ff8698b75fdd0.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=2NKumsITFw,{'value': 'Learning from Noisy Labels via Conditional Distributionally Robust Optimization'},Hui Guo; Grace Yi; Boyu Wang,~Hui_Guo5; ~Grace_Yi1; ~Boyu_Wang3,"{'value': ['noisy label', 'conditional distributionally robust optimization (CDRO)', 'crowdsourcing']}","{'value': 'While crowdsourcing has emerged as a practical solution for labeling large datasets, it presents a significant challenge in learning accurate models due to noisy labels from annotators with varying levels of expertise. Existing methods typically estimate the true label posterior, conditioned on the instance and noisy annotations, to infer true labels or adjust loss functions. These estimates, however, often overlook potential misspecification in the true label posterior, which can degrade model performances, especially in high-noise scenarios. To address this issue, we investigate learning from noisy annotations with an estimated true label posterior through the framework of conditional distributionally robust optimization (CDRO). We propose formulating the problem as minimizing the worst-case risk within a distance-based ambiguity set centered around a reference distribution. By examining the strong duality of the formulation, we derive upper bounds for the worst-case risk and develop an analytical solution for the dual robust risk for each data point. This leads to a novel robust pseudo-labeling algorithm that leverages the likelihood ratio test to construct a pseudo-empirical distribution, providing a robust reference probability distribution in CDRO. Moreover, to devise an efficient algorithm for CDRO, we derive a closed-form expression for the empirical robust risk and the optimal Lagrange multiplier of the dual problem, facilitating a principled balance between robustness and model fitting. Our experimental results on both synthetic and real-world datasets demonstrate the superiority of our method.'}",https://openreview.net{'value': '/pdf/6c6f3a2146dbd1bcafc5e8782b1b8e788bdcfd95.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=20QgErW5zH,{'value': 'Drones Help Drones: A Collaborative Framework for Multi-Drone Object Trajectory Prediction and Beyond'},Zhechao Wang; Peirui Cheng; Minxing Chen; Pengju Tian; Zhirui Wang; Xinming Li; Xue Yang; Xian Sun,~Zhechao_Wang1; ~Peirui_Cheng1; ~Minxing_Chen1; ~Pengju_Tian1; ~Zhirui_Wang4; ~Xinming_Li4; ~Xue_Yang2; ~Xian_Sun2,"{'value': ['Multi-drone Collaboration', 'Perception and Prediction']}","{'value': 'Collaborative trajectory prediction can comprehensively forecast the future motion of objects through multi-view complementary information. However, it encounters two main challenges in multi-drone collaboration settings. The expansive aerial observations make it difficult to generate precise Bird\'s Eye View (BEV) representations. Besides, excessive interactions can not meet real-time prediction requirements within the constrained drone-based communication bandwidth. To address these problems, we propose a novel framework named ""Drones Help Drones"" (DHD). Firstly, we incorporate the ground priors provided by the drone\'s inclined observation to estimate the distance between objects and drones, leading to\xa0more precise BEV generation. Secondly, we design a selective mechanism based on the local feature discrepancy to prioritize the critical information contributing to prediction tasks during inter-drone interactions. Additionally, we create the first dataset for multi-drone collaborative prediction, named ""Air-Co-Pred"", and conduct quantitative and qualitative experiments to validate the effectiveness of our DHD framework. The results demonstrate that compared to state-of-the-art approaches, DHD reduces position deviation in BEV representations by over 20\\% and requires only a quarter of the transmission ratio for interactions while achieving comparable prediction performance. Moreover, DHD also shows promising generalization to the collaborative 3D object detection in CoPerception-UAVs.'}",https://openreview.net{'value': '/pdf/8f21eeda79a3843e1c56da440fe12db2b4a36f1e.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=1u3qkG7BkQ,{'value': 'Language-Driven Interactive Traffic Trajectory Generation'},Junkai XIA; Chenxin Xu; Qingyao Xu; Yanfeng Wang; Siheng Chen,~Junkai_XIA1; ~Chenxin_Xu1; ~Qingyao_Xu1; ~Yanfeng_Wang1; ~Siheng_Chen1,"{'value': ['Trajectory generation', 'Interaction', 'Language control']}","{'value': 'Realistic trajectory generation with natural language control is pivotal for advancing autonomous vehicle technology. However, previous methods focus on individual traffic participant trajectory generation, thus failing to account for the complexity of interactive traffic dynamics. In this work, we propose InteractTraj, the first language-driven traffic trajectory generator that can generate interactive traffic trajectories. InteractTraj interprets abstract trajectory descriptions into concrete formatted interaction-aware numerical codes and learns a mapping between these formatted codes and the final interactive trajectories. To interpret language descriptions, we propose a language-to-code encoder with a novel interaction-aware encoding strategy. To produce interactive traffic trajectories, we propose a code-to-trajectory decoder with interaction-aware feature aggregation that synergizes vehicle interactions with the environmental map and the vehicle moves. Extensive experiments show our method demonstrates superior performance over previous SoTA methods, offering a more realistic generation of interactive traffic trajectories with high controllability via diverse natural language commands.'}",https://openreview.net{'value': '/pdf/be7f3545c0333bfb3a48e442ac5e0f533a848f90.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=1sLdprsbmk,{'value': 'Can Models Learn Skill Composition from Examples?'},Haoyu Zhao; Simran Kaur; Dingli Yu; Anirudh Goyal; Sanjeev Arora,~Haoyu_Zhao1; ~Simran_Kaur1; ~Dingli_Yu1; ~Anirudh_Goyal1; ~Sanjeev_Arora1,"{'value': ['Skill Composition', 'Large Language Model']}","{'value': 'As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization---the capacity to combine learned skills in novel ways not encountered during training---has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the Skill-Mix evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified $k$-tuple of language skills. While small models struggled with composing even with $k=3$, larger models like GPT-4 performed reasonably well with $k=5$ and $6$.\n\nIn this paper, we employ a setup akin to Skill-Mix to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills---including rhetorical, literary, reasoning, theory of mind, and common sense---GPT was used to generate text samples that exhibit random subsets of $k$ skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of $k$, revealed the following findings: (1) Training on combinations of $k=2$ and $3$ skills results in noticeable improvements in the ability to compose texts with $k=4$ and $5$ skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills.\n\nThis study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.'}",https://openreview.net{'value': '/pdf/9f3f6b4982c98bcb4f5b5fbd4c8840e150884be1.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=1jG5ngXVs3,{'value': 'FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity Refiner'},Wenliang Zhao; Minglei Shi; Xumin Yu; Jie Zhou; Jiwen Lu,~Wenliang_Zhao2; ~Minglei_Shi1; ~Xumin_Yu2; ~Jie_Zhou3; ~Jiwen_Lu1,"{'value': ['flow-based generation', 'diffusion models', 'efficient sampling', 'image generation']}","{'value': ""Building on the success of diffusion models in visual generation, flow-based models reemerge as another prominent family of generative models that have achieved competitive or better performance in terms of both visual quality and inference speed. By learning the velocity field through flow-matching, flow-based models tend to produce a straighter sampling trajectory, which is advantageous during the sampling process. However, unlike diffusion models for which fast samplers are well-developed, efficient sampling of flow-based generative models has been rarely explored. In this paper, we propose a framework called FlowTurbo to accelerate the sampling of flow-based models while still enhancing the sampling quality. Our primary observation is that the velocity predictor's outputs in the flow-based models will become stable during the sampling, enabling the estimation of velocity via a lightweight velocity refiner. Additionally, we introduce several techniques including a pseudo corrector and sample-aware compilation to further reduce inference time. Since FlowTurbo does not change the multi-step sampling paradigm, it can be effectively applied for various tasks such as image editing, inpainting, etc. By integrating FlowTurbo into different flow-based models, we obtain an acceleration ratio of 53.1\\%$\\sim$58.3\\% on class-conditional generation and 29.8\\%$\\sim$38.5\\% on text-to-image generation. Notably, FlowTurbo reaches an FID of 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img), achieving the real-time image generation and establishing the new state-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo.""}",https://openreview.net{'value': '/pdf/6f20a0dec44b610dfcb9210877572960e5ca840e.pdf'},{'abstract_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=1iHmhMHNyA,{'value': 'Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation'},Jiawei Wang; Renhe Jiang; Chuang Yang; Zengqing Wu; Makoto Onizuka; Ryosuke Shibasaki; Noboru Koshizuka; Chuan Xiao,~Jiawei_Wang17; ~Renhe_Jiang1; ~Chuang_Yang3; ~Zengqing_Wu3; ~Makoto_Onizuka1; ~Ryosuke_Shibasaki1; ~Noboru_Koshizuka1; ~Chuan_Xiao2,"{'value': ['LLM', 'human mobility', 'urban computing', 'trajectory generation']}","{'value': 'This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.'}",https://openreview.net{'value': '/pdf/68fd30200398640d70b139a21fd58f711886738c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=1MCseWaFZb,{'value': 'CryoSPIN: Improving Ab-Initio Cryo-EM Reconstruction with Semi-Amortized Pose Inference'},Shayan Shekarforoush; David B. Lindell; Marcus A Brubaker; David J. Fleet,~Shayan_Shekarforoush1; ~David_B._Lindell1; ~Marcus_A_Brubaker1; ~David_J._Fleet1,"{'value': ['Cryo-EM 3D reconstruction', 'Pose estimation', 'Semi-Amortization', 'Multi-choice learning']}","{'value': 'Cryo-EM is an increasingly popular method for determining the atomic resolution 3D structure of macromolecular complexes (eg, proteins) from noisy 2D images captured by an electron microscope. The computational task is to reconstruct the 3D density of the particle, along with 3D pose of the particle in each 2D image, for which the posterior pose distribution is highly multi-modal. Recent developments in cryo-EM have focused on deep learning for which amortized inference has been used to predict pose. Here, we address key problems with this approach, and propose a new semi-amortized method, cryoSPIN, in which reconstruction begins with amortized inference and then switches to a form of auto-decoding to refine poses locally using stochastic gradient descent. Through evaluation on synthetic datasets, we demonstrate that cryoSPIN is able to handle multi-modal pose distributions during the amortized inference stage, while the later, more flexible stage of direct pose optimization yields faster and more accurate convergence of poses compared to baselines. On experimental data, we show that cryoSPIN outperforms the state-of-the-art cryoAI in speed and reconstruction quality.'}",https://openreview.net{'value': '/pdf/6c4cb8c1896b91037fa7dbe74f5a53bc084258fe.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=1H2e7USI09,{'value': 'Flow Priors for Linear Inverse Problems via  Iterative Corrupted Trajectory Matching'},Yasi Zhang; Peiyu Yu; Yaxuan Zhu; Yingshan Chang; Feng Gao; Ying Nian Wu; Oscar Leong,~Yasi_Zhang1; ~Peiyu_Yu2; ~Yaxuan_Zhu1; ~Yingshan_Chang1; ~Feng_Gao2; ~Ying_Nian_Wu1; ~Oscar_Leong2,"{'value': ['Flow-based models', 'Inverse problems', 'Neural ODE']}","{'value': ""Generative models based on flow matching have attracted significant attention for their simplicity and superior performance in high-resolution image synthesis. By leveraging the instantaneous change-of-variables formula, one can directly compute image likelihoods from a learned flow, making them enticing candidates as priors for downstream tasks such as inverse problems. In particular, a natural approach would be to incorporate such image probabilities in a maximum-a-posteriori (MAP) estimation problem. A major obstacle, however, lies in the slow computation of the log-likelihood, as it requires backpropagating through an ODE solver, which can be prohibitively slow for high-dimensional problems. In this work, we propose an iterative algorithm to approximate the MAP estimator efficiently to solve a variety of linear inverse problems. Our algorithm is mathematically justified by the observation that the MAP objective can be approximated by a sum of $N$ ``local MAP'' objectives, where $N$ is the number of function evaluations. By leveraging Tweedie's formula, we show that we can perform gradient steps to sequentially optimize these objectives. We validate our approach for various linear inverse problems, such as super-resolution, deblurring, inpainting, and compressed sensing, and demonstrate that we can outperform other methods based on flow matching. Code is available at \\url{https://github.com/YasminZhang/ICTM}.""}",https://openreview.net{'value': '/pdf/41117473ada2fc4333e42064eee99da4bcb3dbed.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=1GpY0hsv2w,{'value': 'Trajectory Diffusion for ObjectGoal Navigation'},Xinyao Yu; Sixian Zhang; Xinhang Song; Xiaorong Qin; Shuqiang Jiang,~Xinyao_Yu1; ~Sixian_Zhang1; ~Xinhang_Song1; ~Xiaorong_Qin1; ~Shuqiang_Jiang1,"{'value': ['Embodied AI', 'visual navigation']}","{'value': 'Object goal navigation requires an agent to navigate to a specified object in an unseen environment based on visual observations and user-specified goals. \nHuman decision-making in navigation is sequential, planning a most likely sequence of actions toward the goal. \nHowever, existing ObjectNav methods, both end-to-end learning methods and modular methods, rely on single-step planning. They output the next action based on the current model input, which easily overlooks temporal consistency and leads to myopic planning.\nTo this end, we aim to learn sequence planning for ObjectNav. Specifically, we propose trajectory diffusion to learn the distribution of trajectory sequences conditioned on the current observation and the goal. \nWe utilize DDPM and automatically collected optimal trajectory segments to train the trajectory diffusion.\nOnce the trajectory diffusion model is trained, it can generate a temporally coherent sequence of future trajectory for agent based on its current observations.\nExperimental results on the Gibson and MP3D datasets demonstrate that the generated trajectories effectively guide the agent, resulting in more accurate and efficient navigation.'}",https://openreview.net{'value': '/pdf/4a98ca3c5b5cc11841f2d3f230131fedb22a7c9a.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=15Jm9v7wCo,{'value': 'No-Regret Learning for Fair Multi-Agent Social Welfare Optimization'},Mengxiao Zhang; Ramiro Deo-Campo Vuong; Haipeng Luo,~Mengxiao_Zhang2; ~Ramiro_Deo-Campo_Vuong1; ~Haipeng_Luo1,"{'value': ['Online Learning', 'Nash Social Welfare']}","{'value': ""We consider the problem of online multi-agent Nash social welfare (NSW) maximization. While previous works of Hossain et al. [2021], Jones et al. [2023] study similar problems in stochastic multi-agent multi-armed bandits and show that $\\sqrt{T}$-regret is possible after $T$ rounds, their fairness measure is the product of all agents' rewards, instead of their NSW (that is, their geometric mean). Given the fundamental role of NSW in the fairness literature, it is more than natural to ask whether no-regret fair learning with NSW as the objective is possible. In this work, we provide a complete answer to this question in various settings. Specifically, in stochastic $N$-agent $K$-armed bandits, we develop an algorithm with $\\widetilde{\\mathcal{O}}(K^{\\frac{2}{N}}T^{\\frac{N-1}{N}})$ regret and prove that the dependence on $T$ is tight, making it a sharp contrast to the $\\sqrt{T}$-regret bounds of Hossain et al. [2021], Jones et al. [2023]. We then consider a more challenging version of the problem with adversarial rewards. Somewhat surprisingly, despite NSW being a concave function, we prove that no algorithm can achieve sublinear regret. To circumvent such negative results, we further consider a setting with full-information feedback and design two algorithms with $\\sqrt{T}$-regret: the first one has no dependence on $N$ at all and is applicable to not just NSW but a broad class of welfare functions, while the second one has better dependence on $K$ and is preferable when $N$ is small.\nFinally, we also show that logarithmic regret is possible whenever there exists one agent who is indifferent about different arms.""}",https://openreview.net{'value': '/pdf/0f176989aaba0b5a9a620922f69b6f4d71d36bd8.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=15460JjocO,{'value': 'Diversity Is Not All You Need: Training A Robust Cooperative Agent Needs Specialist Partners'},Rujikorn Charakorn; Poramate Manoonpong; Nat Dilokthanakul,~Rujikorn_Charakorn1; ~Poramate_Manoonpong1; ~Nat_Dilokthanakul1,"{'value': ['multi-agent', 'reinforcement learning', 'robustness', 'diversity', 'specialization']}","{'value': 'Partner diversity is known to be crucial for training a robust generalist cooperative agent. In this paper, we show that partner specialization, in addition to diversity, is crucial for the robustness of a downstream generalist agent. We propose a principled method for quantifying both the diversity and specialization of a partner population based on the concept of mutual information. Then, we observe that the recently proposed cross-play minimization (XP-min) technique produces diverse and specialized partners. However, the generated partners are overfit, reducing their usefulness as training partners. To address this, we propose simple methods, based on reinforcement learning and supervised learning, for extracting the diverse and specialized behaviors of XP-min generated partners but not their overfitness. We demonstrate empirically that the proposed method effectively removes overfitness, and extracted populations produce more robust generalist agents compared to the source XP-min populations.'}",https://openreview.net{'value': '/pdf/05f91b5d0168fb0c17bc3236085be55c4d2bee4c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=0zWzJj6lO3,{'value': 'Cooperate or Collapse:  Emergence of Sustainable Cooperation in a Society of LLM Agents'},Giorgio Piatti; Zhijing Jin; Max Kleiman-Weiner; Bernhard Schölkopf; Mrinmaya Sachan; Rada Mihalcea,~Giorgio_Piatti1; ~Zhijing_Jin1; ~Max_Kleiman-Weiner1; ~Bernhard_Schölkopf1; ~Mrinmaya_Sachan3; ~Rada_Mihalcea1,"{'value': ['cooperative AI', 'AI safety', 'LLM agents', 'cognitive science', 'language model evaluation', 'dynamic evaluation', 'alignment', 'agency', 'evolving benchmarks', 'multi-agent interactions']}","{'value': 'As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage ""Universalization""-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.'}",https://openreview.net{'value': '/pdf/24a89a718ee1521ef0621aa2434f4e73f69c270c.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=0dtA21q83C,{'value': 'DeNetDM: Debiasing by Network Depth Modulation'},Silpa Vadakkeeveetil Sreelatha; Adarsh Kappiyath; Abhra Chaudhuri; Anjan Dutta,~Silpa_Vadakkeeveetil_Sreelatha1; ~Adarsh_Kappiyath1; ~Abhra_Chaudhuri1; ~Anjan_Dutta1,"{'value': ['Trustworthy Machine Learning', 'Debiasing', 'Robustness']}","{'value': 'Neural networks trained on biased datasets tend to inadvertently learn spurious correlations, hindering generalization. We formally prove that (1) samples that exhibit spurious correlations lie on a lower rank manifold relative to the ones that do not; and (2) the depth of a network acts as an implicit regularizer on the rank of the attribute subspace that is encoded in its representations. Leveraging these insights, we present DeNetDM, a novel debiasing method that uses network depth modulation as a way of developing robustness to spurious correlations. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Our method requires no bias annotations or explicit data augmentation while performing on par with approaches that require either or both. We demonstrate that DeNetDM outperforms existing debiasing techniques on both synthetic and real-world datasets by 5\\%. The project page is available at https://vssilpa.github.io/denetdm/.'}",https://openreview.net{'value': '/pdf/9114261717f42179cdab7dfd174c783d97688e8a.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=0aN7VWwp4g,{'value': 'Fourier Amplitude and Correlation Loss: Beyond Using L2 Loss for Skillful Precipitation Nowcasting'},Chiu-Wai Yan; Shi Quan Foo; Van-Hoan Trinh; Dit-Yan Yeung; Ka-Hing Wong; Wai-Kin Wong,~Chiu-Wai_Yan1; ~Shi_Quan_Foo2; ~Van-Hoan_Trinh2; ~Dit-Yan_Yeung2; ~Ka-Hing_Wong1; ~Wai-Kin_Wong1,"{'value': ['Precipitation Nowcasting', 'Video Prediction', 'Fourier Analysis', 'Loss Function']}","{'value': 'Deep learning approaches have been widely adopted for precipitation nowcasting in recent years. Previous studies mainly focus on proposing new model architectures to improve pixel-wise metrics. However, they frequently result in blurry predictions which provide limited utility to forecasting operations. In this work, we propose a new Fourier Amplitude and Correlation Loss (FACL) which consists of two novel loss terms: Fourier Amplitude Loss (FAL) and Fourier Correlation Loss (FCL). FAL regularizes the Fourier amplitude of the model prediction and FCL complements the missing phase information. The two loss terms work together to replace the traditional L2 losses such as MSE and weighted MSE for the spatiotemporal prediction problem on signal-based data. Our method is generic, parameter-free and efficient. Extensive experiments using one synthetic dataset and three radar echo datasets demonstrate that our method improves perceptual metrics and meteorology skill scores, with a small trade-off to pixel-wise accuracy and structural similarity. Moreover, to improve the error margin in meteorological skill scores such as Critical Success Index (CSI) and Fractions Skill Score (FSS), we propose and adopt the Regional Histogram Divergence (RHD), a distance metric that considers the patch-wise similarity between signal-based imagery patterns with tolerance to local transforms.'}",https://openreview.net{'value': '/pdf/83342cf0e9e7532a338787e802069fff91f9936d.pdf'},{'abstract_filter': 'Synthetic'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=0KvYLaTBTE,{'value': 'Latent Plan Transformer for Trajectory Abstraction: Planning as Latent Space Inference'},Deqian Kong; Dehong Xu; Minglu Zhao; Bo Pang; Jianwen Xie; Andrew Lizarraga; Yuhao Huang; Sirui Xie; Ying Nian Wu,~Deqian_Kong1; ~Dehong_Xu1; ~Minglu_Zhao1; ~Bo_Pang4; ~Jianwen_Xie1; ~Andrew_Lizarraga1; ~Yuhao_Huang3; ~Sirui_Xie1; ~Ying_Nian_Wu1,"{'value': ['Generative models', 'Reinforcement learning', 'Decision transformer']}","{'value': 'In tasks aiming for long-term returns, planning becomes essential. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent variable to connect a Transformer- based trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally integrates sub-trajectories to form a consistent abstrac- tion despite the finite context. At test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference. Our experiments demonstrate that LPT can discover improved decisions from sub- optimal trajectories, achieving competitive performance across several benchmarks, including Gym-Mujoco, Franka Kitchen, Maze2D, and Connect Four. It exhibits capabilities in nuanced credit assignments, trajectory stitching, and adaptation to environmental contingencies. These results validate that latent variable inference can be a strong alternative to step-wise reward prompting.'}",https://openreview.net{'value': '/pdf/0b6e118d3320c87fc9990c0f1752d8af260e3055.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=0C3bLHwjsY,{'value': 'Promoting Fairness Among Dynamic Agents in Online-Matching Markets under Known Stationary Arrival Distributions'},Will Ma; Pan Xu,~Will_Ma1; ~Pan_Xu2,"{'value': ['Online Matching', 'Competitive Analysis', 'Fairness Maximization']}","{'value': 'Online (bipartite) matching under known stationary arrivals is a fundamental model that has been  studied extensively under the objective of maximizing the total number of customers served. We instead study the objective of *maximizing the minimum matching rate across all online types*, which is referred to as long-run (individual) fairness. For Online Matching under long-run Fairness (OM-LF) with a single offline agent, we show  that the first-come-first-serve (FCFS) policy is $1$-competitive, i.e., matching any optimal clairvoyant. For the general case of OM-LF: We present a sampling algorithm (SAMP) and show that (1) SAMP is of competitiveness of at least $1-1/e$ and (2) it is asymptotically optimal with competitiveness approaches one in different regimes when either all offline agents have a sufficiently large matching capacity, or all online types have a sufficiently large arrival rate, or highly imbalance between the total offline matching capacity and the number of online arrivals. To complement the competitive results, we show the following hardness results for OM-LF: (1) Any non-rejecting policy (matching every arriving online agent if possible) is no more than $1/2$-competitive; (2) Any (randomized) policy is no more than $(\\sqrt{3}-1)$-competitive; (3) SAMP can be no more than $(1-1/e)$-competitive suggesting the tightness of competitive analysis for SAMP. We stress that all hardness results mentioned here are independent of any benchmarks.  We also consider a few extensions of OM-LF by proposing a few variants of fairness metrics, including long-run group-level fairness and short-run fairness, and we devise related algorithms with provable competitive performance.'}",https://openreview.net{'value': '/pdf/61ecba74434d4fc5596cb759256dd94610bf7414.pdf'},{'title_filter': 'Agent'},NeurIPS.cc,2024,Conference
https://openreview.net/forum?id=08A6X7FSTs,{'value': 'Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text'},Xinyang Li; Zhangyu Lai; Linning Xu; Yansong Qu; Liujuan Cao; Shengchuan Zhang; Bo Dai; Rongrong Ji,~Xinyang_Li1; ~Zhangyu_Lai1; ~Linning_Xu2; ~Yansong_Qu1; ~Liujuan_Cao1; ~Shengchuan_Zhang1; ~Bo_Dai2; ~Rongrong_Ji5,"{'value': ['Text-to-3D Scene Generation', 'Latent Diffusion Models']}","{'value': 'Recent advancements in 3D generation have leveraged synthetic datasets with ground truth 3D assets and predefined camera trajectories. However, the potential of adopting real-world datasets, which can produce significantly more realistic 3D scenes, remains largely unexplored. In this work, we delve into the key challenge of the complex and scene-specific camera trajectories found in real-world captures. We introduce Director3D, a robust open-world text-to-3D generation framework, designed to generate both real-world 3D scenes and adaptive camera trajectories. To achieve this, (1) we first utilize a Trajectory Diffusion Transformer, acting as the \\emph{Cinematographer}, to model the distribution of camera trajectories based on textual descriptions. Next, a Gaussian-driven Multi-view Latent Diffusion Model serves as the \\emph{Decorator}, modeling the image sequence distribution given the camera trajectories and texts. This model, fine-tuned from a 2D diffusion model, directly generates pixel-aligned 3D Gaussians as an immediate 3D scene representation for consistent denoising. Lastly, the 3D Gaussians are further refined by a novel SDS++ loss as the \\emph{Detailer}, which incorporates the prior of the 2D diffusion model. Extensive experiments demonstrate that Director3D outperforms existing methods, offering superior performance in real-world 3D generation.'}",https://openreview.net{'value': '/pdf/7d0e61c2f52b953842ed9561fdffaa8dd330d197.pdf'},{'title_filter': 'Trajectory'},NeurIPS.cc,2024,Conference
