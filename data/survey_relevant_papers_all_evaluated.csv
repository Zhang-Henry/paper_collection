forum,title,abstract,keywords,authors,pdf,conference,year,venue,original_file,gpt_relevant,gpt_confidence,gpt_relevance_score,gpt_reasoning,gpt_key_aspects
https://openreview.net/forum?id=zAdUB0aCTQ,AgentBench: Evaluating LLMs as Agents,"{'value': ""The potential of Large Language Model (LLM) as agents has been widely acknowledged recently.\nThus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments.\nWe present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities.\nOur extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B.\nWe identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.\nImproving instruction following and training on high quality multi-round alignment data could improve agent performance.\nAnd different from existing assumptions, training on code present ambivalent impacts on different agent tasks.\nDatasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.""}","['Large language models', 'Autonomous agents', 'Reasoning', 'Evaluation', 'Benchmark']",,https://openreview.net{'value': '/pdf/6eee0bd1fd98c135372baedb2a5644233a013bb2.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses agents, specifically Large Language Models (LLMs) acting as agents, it does not focus on data synthesis or synthetic data generation techniques. The evaluation of LLMs in interactive environments does not align with the survey's emphasis on agent learning methods that utilize synthetic data for training or performance improvement.","Large Language Models, Autonomous agents, Evaluation, Benchmark"
https://openreview.net/forum?id=z6KS9D1dxt,Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game,"In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist and the corresponding policy weakly dominates previous approaches as time goes to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experiments on matrix game, Level-based Foraging and StarCraft II indicate that, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations, showing resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.","['Multi-agent reinforcement learning', 'Robustness', 'Game Theory', 'Adversarial Attack']",,https://openreview.net{'value': '/pdf/ae82041a50b5244c3cc55d71579b529584e36982.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent reinforcement learning and addresses robustness against adversarial actions, it does not focus on data synthesis, synthetic data generation, or simulated data. The primary emphasis is on the robustness of agents in adversarial settings rather than on utilizing synthetic data for training or improving agent performance.","Multi-agent reinforcement learning, Robustness, Adversarial Attack"
https://openreview.net/forum?id=yoVq2BGQdP,Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning,"{'value': ""Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a  Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach.""}","['Fairness', 'Multi-Agent Reinforcement Learning', 'Markov Decision Process']",,https://openreview.net{'value': '/pdf/832d7ea57a154b15de9382f6f8db49dad34bc05e.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent reinforcement learning and proposes methods for achieving fairness in such systems, it does not explicitly involve data synthesis, synthetic data generation, or simulated data. The focus is primarily on fairness in unknown environments rather than on the use of synthetic data for training agents.","Multi-Agent Reinforcement Learning, Fairness, Markov Decision Process"
https://openreview.net/forum?id=wFWuX1Fhtj,On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning,"Constrained cooperative multi-agent reinforcement learning (MARL) is an emerging learning framework that has been widely applied to manage multi-agent systems, and many primal-dual type algorithms have been developed for it. However, the convergence of primal-dual algorithms crucially relies on strong duality -- a condition that has not been formally proved in constrained cooperative MARL. In this work, we prove that strong duality fails to hold in constrained cooperative MARL, by revealing a nonconvex quadratic type constraint on the occupation measure induced by the product policy. Consequently, our reanalysis of the primal-dual algorithm shows that its convergence rate is hindered by the nonzero duality gap. Then, we propose a decentralized primal approach for constrained cooperative MARL to avoid the duality gap, and our analysis shows that its convergence is hindered by another gap induced by the advantage functions. Moreover, we compare these two types of algorithms via concrete examples, and show that neither of them always outperforms the other one. Our study reveals that constrained cooperative MARL is generally a challenging and highly nonconvex problem, and its fundamental structure is very different from that of single-agent constrained RL.","['Multi-Agent Reinforcement Learning', 'Constrained reinforcement learning', 'Primal-Dual', 'Duality gap', 'Primal algorithm']",,https://openreview.net{'value': '/pdf/fbb1a9eec51b42b14f0f37676f9db8b154cbc9d2.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses multi-agent reinforcement learning (MARL), it does not address data synthesis, synthetic data generation, or simulated data. The focus is on the theoretical aspects of constrained cooperative MARL and the convergence of algorithms, which does not align with the survey's emphasis on data synthesis techniques.","Multi-Agent Reinforcement Learning, Constrained reinforcement learning, Primal-Dual algorithms"
https://openreview.net/forum?id=vZZ4hhniJU,Learning Multi-Agent Communication with Contrastive Learning,"Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.","['Multi-Agent Reinforcement Learning', 'Emergent Communication', 'Contrastive Learning']",,https://openreview.net{'value': '/pdf/2c9dcdc69b3da56a7ebdec7279ee8bc8087b5c39.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent reinforcement learning and communication, it does not explicitly involve data synthesis, synthetic data generation, or simulated data. The focus is on learning communication strategies rather than utilizing synthetic data for training agents.","Multi-Agent Reinforcement Learning, Emergent Communication, Contrastive Learning"
https://openreview.net/forum?id=tmqOhBC4a5,Maximum Entropy Heterogeneous-Agent Reinforcement Learning,"*Multi-agent reinforcement learning* (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning \emph{stochastic} policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose *Heterogeneous-Agent Soft Actor-Critic* (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to *quantal response equilibrium* (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named *Maximum Entropy Heterogeneous-Agent Mirror Learning* (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration.","['cooperative multi-agent reinforcement learning', 'heterogeneous-agent soft actor-critic', 'maximum entropy heterogeneous-agent mirror learning']",,https://openreview.net{'value': '/pdf/82bacc9b0a9551bf4922e43270f4c315044f70af.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent reinforcement learning and proposes a new algorithm for improving sample efficiency and robustness, it does not explicitly address data synthesis, synthetic data generation, or the use of simulated environments. The focus is primarily on algorithmic improvements rather than on the synthesis of data for training agents.","multi-agent reinforcement learning, sample efficiency, algorithmic improvements"
https://openreview.net/forum?id=s9z0HzWJJp,SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series,"{'value': 'We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called ""hyperportfolio"", that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to ""invest"". We also propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis & Proof prompting for producing in-depth analyses on input news, articles, etc. to assist decision-making. We perform experiments and ablation studies to explore the factors that impact performance. The results show that our proposed method achieves improvements of 32.4% and 30.4% compared to the state-of-the-art method in the two experimental settings.'}","['Large Langauge Models', 'Agent', 'Prompt Tunning', 'Time series forcasting']",,https://openreview.net{'value': '/pdf/473beb76f232cb31d5e4d7594a17375a1347cef9.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses autonomous agents and their decision-making capabilities, it does not explicitly focus on data synthesis or synthetic data generation techniques. The emphasis is on real-world data sources and time series analysis rather than synthetic data for training agents. Therefore, it does not align closely with the survey's focus on agent learning with data synthesis.","autonomous agents, real-world data, time series analysis"
https://openreview.net/forum?id=pAoqRlTBtY,Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning,"{'value': ""Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.""}","['Causal Reasoning', 'Causal Discovery', 'Structural Causal Models', 'Large Language Models']",,https://openreview.net{'value': '/pdf/62fc3766e10c6f5fa2f2a9b44b46098519f89596.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses a novel framework involving agents for causal discovery, it does not focus on agent learning methods that utilize data synthesis or synthetic data generation techniques. The primary emphasis is on causal reasoning and metadata integration rather than on the specific applications of synthetic data in agent training or performance improvement.",
https://openreview.net/forum?id=oQKKlzxV1o,Online Information Acquisition: Hiring Multiple Agents,"{'value': ""We investigate the mechanism design problem faced by a principal who hires \\emph{multiple} agents to gather and report costly information. Then, the principal exploits the  information to make an informed decision.  We model this problem as a game, where the principal announces a mechanism consisting in action recommendations and a payment function, a.k.a. scoring rule. Then, each agent chooses an effort level and receives partial information about an underlying state of nature based on the effort. Finally, the agents report the information (possibly non-truthfully), the principal takes a decision based on this information, and the agents are paid according to the scoring rule. While previous work focuses on single-agent problems, we consider multi-agents settings. This poses the challenge of coordinating the agents' efforts and aggregating correlated information. Indeed, we show that optimal mechanisms must correlate agents' efforts, which introduces externalities among the agents, and hence complex incentive compatibility constraints and equilibrium selection problems. First, we design a polynomial-time algorithm to find an optimal incentive compatible mechanism. Then, we study an online problem, where the principal repeatedly interacts with a group of unknown agents. We design a no-regret algorithm that provides $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret with respect to an optimal mechanism, matching the state-of-the-art bound for single-agent settings.""}","['online learning', 'information acquisition', 'mechanism design']",,https://openreview.net{'value': '/pdf/83fcb8ac23e715a1b18621cd00f0a8a7d023dc7a.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses a multi-agent system and the coordination of agents, it primarily focuses on mechanism design and information acquisition rather than on agent learning methods that utilize data synthesis or synthetic data generation. There is no clear connection to synthetic data or its application in training agents.","multi-agent systems, mechanism design, information acquisition"
https://openreview.net/forum?id=oKn9c6ytLx,WebArena: A Realistic Web Environment for Building Autonomous Agents,"With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting.  The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \ours can be used to measure such progress.\footnote{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.}",['language-guided agents; web automation; benchmark'],,https://openreview.net{'value': '/pdf/384c500abb2b5adc4f3c40956a267477925c4b94.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses autonomous agents and their performance in a realistic web environment, it does not focus on data synthesis or synthetic data generation techniques. The emphasis is on creating a realistic environment rather than utilizing synthetic data for training or improving agent performance, which is a core aspect of the survey topic.","autonomous agents, realistic environment, benchmark tasks"
https://openreview.net/forum?id=o7qhUMylLU,Sample-Efficient Multi-Agent RL: An Optimization Perspective,"We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under general function approximation. 
    In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.","['multi-agent reinforcement learning', 'theory', 'general function approximation', 'general-sum Markov Games']",,https://openreview.net{'value': '/pdf/fe5b0154a5ddbf71df6a0cd9077a345a6fa6065c.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent reinforcement learning (MARL) and introduces a novel algorithmic framework for sample-efficient learning, it does not explicitly address data synthesis, synthetic data generation, or the use of simulated environments. The focus is primarily on theoretical aspects and optimization rather than on the synthesis of data for training agents.","multi-agent reinforcement learning, sample efficiency, theoretical framework"
https://openreview.net/forum?id=mM7VurbA4r,SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents,"{'value': ""*Humans are social beings*; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and *interact* under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.""}","['Social', 'Interaction', 'Agent', 'Social intelligence', 'Large Language Models', 'Evaluation', 'Theory of Mind']",,https://openreview.net{'value': '/pdf/6aece1f9088fc415196df5830f1ca62e6dbf37d3.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses agents and their interactions in a social context, it does not focus on data synthesis or synthetic data generation techniques. The primary emphasis is on evaluating social intelligence rather than on learning from synthetic data or using synthetic data for training agents.","Social intelligence, Agent interactions, Evaluation framework"
https://openreview.net/forum?id=hNhwSmtXRh,Lemur: Harmonizing Natural Language and Code for Language Agents,"We introduce Lemur and Lemur-Chat, openly accessible language models optimized
for both natural language and coding capabilities to serve as the backbone
of versatile language agents. The evolution from language chat models to
functional language agents demands that models not only master human interaction,
reasoning, and planning but also ensure grounding in the relevant environments.
This calls for a harmonious blend of language and coding capabilities
in the models. Lemur and Lemur-Chat are proposed to address this necessity,
demonstrating balanced proficiencies in both domains, unlike existing
open-source models that tend to specialize in either. Through meticulous pretraining
using a code-intensive corpus and instruction fine-tuning on text and code
data, our models achieve state-of-the-art averaged performance across diverse
text and coding benchmarks. Comprehensive experiments demonstrate Lemur’s
superiority over existing open-source models and its proficiency across various
agent tasks involving human communication, tool usage, and interaction under
fully- and partially- observable environments. The harmonization between natural
and programming languages enables Lemur-Chat to significantly narrow the
gap with proprietary models on agent abilities, providing key insights into developing
advanced open-source agents adept at reasoning, planning, and operating
seamlessly across environments. Our model and code have been open-sourced at
https://github.com/OpenLemur/Lemur.","['large language model', 'agent', 'code generation', 'reasoning', 'decision making']",,https://openreview.net{'value': '/pdf/8ef62990871ebf2cac77dc6ea498085f167f070a.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses language agents and their capabilities, it does not focus on data synthesis or synthetic data generation techniques. The emphasis is on harmonizing natural language and coding for agent tasks rather than on how synthetic data is used to enhance agent learning. Therefore, it does not align closely with the survey's scope.",
https://openreview.net/forum?id=Z59Rb5bPPP,Trajeglish: Traffic Modeling as Next-Token Prediction,"A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.","['self-driving', 'traffic modeling', 'autonomous vehicles', 'simulation', 'motion prediction', 'transformer']",,https://openreview.net{'value': '/pdf/6a1754e5443464338e361380403b4fec2f5d94b2.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,True,0.85,8,"The paper discusses traffic modeling for self-driving vehicles, which involves multi-agent interactions and the generation of synthetic scenarios for training autonomous agents. It utilizes a data-driven approach to synthesize realistic driving scenarios, aligning well with the survey's focus on agent learning with synthetic data.","multi-agent systems, synthetic scenario generation, autonomous agents, data-driven modeling, improving agent performance"
https://openreview.net/forum?id=W3VsHuga3j,Modeling Boundedly Rational Agents with Latent Inference Budgets,"We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating constrained inference. In this work, we introduce a latent inference budget model (L-IBM) that models these constraints explicitly, via a latent variable (inferred jointly with a model of agents’ goals) that controls the runtime of an iterative inference algorithm. L-IBMs make it possible to learn agent models using data from diverse populations of suboptimal actors. In three modeling tasks—inferring navigation goals from routes, inferring communicative intents from human utterances, and predicting next moves in human chess games—we show that L-IBMs match or outperforms Boltzmann models of decision-making under uncertainty. Moreover, the inferred inference budgets are themselves meaningful, efficient to compute, and correlated with measures of player skill, partner skill and task difficulty.","['neurosymbolic', 'planning', 'rationality']",,https://openreview.net{'value': '/pdf/b6fcabc7a4521fa9b8ff72f6d09fbafc1e118951.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses modeling agents and their decision-making processes, it does not explicitly involve data synthesis or synthetic data generation techniques. The focus is on bounded rationality and inference budgets rather than on training agents with synthetic data or improving agent performance through data synthesis.","bounded rationality, agent modeling, inference budgets"
https://openreview.net/forum?id=VtmBAGCN7o,MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework,"Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.","['Autonomous Agent', 'Meta Programming', 'Multi-Agent Society', 'Group Intelligence']",,https://openreview.net{'value': '/pdf/474fc6dad3bd9bf7fdb97c7cd72b2cc0649a9647.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses a multi-agent system and the collaboration of agents, it does not focus on data synthesis, synthetic data generation, or the use of simulated environments for training agents. The emphasis is on improving workflows and coherence in task execution rather than on learning from synthetic data or enhancing agent performance through data synthesis techniques.","Multi-Agent Society, Collaboration, Task Decomposition"
https://openreview.net/forum?id=UBVNwD3hPN,CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents,"The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization’s profound alignment with human society requires sophisticated learning and prior knowledge, while its ever-changing space and action space demand robust reasoning for generalization. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. Overall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.","['Interactive Environments', 'Benchmark', 'Reinforcement Learning', 'Language Agent', 'Multi-agent']",,https://openreview.net{'value': '/pdf/8e9b4884f752883a17e58942608d9cc769610a01.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses decision-making agents and their learning and reasoning capabilities in a complex environment, it does not explicitly focus on data synthesis or synthetic data generation techniques. The emphasis is more on the learning and reasoning aspects of agents rather than on utilizing synthetic data for training or performance improvement.","Decision-making agents, Reinforcement learning, Multi-agent systems"
https://openreview.net/forum?id=Ts95eXsPBc,Spatially-Aware Transformers for Embodied Agents,"Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning. The source code for our models and experiments will be available at \href{https://github.com/spatially_aware_transformer}{https://github.com/spatially_aware_transformer}.","['Episodic Memory', 'Spatial Inference', 'Prediction', 'Generation', 'Reinforcement Learning']",,https://openreview.net{'value': '/pdf/1dc0ff45874e872749597c38bcfc2df0fa7ed0d6.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses reinforcement learning and episodic memory in the context of agents, it does not focus on data synthesis or synthetic data generation techniques. The primary emphasis is on improving memory utilization and spatial awareness rather than on utilizing synthetic data for training agents.","Reinforcement Learning, Episodic Memory, Spatial Awareness"
https://openreview.net/forum?id=SBoRhRCzM3,THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS,"Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. 
However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}.
To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs.
These analogous problems are related to the input one, with reusable solutions and problem-solving strategies.
Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. 
To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. 
Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch.
TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. 
Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\% improvement of human preference in Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent Planning.","['LLM Complex Reasoning', 'Language Model Reasoning']",,https://openreview.net{'value': '/pdf/1cddcaa3f523f7ad1c9aea87d5180f2d173e4bb8.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.9,2,"The paper focuses on enhancing reasoning capabilities of Large Language Models (LLMs) through an approach called Thought Propagation, which does not involve agent learning methods or synthetic data generation techniques relevant to the survey scope. While it discusses problem-solving strategies, it does not address reinforcement learning, multi-agent systems, or data synthesis in the context of agent training.",
https://openreview.net/forum?id=S2oTVrlcp3,SmartPlay : A Benchmark for LLMs as Intelligent Agents,"{'value': ""Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately.\nSmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. \nWe release our benchmark at https://github.com/microsoft/SmartPlay""}",['Large Language Models'],,https://openreview.net{'value': '/pdf/f84669704e333d9a0d57f5fd1dfd011b03109021.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses intelligent agents and evaluates their capabilities through a benchmark, it does not focus on data synthesis or synthetic data generation techniques. The primary emphasis is on evaluating large language models (LLMs) as agents rather than on methods that utilize synthetic data for training or improving agent performance.","intelligent agents, benchmarking, evaluation of capabilities"
https://openreview.net/forum?id=Qox9rO0kN0,Learning Multi-Agent Communication from Graph Modeling Perspective,"In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.","['communication learning', 'multi-agent reinforcement learning']",,https://openreview.net{'value': '/pdf/b18ae005fc01451473ec4301d9802e5267831053.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent systems and communication learning, it does not explicitly involve data synthesis, synthetic data generation, or simulated data. The focus is on optimizing communication among agents rather than on the use of synthetic data for training or improving agent performance.","multi-agent systems, communication learning, cooperative tasks"
https://openreview.net/forum?id=NltzxpG0nz,Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds,"{'value': ""Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks and carry out experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. The project’s website and code can be found at https://sites.google.com/view/steve-eye.""}","['large multimodal pre-training', 'open-world embodied agent', 'large language model']",,https://openreview.net{'value': '/pdf/a46ce05ff44a87fada32b918f094df26df3d2f07.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses embodied agents and their interaction with the environment, it primarily focuses on integrating large language models with visual perception rather than on agent learning methods that utilize data synthesis techniques. The dataset mentioned is collected semi-automatically, but it does not explicitly involve synthetic data generation or augmentation techniques relevant to the survey topic.","embodied agents, visual perception, large language models"
https://openreview.net/forum?id=MCNqgUFTHI,Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents,"Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.","['Dialogue Policy Planning', 'Proactive Dialogue', 'Large Language Model']",,https://openreview.net{'value': '/pdf/30c7e22635f23140a4f37883885b6e80b5f7c750.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses a dialogue agent and involves reinforcement learning from goal-oriented AI feedback, it primarily focuses on dialogue policy planning and does not explicitly address data synthesis or synthetic data generation techniques. The connection to synthetic data is weak, as the self-play simulation mentioned does not clearly align with the survey's focus on synthetic data for training agents.","Dialogue Policy Planning, Reinforcement Learning, Large Language Model"
https://openreview.net/forum?id=M6XWoEdmwf,AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents,"{'value': ""We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments.""}","['Meta-RL', 'Generalization', 'Long-Term Memory', 'Transformers']",,https://openreview.net{'value': '/pdf/6ffd1eb5dc0bd2b144d5d0309763b3ed5e114e8b.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses an RL agent (AMAGO) and addresses challenges in reinforcement learning, it does not explicitly mention the use of synthetic data or data synthesis techniques. The focus is more on improving the agent's learning capabilities and handling sparse rewards rather than utilizing synthetic data for training or performance enhancement.","Reinforcement Learning, Meta-RL, Generalization, Long-Term Memory"
https://openreview.net/forum?id=LjivA1SLZ6,Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning,"In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.","['Multi-agent reinforcement learning', 'episodic control', 'episodic incentive', 'state embedding']",,https://openreview.net{'value': '/pdf/8b2d5ac5539754d00bf99458a60c63157c74fbdb.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses cooperative multi-agent reinforcement learning and introduces a method to improve learning efficiency, it does not explicitly involve data synthesis or synthetic data generation techniques. The focus is on memory utilization and reward structures rather than on generating or using synthetic data for training agents.","multi-agent reinforcement learning, episodic memory, reward structure"
https://openreview.net/forum?id=KOZu91CzbK,Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization,"Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.","['Language Agent', 'AI Agent', 'Reinforcement Learning']",,https://openreview.net{'value': '/pdf/d02f39256b41ec50ca8cd3a5b136065bc7a4caae.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses reinforcement learning in the context of language agents and involves learning from environment feedback, it does not explicitly address data synthesis or synthetic data generation techniques. The focus is more on optimizing language agents through policy gradient methods rather than on the synthesis of data for training agents.","Reinforcement Learning, Language Agents, Policy Gradient Optimization"
https://openreview.net/forum?id=GEcwtMk1uA,Identifying the Risks of LM Agents with an LM-Emulated Sandbox,"Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes toolkits and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.","['Language Model Agent', 'Tool Use', 'Evaluation', 'Safety', 'Language Model']",,https://openreview.net{'value': '/pdf/d1601f78407737fc216de9e6ec0085038f8c885f.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses Language Model agents and their evaluation in a sandbox environment, it does not focus on data synthesis or synthetic data generation techniques relevant to agent learning. The primary emphasis is on risk identification and safety evaluation rather than on methods of training agents using synthetic data.",
https://openreview.net/forum?id=FQepisCUWu,ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,"{'value': ""Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies.\nIn this paper, we construct a multi-agent referee team called $\\textbf{ChatEval}$ to autonomously discuss and evaluate the quality of different texts. \nOur experiments on two benchmarks illustrate that ChatEval delivers superior accuracy and correlation in alignment with human assessment. Furthermore, we find that the diverse role prompts (different personas) are essential in the multi-agent debate process; that is, utilizing the same role description in the prompts can lead to a degradation in performance. Our qualitative analysis also shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments.""}","['Large Language Model', 'Multi-Agent Debate', 'LLM evaluators']",,https://openreview.net{'value': '/pdf/6950b70dde4f8d7425424dce9cc4063f505b2194.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses a multi-agent system (ChatEval) for text evaluation, it does not focus on agent learning methods that utilize data synthesis techniques. The primary focus is on evaluating text quality rather than training agents with synthetic data or improving agent performance through data synthesis. There is no clear connection to the specific themes of agent learning with synthetic data generation or augmentation techniques.",
https://openreview.net/forum?id=EpVe8jAjdx,Privileged Sensing Scaffolds Reinforcement Learning,"We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon “sensory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose “Scaffolder”, a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new “S3” suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/","['reinforcement learning', 'model-based reinforcement learning', 'world models', 'robotics', 'privileged information', 'asymmetric learning', 'multimodality', 'perception', 'sensing']",,https://openreview.net{'value': '/pdf/8ab7ec56b6e56ac45ce2b68e95b277283dbb8377.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses reinforcement learning agents and their training with privileged sensing, it does not explicitly focus on data synthesis or synthetic data generation techniques. The emphasis is more on leveraging additional sensory information during training rather than generating synthetic data or using simulated environments for training agents. Therefore, it does not align closely with the survey's focus on agent learning with data synthesis.","reinforcement learning, privileged sensing, robotics, training-time access, auxiliary components"
https://openreview.net/forum?id=Ep0TtjVoap,ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving,"{'value': ""Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.""}","['Large Language Models', 'Mathematical Reasoning', 'Tool Learning']",,https://openreview.net{'value': '/pdf/2b0b45b11d0f61912efb1a932fb494d36f7b88e6.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses a reasoning agent (ToRA) that utilizes tool integration for mathematical problem solving, it does not focus on synthetic data generation or data synthesis techniques relevant to agent learning. The primary emphasis is on tool interaction and imitation learning rather than on the use of synthetic data for training agents in the context outlined in the survey.",
https://openreview.net/forum?id=EnXJfQqy0K,Building Cooperative Embodied Agents Modularly with Large Language Models,"In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.","['Large Language Models', 'Embodied Intelligence', 'Multi-Agent Cooperation', 'Human-AI Interaction', 'Communication']",,https://openreview.net{'value': '/pdf/9baa34313f1aea292b16e30697c20766afadddb1.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent cooperation and the use of large language models in agent learning, it does not explicitly focus on data synthesis or synthetic data generation techniques. The emphasis is more on communication and cooperation rather than on the synthesis of data for training agents.","Multi-Agent Cooperation, Large Language Models, Embodied Agents, Human-Agent Interaction"
https://openreview.net/forum?id=EHg5GDnyq1,AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors,"Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework AgentVerse that can effectively orchestrate a collaborative group of expert agents as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that AgentVerse can proficiently deploy multi-agent groups that outperform a single agent. Extensive experiments on text understanding, reasoning, coding, tool utilization, and embodied AI confirm the effectiveness of AgentVerse. Moreover, our analysis of agent interactions within AgentVerse reveals the emergence of specific collaborative behaviors, contributing to heightened group efficiency. We will release our codebase, AgentVerse, to further facilitate multi-agent research.","['large language mode', 'agent', 'multi-agent']",,https://openreview.net{'value': '/pdf/7c9bd9a841a2ba0c11ea97abb5e982430c2fc95e.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent systems and collaboration among agents, it does not explicitly address data synthesis, synthetic data generation, or the use of simulated environments for training. The focus is more on the emergent behaviors and collaboration rather than on learning from synthetic data or enhancing agent performance through data synthesis techniques.","multi-agent systems, collaborative behaviors, agent interactions"
https://openreview.net/forum?id=BqEvdOS1Hs,Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain,"{'value': 'Existing game AI research mainly focuses on enhancing agents\' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading to poor experiences for their human partners. In other words, most game AI agents are modeled in a ""self-centered"" manner. In this paper, we propose a ""human-centered"" modeling scheme for collaborative agents that aims to enhance the experience of humans. Specifically, we model the experience of humans as the goals they expect to achieve during the task. We expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents\' original abilities (e.g., winning games). To achieve this, we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG approach introduces a ""baseline"", which corresponds to the extent to which humans primitively achieve their goals, and encourages agents to learn behaviors that can effectively enhance humans in achieving their goals better. We evaluate the RLHG agent in the popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both objective performance and subjective preference results show that the RLHG agent provides participants better gaming experience.'}","['human enhancement', 'human-agent collaboration', 'game playing', 'deep reinforcement learning']",,https://openreview.net{'value': '/pdf/6ce678ca6108e4a85517e2526c83d7d968f7108b.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses a reinforcement learning approach for enhancing human-agent collaboration, it does not focus on data synthesis or synthetic data generation. The primary emphasis is on improving the human experience in collaboration rather than utilizing synthetic data for training agents. Therefore, it does not align with the survey's scope.",
https://openreview.net/forum?id=ADSxCpCu9s,LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents,"Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.","['task planning', 'language models', 'benchmarking', 'embodied agents', 'home robots']",,https://openreview.net{'value': '/pdf/35abce446ca9b6e7a9136f7c38556084c63538ec.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses embodied agents and task planning, it primarily focuses on benchmarking language-oriented task planners rather than on agent learning methods that utilize data synthesis techniques. There is no clear emphasis on synthetic data generation or data synthesis in the context of training agents.","embodied agents, task planning, benchmarking, language models"
https://openreview.net/forum?id=9JQtrumvg8,"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis","Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.
However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.
We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.
WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.
We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.
We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.","['Web Navigation', 'Web Automation', 'Large Language Models', 'Language Model Agents', 'Tool Use', 'Program Synthesis']",,https://openreview.net{'value': '/pdf/0b27823f96e3efd0ed6921aafc4fe4643d1aeec5.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses an agent (WebAgent) that learns from self-experience and utilizes planning and program synthesis, it does not explicitly focus on data synthesis or synthetic data generation techniques. The emphasis is on improving performance on real-world tasks rather than on the use of synthetic data for training or enhancing agent learning. Therefore, it does not align closely with the survey's scope.","agent learning, self-experience, planning, program synthesis"
https://openreview.net/forum?id=8HCARN2hhw,Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction,"Agents navigating in 3D environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly defined interpretation, whereas classical robotics addresses this with scene reconstruction resulting in some form of map, usually estimated with geometry and sensor models and/or learning. In this work we propose to learn an actionable representation of the scene independently of the targeted downstream task and without explicitly optimizing reconstruction. The learned representation is optimized by a blind auxiliary agent trained to navigate with it on multiple short sub episodes branching out from a waypoint and, most importantly, without any direct visual observation. We argue and show that the blindness property is important and forces the (trained) latent representation to be the only means for planning. With probing experiments we show that the learned representation optimizes navigability and not reconstruction. On downstream tasks we show that it is robust to changes in distribution, in particular the sim2real gap, which we evaluate with a real physical robot in a real office building, significantly improving performance.","['Navigation', 'Embodied AI', 'Perception']",,https://openreview.net{'value': '/pdf/2b5b56f45f498f5877f3de2ea54e150352e0a408.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses agents and their navigation capabilities, it does not explicitly involve data synthesis or synthetic data generation techniques. The focus is on learning representations for navigation without reconstruction, which does not align with the survey's emphasis on synthetic data methods.","Agent navigation, Latent representations, Sim2real gap"
https://openreview.net/forum?id=7M0EzjugaN,Online Continual Learning for Interactive Instruction Following Agents,"In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic, since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous ‘data prior’ based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed challenging Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior arts in our empirical validations by noticeable margins.","['Embodied AI', 'Continual Learning']",,https://openreview.net{'value': '/pdf/9bd1847dd40bdb80227f6ddd6480e1eee45a4999.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.7,3,"While the paper discusses learning in embodied agents and continual learning setups, it does not explicitly involve data synthesis, synthetic data generation, or simulated environments. The focus is on learning new behaviors and environments without the use of synthetic data, which does not align with the survey's scope.","Embodied agents, Continual learning, Behavior Incremental Learning, Environment Incremental Learning"
https://openreview.net/forum?id=22pyNMuIoa,PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization,"{'value': ""Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts.""}","['Large Language Models', 'Expert-level Prompt Optimization', 'Strategic Planning']",,https://openreview.net{'value': '/pdf/d4dbdee0d105cd3020bffdc2f56d99c33429d49c.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"The paper focuses on prompt optimization for large language models and does not involve reinforcement learning agents, multi-agent systems, or synthetic data generation in the context of agent learning. While it discusses strategic planning and optimization, it does not align with the survey's focus on agent learning with data synthesis techniques.",
https://openreview.net/forum?id=zAdUB0aCTQ,AgentBench: Evaluating LLMs as Agents,"{'value': ""The potential of Large Language Model (LLM) as agents has been widely acknowledged recently.\nThus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments.\nWe present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities.\nOur extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B.\nWe identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.\nImproving instruction following and training on high quality multi-round alignment data could improve agent performance.\nAnd different from existing assumptions, training on code present ambivalent impacts on different agent tasks.\nDatasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.""}","['Large language models', 'Autonomous agents', 'Reasoning', 'Evaluation', 'Benchmark']",,https://openreview.net{'value': '/pdf/6eee0bd1fd98c135372baedb2a5644233a013bb2.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses agents, specifically Large Language Models (LLMs) acting as agents, it does not focus on data synthesis or synthetic data generation techniques. The evaluation of LLMs in interactive environments does not align with the survey's emphasis on agent learning methods that utilize synthetic data for training or improvement.","Large Language Models, Autonomous agents, Evaluation, Benchmark"
https://openreview.net/forum?id=z6KS9D1dxt,Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game,"In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist and the corresponding policy weakly dominates previous approaches as time goes to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experiments on matrix game, Level-based Foraging and StarCraft II indicate that, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations, showing resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.","['Multi-agent reinforcement learning', 'Robustness', 'Game Theory', 'Adversarial Attack']",,https://openreview.net{'value': '/pdf/ae82041a50b5244c3cc55d71579b529584e36982.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent reinforcement learning and addresses robustness against adversarial actions, it does not focus on data synthesis or synthetic data generation techniques. The primary emphasis is on the robustness of agents in adversarial settings rather than on utilizing synthetic data for training or improving agent performance.","Multi-agent reinforcement learning, Robustness, Adversarial Attack"
https://openreview.net/forum?id=yoVq2BGQdP,Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning,"{'value': ""Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a  Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach.""}","['Fairness', 'Multi-Agent Reinforcement Learning', 'Markov Decision Process']",,https://openreview.net{'value': '/pdf/832d7ea57a154b15de9382f6f8db49dad34bc05e.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent reinforcement learning and proposes methods for achieving fairness in multi-agent systems, it does not explicitly involve data synthesis, synthetic data generation, or simulated data. The focus is primarily on fairness in unknown environments rather than on the use of synthetic data for training agents.","Multi-Agent Reinforcement Learning, Fairness in MDPs, Unknown Environments"
https://openreview.net/forum?id=wFWuX1Fhtj,On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning,"Constrained cooperative multi-agent reinforcement learning (MARL) is an emerging learning framework that has been widely applied to manage multi-agent systems, and many primal-dual type algorithms have been developed for it. However, the convergence of primal-dual algorithms crucially relies on strong duality -- a condition that has not been formally proved in constrained cooperative MARL. In this work, we prove that strong duality fails to hold in constrained cooperative MARL, by revealing a nonconvex quadratic type constraint on the occupation measure induced by the product policy. Consequently, our reanalysis of the primal-dual algorithm shows that its convergence rate is hindered by the nonzero duality gap. Then, we propose a decentralized primal approach for constrained cooperative MARL to avoid the duality gap, and our analysis shows that its convergence is hindered by another gap induced by the advantage functions. Moreover, we compare these two types of algorithms via concrete examples, and show that neither of them always outperforms the other one. Our study reveals that constrained cooperative MARL is generally a challenging and highly nonconvex problem, and its fundamental structure is very different from that of single-agent constrained RL.","['Multi-Agent Reinforcement Learning', 'Constrained reinforcement learning', 'Primal-Dual', 'Duality gap', 'Primal algorithm']",,https://openreview.net{'value': '/pdf/fbb1a9eec51b42b14f0f37676f9db8b154cbc9d2.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses multi-agent reinforcement learning (MARL), it focuses on the theoretical aspects of constrained cooperative MARL and the convergence of algorithms rather than on data synthesis or synthetic data generation. There is no mention of synthetic data or its application in training agents, which is a key requirement for inclusion in the survey.","Multi-Agent Reinforcement Learning, Constrained reinforcement learning, Theoretical analysis of algorithms"
https://openreview.net/forum?id=vZZ4hhniJU,Learning Multi-Agent Communication with Contrastive Learning,"Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.","['Multi-Agent Reinforcement Learning', 'Emergent Communication', 'Contrastive Learning']",,https://openreview.net{'value': '/pdf/2c9dcdc69b3da56a7ebdec7279ee8bc8087b5c39.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent reinforcement learning and communication, it does not explicitly involve data synthesis or synthetic data generation techniques. The focus is on learning communication strategies rather than utilizing synthetic data for training agents.","Multi-Agent Reinforcement Learning, Emergent Communication, Contrastive Learning"
https://openreview.net/forum?id=tmqOhBC4a5,Maximum Entropy Heterogeneous-Agent Reinforcement Learning,"*Multi-agent reinforcement learning* (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning \emph{stochastic} policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose *Heterogeneous-Agent Soft Actor-Critic* (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to *quantal response equilibrium* (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named *Maximum Entropy Heterogeneous-Agent Mirror Learning* (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration.","['cooperative multi-agent reinforcement learning', 'heterogeneous-agent soft actor-critic', 'maximum entropy heterogeneous-agent mirror learning']",,https://openreview.net{'value': '/pdf/82bacc9b0a9551bf4922e43270f4c315044f70af.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent reinforcement learning and proposes a new algorithm for improving agent performance, it does not explicitly address data synthesis, synthetic data generation, or the use of simulated environments. The focus is primarily on algorithmic improvements and theoretical guarantees rather than on the synthesis of data for training agents.","multi-agent reinforcement learning, maximum entropy framework, algorithmic design"
https://openreview.net/forum?id=s9z0HzWJJp,SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series,"{'value': 'We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called ""hyperportfolio"", that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to ""invest"". We also propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis & Proof prompting for producing in-depth analyses on input news, articles, etc. to assist decision-making. We perform experiments and ablation studies to explore the factors that impact performance. The results show that our proposed method achieves improvements of 32.4% and 30.4% compared to the state-of-the-art method in the two experimental settings.'}","['Large Langauge Models', 'Agent', 'Prompt Tunning', 'Time series forcasting']",,https://openreview.net{'value': '/pdf/473beb76f232cb31d5e4d7594a17375a1347cef9.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses autonomous agents and their decision-making capabilities, it does not explicitly focus on data synthesis or synthetic data generation techniques. The emphasis is on real-world data sources and time series analysis rather than synthetic data for training agents. Therefore, it does not align closely with the survey's focus on agent learning with data synthesis.","autonomous agents, real-world data, time series analysis"
https://openreview.net/forum?id=pAoqRlTBtY,Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning,"{'value': ""Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.""}","['Causal Reasoning', 'Causal Discovery', 'Structural Causal Models', 'Large Language Models']",,https://openreview.net{'value': '/pdf/62fc3766e10c6f5fa2f2a9b44b46098519f89596.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses a novel framework involving agents for causal discovery, it does not focus on agent learning methods that utilize data synthesis techniques. The emphasis is on causal reasoning and metadata integration rather than on synthetic data generation or training agents with synthetic data.",
https://openreview.net/forum?id=oQKKlzxV1o,Online Information Acquisition: Hiring Multiple Agents,"{'value': ""We investigate the mechanism design problem faced by a principal who hires \\emph{multiple} agents to gather and report costly information. Then, the principal exploits the  information to make an informed decision.  We model this problem as a game, where the principal announces a mechanism consisting in action recommendations and a payment function, a.k.a. scoring rule. Then, each agent chooses an effort level and receives partial information about an underlying state of nature based on the effort. Finally, the agents report the information (possibly non-truthfully), the principal takes a decision based on this information, and the agents are paid according to the scoring rule. While previous work focuses on single-agent problems, we consider multi-agents settings. This poses the challenge of coordinating the agents' efforts and aggregating correlated information. Indeed, we show that optimal mechanisms must correlate agents' efforts, which introduces externalities among the agents, and hence complex incentive compatibility constraints and equilibrium selection problems. First, we design a polynomial-time algorithm to find an optimal incentive compatible mechanism. Then, we study an online problem, where the principal repeatedly interacts with a group of unknown agents. We design a no-regret algorithm that provides $\\widetilde{\\mathcal{O}}(T^{2/3})$ regret with respect to an optimal mechanism, matching the state-of-the-art bound for single-agent settings.""}","['online learning', 'information acquisition', 'mechanism design']",,https://openreview.net{'value': '/pdf/83fcb8ac23e715a1b18621cd00f0a8a7d023dc7a.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.9,3,"While the paper discusses multi-agent systems and the coordination of agents, it primarily focuses on mechanism design and information acquisition rather than on agent learning methods that utilize data synthesis techniques. There is no mention of synthetic data generation or its application in training agents, which is a key aspect of the survey topic.","multi-agent systems, information acquisition, mechanism design"
https://openreview.net/forum?id=oKn9c6ytLx,WebArena: A Realistic Web Environment for Building Autonomous Agents,"With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting.  The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \ours can be used to measure such progress.\footnote{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.}",['language-guided agents; web automation; benchmark'],,https://openreview.net{'value': '/pdf/384c500abb2b5adc4f3c40956a267477925c4b94.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses autonomous agents and their performance in a realistic web environment, it does not focus on data synthesis or synthetic data generation techniques. The emphasis is on creating a realistic environment rather than utilizing synthetic data for training or improving agent performance, which is a key aspect of the survey topic.","autonomous agents, realistic environment, benchmark tasks"
https://openreview.net/forum?id=o7qhUMylLU,Sample-Efficient Multi-Agent RL: An Optimization Perspective,"We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under general function approximation. 
    In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.","['multi-agent reinforcement learning', 'theory', 'general function approximation', 'general-sum Markov Games']",,https://openreview.net{'value': '/pdf/fe5b0154a5ddbf71df6a0cd9077a345a6fa6065c.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent reinforcement learning (MARL) and introduces a framework for sample-efficient learning, it does not explicitly address data synthesis, synthetic data generation, or the use of simulated data. The focus is primarily on theoretical aspects and optimization rather than on the synthesis of data for training agents.","multi-agent reinforcement learning, sample efficiency, theoretical framework"
https://openreview.net/forum?id=mM7VurbA4r,SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents,"{'value': ""*Humans are social beings*; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and *interact* under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.""}","['Social', 'Interaction', 'Agent', 'Social intelligence', 'Large Language Models', 'Evaluation', 'Theory of Mind']",,https://openreview.net{'value': '/pdf/6aece1f9088fc415196df5830f1ca62e6dbf37d3.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses agents and their interactions in a social context, it does not focus on data synthesis or synthetic data generation techniques. The primary emphasis is on evaluating social intelligence rather than on methods of training agents with synthetic data or improving performance through data synthesis.","Social intelligence, Agent interaction, Evaluation framework"
https://openreview.net/forum?id=hNhwSmtXRh,Lemur: Harmonizing Natural Language and Code for Language Agents,"We introduce Lemur and Lemur-Chat, openly accessible language models optimized
for both natural language and coding capabilities to serve as the backbone
of versatile language agents. The evolution from language chat models to
functional language agents demands that models not only master human interaction,
reasoning, and planning but also ensure grounding in the relevant environments.
This calls for a harmonious blend of language and coding capabilities
in the models. Lemur and Lemur-Chat are proposed to address this necessity,
demonstrating balanced proficiencies in both domains, unlike existing
open-source models that tend to specialize in either. Through meticulous pretraining
using a code-intensive corpus and instruction fine-tuning on text and code
data, our models achieve state-of-the-art averaged performance across diverse
text and coding benchmarks. Comprehensive experiments demonstrate Lemur’s
superiority over existing open-source models and its proficiency across various
agent tasks involving human communication, tool usage, and interaction under
fully- and partially- observable environments. The harmonization between natural
and programming languages enables Lemur-Chat to significantly narrow the
gap with proprietary models on agent abilities, providing key insights into developing
advanced open-source agents adept at reasoning, planning, and operating
seamlessly across environments. Our model and code have been open-sourced at
https://github.com/OpenLemur/Lemur.","['large language model', 'agent', 'code generation', 'reasoning', 'decision making']",,https://openreview.net{'value': '/pdf/8ef62990871ebf2cac77dc6ea498085f167f070a.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses language agents and their capabilities in reasoning and planning, it does not explicitly address the use of synthetic data or data synthesis techniques in the context of agent learning. The focus is more on the integration of natural language and coding rather than on synthetic data generation or augmentation methods that would be relevant to the survey topic.","language agents, reasoning, planning, coding capabilities"
https://openreview.net/forum?id=Z59Rb5bPPP,Trajeglish: Traffic Modeling as Next-Token Prediction,"A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.","['self-driving', 'traffic modeling', 'autonomous vehicles', 'simulation', 'motion prediction', 'transformer']",,https://openreview.net{'value': '/pdf/6a1754e5443464338e361380403b4fec2f5d94b2.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,True,0.85,8,"The paper discusses traffic modeling for self-driving vehicles, which involves multi-agent interactions and the generation of synthetic scenarios based on recorded driving logs. This aligns with the survey's focus on agent learning methods that utilize synthetic data for training and simulation. The use of a model that generates realistic traffic scenarios can be seen as a form of data synthesis that enhances agent performance in autonomous driving contexts.","multi-agent systems, synthetic scenario generation, autonomous agents, data-driven modeling, improving agent performance"
https://openreview.net/forum?id=W3VsHuga3j,Modeling Boundedly Rational Agents with Latent Inference Budgets,"We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating constrained inference. In this work, we introduce a latent inference budget model (L-IBM) that models these constraints explicitly, via a latent variable (inferred jointly with a model of agents’ goals) that controls the runtime of an iterative inference algorithm. L-IBMs make it possible to learn agent models using data from diverse populations of suboptimal actors. In three modeling tasks—inferring navigation goals from routes, inferring communicative intents from human utterances, and predicting next moves in human chess games—we show that L-IBMs match or outperforms Boltzmann models of decision-making under uncertainty. Moreover, the inferred inference budgets are themselves meaningful, efficient to compute, and correlated with measures of player skill, partner skill and task difficulty.","['neurosymbolic', 'planning', 'rationality']",,https://openreview.net{'value': '/pdf/b6fcabc7a4521fa9b8ff72f6d09fbafc1e118951.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses modeling agents and their decision-making processes, it does not explicitly involve data synthesis or synthetic data generation techniques. The focus is on bounded rationality and inference budgets rather than on using synthetic data for training or improving agent performance.","bounded rationality, agent modeling, inference budgets"
https://openreview.net/forum?id=VtmBAGCN7o,MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework,"Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.","['Autonomous Agent', 'Meta Programming', 'Multi-Agent Society', 'Group Intelligence']",,https://openreview.net{'value': '/pdf/474fc6dad3bd9bf7fdb97c7cd72b2cc0649a9647.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses a multi-agent system and the collaboration of agents, it does not focus on data synthesis or synthetic data generation techniques. The emphasis is on improving workflows and coherence in task execution rather than on learning from synthetic data or utilizing synthetic data for training agents.","Multi-Agent System, Collaboration, Task Decomposition"
https://openreview.net/forum?id=UBVNwD3hPN,CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents,"The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization’s profound alignment with human society requires sophisticated learning and prior knowledge, while its ever-changing space and action space demand robust reasoning for generalization. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. Overall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.","['Interactive Environments', 'Benchmark', 'Reinforcement Learning', 'Language Agent', 'Multi-agent']",,https://openreview.net{'value': '/pdf/8e9b4884f752883a17e58942608d9cc769610a01.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses decision-making agents and their learning and reasoning capabilities in a complex environment, it does not explicitly focus on data synthesis or synthetic data generation techniques. The emphasis is more on the learning and reasoning aspects of agents rather than on utilizing synthetic data for training or performance improvement.","Decision-making agents, Reinforcement learning, Multi-agent systems"
https://openreview.net/forum?id=Ts95eXsPBc,Spatially-Aware Transformers for Embodied Agents,"Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning. The source code for our models and experiments will be available at \href{https://github.com/spatially_aware_transformer}{https://github.com/spatially_aware_transformer}.","['Episodic Memory', 'Spatial Inference', 'Prediction', 'Generation', 'Reinforcement Learning']",,https://openreview.net{'value': '/pdf/1dc0ff45874e872749597c38bcfc2df0fa7ed0d6.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses reinforcement learning and episodic memory in the context of agents, it does not explicitly address data synthesis or synthetic data generation techniques. The focus is more on improving memory utilization in agents rather than on utilizing synthetic data for training or performance enhancement.","Reinforcement Learning, Episodic Memory, Spatial Information"
https://openreview.net/forum?id=SBoRhRCzM3,THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS,"Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. 
However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}.
To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs.
These analogous problems are related to the input one, with reusable solutions and problem-solving strategies.
Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. 
To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. 
Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch.
TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. 
Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\% improvement of human preference in Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent Planning.","['LLM Complex Reasoning', 'Language Model Reasoning']",,https://openreview.net{'value': '/pdf/1cddcaa3f523f7ad1c9aea87d5180f2d173e4bb8.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.9,2,"The paper focuses on enhancing reasoning capabilities of Large Language Models (LLMs) through an approach called Thought Propagation, which does not involve agent learning or the use of synthetic data in the context of training agents. While it discusses problem-solving strategies, it does not align with the survey's focus on agent learning methods that utilize data synthesis techniques.",
https://openreview.net/forum?id=S2oTVrlcp3,SmartPlay : A Benchmark for LLMs as Intelligent Agents,"{'value': ""Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately.\nSmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. \nWe release our benchmark at https://github.com/microsoft/SmartPlay""}",['Large Language Models'],,https://openreview.net{'value': '/pdf/f84669704e333d9a0d57f5fd1dfd011b03109021.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses intelligent agents and evaluates their capabilities, it does not focus on data synthesis or synthetic data generation techniques. The benchmark introduced is primarily for evaluating large language models in various game settings, which does not align with the survey's focus on agent learning methods that utilize synthetic data for training or performance improvement.",
https://openreview.net/forum?id=Qox9rO0kN0,Learning Multi-Agent Communication from Graph Modeling Perspective,"In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.","['communication learning', 'multi-agent reinforcement learning']",,https://openreview.net{'value': '/pdf/b18ae005fc01451473ec4301d9802e5267831053.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent systems and communication learning, it does not explicitly involve data synthesis, synthetic data generation, or simulated data. The focus is on optimizing communication among agents rather than on the use of synthetic data for training or improving agent performance.","multi-agent systems, communication learning, cooperative tasks"
https://openreview.net/forum?id=NltzxpG0nz,Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds,"{'value': ""Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks and carry out experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. The project’s website and code can be found at https://sites.google.com/view/steve-eye.""}","['large multimodal pre-training', 'open-world embodied agent', 'large language model']",,https://openreview.net{'value': '/pdf/a46ce05ff44a87fada32b918f094df26df3d2f07.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses embodied agents and their interaction with the environment, it primarily focuses on integrating visual perception with large language models rather than on agent learning methods that utilize data synthesis techniques. The dataset mentioned is collected semi-automatically, but it does not explicitly address synthetic data generation or augmentation techniques relevant to the survey's scope.","embodied agents, visual perception, large language models"
https://openreview.net/forum?id=MCNqgUFTHI,Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents,"Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.","['Dialogue Policy Planning', 'Proactive Dialogue', 'Large Language Model']",,https://openreview.net{'value': '/pdf/30c7e22635f23140a4f37883885b6e80b5f7c750.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses a dialogue agent that utilizes reinforcement learning and self-play simulation, it primarily focuses on dialogue policy planning and does not explicitly address data synthesis or synthetic data generation techniques. The connection to synthetic data is weak, as the emphasis is on human-annotated data and reinforcement learning from AI feedback rather than on generating synthetic data for training.","Dialogue agents, Reinforcement learning, Self-play simulation"
https://openreview.net/forum?id=M6XWoEdmwf,AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents,"{'value': ""We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments.""}","['Meta-RL', 'Generalization', 'Long-Term Memory', 'Transformers']",,https://openreview.net{'value': '/pdf/6ffd1eb5dc0bd2b144d5d0309763b3ed5e114e8b.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses a reinforcement learning agent (AMAGO) and addresses challenges in generalization and long-term memory, it does not explicitly mention the use of synthetic data or data synthesis techniques. The focus is more on improving the agent's learning capabilities and scalability rather than on utilizing synthetic data for training or performance enhancement.","Reinforcement Learning, Meta-RL, Long-Term Memory"
https://openreview.net/forum?id=LjivA1SLZ6,Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning,"In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.","['Multi-agent reinforcement learning', 'episodic control', 'episodic incentive', 'state embedding']",,https://openreview.net{'value': '/pdf/8b2d5ac5539754d00bf99458a60c63157c74fbdb.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses cooperative multi-agent reinforcement learning and introduces a method to improve learning efficiency, it does not explicitly involve data synthesis or synthetic data generation techniques. The focus is on episodic memory utilization and reward structures rather than on utilizing synthetic data for training agents.","multi-agent reinforcement learning, episodic memory, reward structure"
https://openreview.net/forum?id=KOZu91CzbK,Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization,"Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.","['Language Agent', 'AI Agent', 'Reinforcement Learning']",,https://openreview.net{'value': '/pdf/d02f39256b41ec50ca8cd3a5b136065bc7a4caae.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses reinforcement learning in the context of language agents and involves optimizing agent behavior through environment feedback, it does not explicitly address data synthesis, synthetic data generation, or simulated environments. The focus is more on the optimization of language models rather than the synthesis of data for training agents.","Reinforcement Learning, Language Agents, Policy Gradient Optimization"
https://openreview.net/forum?id=GEcwtMk1uA,Identifying the Risks of LM Agents with an LM-Emulated Sandbox,"Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes toolkits and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.","['Language Model Agent', 'Tool Use', 'Evaluation', 'Safety', 'Language Model']",,https://openreview.net{'value': '/pdf/d1601f78407737fc216de9e6ec0085038f8c885f.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses Language Model agents and their evaluation in a sandbox environment, it primarily focuses on risk identification and safety evaluation rather than on agent learning methods that utilize data synthesis techniques. There is no clear connection to synthetic data generation or training methods relevant to the survey topic.",
https://openreview.net/forum?id=FQepisCUWu,ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,"{'value': ""Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies.\nIn this paper, we construct a multi-agent referee team called $\\textbf{ChatEval}$ to autonomously discuss and evaluate the quality of different texts. \nOur experiments on two benchmarks illustrate that ChatEval delivers superior accuracy and correlation in alignment with human assessment. Furthermore, we find that the diverse role prompts (different personas) are essential in the multi-agent debate process; that is, utilizing the same role description in the prompts can lead to a degradation in performance. Our qualitative analysis also shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments.""}","['Large Language Model', 'Multi-Agent Debate', 'LLM evaluators']",,https://openreview.net{'value': '/pdf/6950b70dde4f8d7425424dce9cc4063f505b2194.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses a multi-agent system (ChatEval) for text evaluation, it does not focus on agent learning methods that utilize data synthesis techniques. The primary emphasis is on evaluating text quality rather than training agents with synthetic data or improving agent performance through data synthesis. There is no clear connection to the survey's focus on agent learning with synthetic data.",
https://openreview.net/forum?id=EpVe8jAjdx,Privileged Sensing Scaffolds Reinforcement Learning,"We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon “sensory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose “Scaffolder”, a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new “S3” suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/","['reinforcement learning', 'model-based reinforcement learning', 'world models', 'robotics', 'privileged information', 'asymmetric learning', 'multimodality', 'perception', 'sensing']",,https://openreview.net{'value': '/pdf/8ab7ec56b6e56ac45ce2b68e95b277283dbb8377.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses reinforcement learning agents and their training using privileged sensing, it does not explicitly focus on data synthesis or synthetic data generation techniques. The emphasis is more on leveraging privileged information during training rather than generating synthetic data or using simulated environments for training agents. Therefore, it does not align closely with the survey's focus on agent learning with data synthesis.","reinforcement learning, privileged information, training agents, simulated robotic tasks"
https://openreview.net/forum?id=Ep0TtjVoap,ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving,"{'value': ""Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.""}","['Large Language Models', 'Mathematical Reasoning', 'Tool Learning']",,https://openreview.net{'value': '/pdf/2b0b45b11d0f61912efb1a932fb494d36f7b88e6.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses a reasoning agent (ToRA) that utilizes tool integration for mathematical problem solving, it does not focus on data synthesis or synthetic data generation in the context of agent learning. The primary emphasis is on tool interaction and imitation learning rather than the synthesis of data for training agents. Therefore, it does not align well with the survey's scope.",
https://openreview.net/forum?id=EnXJfQqy0K,Building Cooperative Embodied Agents Modularly with Large Language Models,"In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.","['Large Language Models', 'Embodied Intelligence', 'Multi-Agent Cooperation', 'Human-AI Interaction', 'Communication']",,https://openreview.net{'value': '/pdf/9baa34313f1aea292b16e30697c20766afadddb1.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent cooperation and the use of large language models in agent learning, it does not explicitly focus on data synthesis or synthetic data generation techniques. The emphasis is more on communication and cooperation rather than on the synthesis of data for training agents.","Multi-Agent Cooperation, Large Language Models, Embodied Agents, Human-Agent Interaction"
https://openreview.net/forum?id=EHg5GDnyq1,AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors,"Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework AgentVerse that can effectively orchestrate a collaborative group of expert agents as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that AgentVerse can proficiently deploy multi-agent groups that outperform a single agent. Extensive experiments on text understanding, reasoning, coding, tool utilization, and embodied AI confirm the effectiveness of AgentVerse. Moreover, our analysis of agent interactions within AgentVerse reveals the emergence of specific collaborative behaviors, contributing to heightened group efficiency. We will release our codebase, AgentVerse, to further facilitate multi-agent research.","['large language mode', 'agent', 'multi-agent']",,https://openreview.net{'value': '/pdf/7c9bd9a841a2ba0c11ea97abb5e982430c2fc95e.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses multi-agent systems and collaboration among agents, it does not explicitly address data synthesis, synthetic data generation, or the use of simulated environments for agent training. The focus is more on the emergent behaviors and collaboration rather than on learning from synthetic data or data augmentation techniques.","multi-agent systems, collaborative behaviors, autonomous agents"
https://openreview.net/forum?id=BqEvdOS1Hs,Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain,"{'value': 'Existing game AI research mainly focuses on enhancing agents\' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading to poor experiences for their human partners. In other words, most game AI agents are modeled in a ""self-centered"" manner. In this paper, we propose a ""human-centered"" modeling scheme for collaborative agents that aims to enhance the experience of humans. Specifically, we model the experience of humans as the goals they expect to achieve during the task. We expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents\' original abilities (e.g., winning games). To achieve this, we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG approach introduces a ""baseline"", which corresponds to the extent to which humans primitively achieve their goals, and encourages agents to learn behaviors that can effectively enhance humans in achieving their goals better. We evaluate the RLHG agent in the popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both objective performance and subjective preference results show that the RLHG agent provides participants better gaming experience.'}","['human enhancement', 'human-agent collaboration', 'game playing', 'deep reinforcement learning']",,https://openreview.net{'value': '/pdf/6ce678ca6108e4a85517e2526c83d7d968f7108b.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,3,"While the paper discusses reinforcement learning agents and their collaboration with humans, it does not focus on data synthesis, synthetic data generation, or simulated data. The primary emphasis is on enhancing human experience rather than on the use of synthetic data for training agents.","human-agent collaboration, reinforcement learning, game AI"
https://openreview.net/forum?id=ADSxCpCu9s,LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents,"Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.","['task planning', 'language models', 'benchmarking', 'embodied agents', 'home robots']",,https://openreview.net{'value': '/pdf/35abce446ca9b6e7a9136f7c38556084c63538ec.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses embodied agents and task planning, it primarily focuses on benchmarking language-oriented task planners rather than on agent learning methods that utilize data synthesis techniques. There is no clear emphasis on synthetic data generation or its application in training agents, which is a core aspect of the survey topic.","task planning, embodied agents, benchmarking, language models"
https://openreview.net/forum?id=9JQtrumvg8,"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis","Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.
However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.
We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.
WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.
We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.
We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.","['Web Navigation', 'Web Automation', 'Large Language Models', 'Language Model Agents', 'Tool Use', 'Program Synthesis']",,https://openreview.net{'value': '/pdf/0b27823f96e3efd0ed6921aafc4fe4643d1aeec5.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses an agent (WebAgent) that learns from self-experience and utilizes planning and summarization techniques, it does not explicitly focus on data synthesis or synthetic data generation. The emphasis is on improving performance on real-world tasks rather than on the use of synthetic data for training or enhancing agent learning. Therefore, it does not align closely with the survey's focus on agent learning with data synthesis.","agent learning, large language models, task completion, real-world application"
https://openreview.net/forum?id=8HCARN2hhw,Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction,"Agents navigating in 3D environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly defined interpretation, whereas classical robotics addresses this with scene reconstruction resulting in some form of map, usually estimated with geometry and sensor models and/or learning. In this work we propose to learn an actionable representation of the scene independently of the targeted downstream task and without explicitly optimizing reconstruction. The learned representation is optimized by a blind auxiliary agent trained to navigate with it on multiple short sub episodes branching out from a waypoint and, most importantly, without any direct visual observation. We argue and show that the blindness property is important and forces the (trained) latent representation to be the only means for planning. With probing experiments we show that the learned representation optimizes navigability and not reconstruction. On downstream tasks we show that it is robust to changes in distribution, in particular the sim2real gap, which we evaluate with a real physical robot in a real office building, significantly improving performance.","['Navigation', 'Embodied AI', 'Perception']",,https://openreview.net{'value': '/pdf/2b5b56f45f498f5877f3de2ea54e150352e0a408.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.8,4,"While the paper discusses agents navigating in 3D environments and emphasizes learning actionable representations for decision-making, it does not explicitly involve data synthesis, synthetic data generation, or simulated data. The focus is more on representation learning and navigation rather than on the use of synthetic data for training agents.","Agent navigation, Latent representation, Robustness to distribution changes"
https://openreview.net/forum?id=7M0EzjugaN,Online Continual Learning for Interactive Instruction Following Agents,"In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic, since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous ‘data prior’ based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed challenging Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior arts in our empirical validations by noticeable margins.","['Embodied AI', 'Continual Learning']",,https://openreview.net{'value': '/pdf/9bd1847dd40bdb80227f6ddd6480e1eee45a4999.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.7,4,"While the paper discusses learning in embodied agents and continual learning setups, it does not explicitly involve data synthesis, synthetic data generation, or simulated data. The focus is on learning new behaviors and environments rather than utilizing synthetic data for training or performance improvement.","Embodied agents, Continual learning, Behavior Incremental Learning, Environment Incremental Learning"
https://openreview.net/forum?id=22pyNMuIoa,PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization,"{'value': ""Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts.""}","['Large Language Models', 'Expert-level Prompt Optimization', 'Strategic Planning']",,https://openreview.net{'value': '/pdf/d4dbdee0d105cd3020bffdc2f56d99c33429d49c.pdf'},ICLR,2024,ICLR.cc,data/ICLR/2024/papers.csv,False,0.9,3,"While the paper discusses an agent (PromptAgent) that optimizes prompts for language models, it does not involve reinforcement learning, multi-agent systems, or synthetic data generation in the context of agent learning. The focus is on prompt optimization rather than on learning from synthetic data or environments, which is central to the survey topic.",
